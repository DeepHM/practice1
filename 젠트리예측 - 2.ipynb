{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hm\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 7\n",
    "data_dim = 5\n",
    "hidden_dim = 5\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 500\n",
    "layer_size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape:  (423, 7, 5)\n",
      "Y Shape:  (423, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.loadtxt('E:\\\\amazone/x0916.csv', delimiter=',')\n",
    "y = np.loadtxt('E:\\\\amazone/y16.csv', delimiter=',')\n",
    "x_norm = MinMaxScaler(x).reshape([-1,seq_length, data_dim])\n",
    "y_norm = MinMaxScaler(y).reshape([-1,output_dim])\n",
    "print(\"X Shape: \", x_norm.shape)\n",
    "print(\"Y Shape: \", y_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train X Y Shape (323, 7, 5) (323, 1)\n",
      "test X Y Shape (100, 7, 5) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_norm[:323,:,:]\n",
    "y_train = y_norm[:323,:]\n",
    "x_test = x_norm[323:,:,:]\n",
    "y_test = y_norm[323:,:]\n",
    "print(\"train X Y Shape\", x_train.shape, y_train.shape)\n",
    "print(\"test X Y Shape\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02250192, 0.01971394, 0.0196591 , ..., 0.01584943, 0.0123388 ,\n",
       "       0.01526602])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = MinMaxScaler(x)\n",
    "tmp[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"rnn/Const:0\", shape=(1,), dtype=int32) must be from the same graph as Tensor(\"ExpandDims:0\", shape=(1,), dtype=int32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-0ba8ff49ef0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftsign\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmulti_cells\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiRNNCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlayer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmulti_cells\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m Y_pred = tf.contrib.layers.fully_connected(\n\u001b[0;32m      6\u001b[0m     outputs[:, -1], output_dim, activation_fn=None)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[1;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m         dtype=dtype)\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[1;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m   flat_zero_output = tuple(_create_zero_arrays(output)\n\u001b[1;32m--> 724\u001b[1;33m                            for output in flat_output_size)\n\u001b[0m\u001b[0;32m    725\u001b[0m   zero_output = nest.pack_sequence_as(structure=cell.output_size,\n\u001b[0;32m    726\u001b[0m                                       flat_sequence=flat_zero_output)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m   flat_zero_output = tuple(_create_zero_arrays(output)\n\u001b[1;32m--> 724\u001b[1;33m                            for output in flat_output_size)\n\u001b[0m\u001b[0;32m    725\u001b[0m   zero_output = nest.pack_sequence_as(structure=cell.output_size,\n\u001b[0;32m    726\u001b[0m                                       flat_sequence=flat_zero_output)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_create_zero_arrays\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m    717\u001b[0m   \u001b[1;31m# Prepare dynamic conditional copying of state & output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_zero_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m     \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m     return array_ops.zeros(\n\u001b[0;32m    721\u001b[0m         array_ops.stack(size), _infer_state_dtype(dtype, state))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m_concat\u001b[1;34m(prefix, suffix, static)\u001b[0m\n\u001b[0;32m    119\u001b[0m       raise ValueError(\"Provided a prefix or suffix of None: %s and %s\"\n\u001b[0;32m    120\u001b[0m                        % (prefix, suffix))\n\u001b[1;32m--> 121\u001b[1;33m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   1179\u001b[0m               tensor_shape.scalar())\n\u001b[0;32m   1180\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1181\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[0m_attr_N\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 1101\u001b[1;33m         \"ConcatV2\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[0;32m   1102\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    348\u001b[0m       \u001b[1;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m       \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m       \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[1;34m(op_input_list, graph)\u001b[0m\n\u001b[0;32m   5428\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5429\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5430\u001b[1;33m         \u001b[0m_assert_same_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5431\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5432\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[1;34m(original_item, item)\u001b[0m\n\u001b[0;32m   5364\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5365\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" % (item,\n\u001b[1;32m-> 5366\u001b[1;33m                                                                 original_item))\n\u001b[0m\u001b[0;32m   5367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor(\"rnn/Const:0\", shape=(1,), dtype=int32) must be from the same graph as Tensor(\"ExpandDims:0\", shape=(1,), dtype=int32)."
     ]
    }
   ],
   "source": [
    "#cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.nn.softsign)\n",
    "multi_cells = tf.contrib.rnn.MultiRNNCell([cell]*layer_size, state_is_tuple=True)\n",
    "outputs, _states = tf.nn.dynamic_rnn(multi_cells, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 46.373111724853516\n",
      "[step: 1] loss: 22.58043670654297\n",
      "[step: 2] loss: 23.869077682495117\n",
      "[step: 3] loss: 18.83365821838379\n",
      "[step: 4] loss: 8.788361549377441\n",
      "[step: 5] loss: 3.513957977294922\n",
      "[step: 6] loss: 5.742376804351807\n",
      "[step: 7] loss: 6.600801944732666\n",
      "[step: 8] loss: 3.5705788135528564\n",
      "[step: 9] loss: 4.007412910461426\n",
      "[step: 10] loss: 5.682740211486816\n",
      "[step: 11] loss: 4.794976711273193\n",
      "[step: 12] loss: 3.177976131439209\n",
      "[step: 13] loss: 3.099640130996704\n",
      "[step: 14] loss: 3.8938934803009033\n",
      "[step: 15] loss: 3.9393274784088135\n",
      "[step: 16] loss: 3.2396256923675537\n",
      "[step: 17] loss: 2.740412950515747\n",
      "[step: 18] loss: 2.794477701187134\n",
      "[step: 19] loss: 3.0985300540924072\n",
      "[step: 20] loss: 3.2682487964630127\n",
      "[step: 21] loss: 3.1633622646331787\n",
      "[step: 22] loss: 2.8886799812316895\n",
      "[step: 23] loss: 2.647473096847534\n",
      "[step: 24] loss: 2.581526279449463\n",
      "[step: 25] loss: 2.67769193649292\n",
      "[step: 26] loss: 2.7958786487579346\n",
      "[step: 27] loss: 2.804797410964966\n",
      "[step: 28] loss: 2.697303056716919\n",
      "[step: 29] loss: 2.5688769817352295\n",
      "[step: 30] loss: 2.508943796157837\n",
      "[step: 31] loss: 2.5318031311035156\n",
      "[step: 32] loss: 2.589318037033081\n",
      "[step: 33] loss: 2.62251615524292\n",
      "[step: 34] loss: 2.6029388904571533\n",
      "[step: 35] loss: 2.543762445449829\n",
      "[step: 36] loss: 2.4836912155151367\n",
      "[step: 37] loss: 2.4573731422424316\n",
      "[step: 38] loss: 2.470853567123413\n",
      "[step: 39] loss: 2.499119281768799\n",
      "[step: 40] loss: 2.508960008621216\n",
      "[step: 41] loss: 2.4877593517303467\n",
      "[step: 42] loss: 2.451246976852417\n",
      "[step: 43] loss: 2.425156593322754\n",
      "[step: 44] loss: 2.4219748973846436\n",
      "[step: 45] loss: 2.433828353881836\n",
      "[step: 46] loss: 2.442984104156494\n",
      "[step: 47] loss: 2.4373416900634766\n",
      "[step: 48] loss: 2.4185991287231445\n",
      "[step: 49] loss: 2.3988254070281982\n",
      "[step: 50] loss: 2.3895111083984375\n",
      "[step: 51] loss: 2.392026662826538\n",
      "[step: 52] loss: 2.3973677158355713\n",
      "[step: 53] loss: 2.3953161239624023\n",
      "[step: 54] loss: 2.384045362472534\n",
      "[step: 55] loss: 2.3706657886505127\n",
      "[step: 56] loss: 2.3631420135498047\n",
      "[step: 57] loss: 2.3627448081970215\n",
      "[step: 58] loss: 2.3640685081481934\n",
      "[step: 59] loss: 2.3611271381378174\n",
      "[step: 60] loss: 2.352928400039673\n",
      "[step: 61] loss: 2.3436598777770996\n",
      "[step: 62] loss: 2.3380250930786133\n",
      "[step: 63] loss: 2.336534023284912\n",
      "[step: 64] loss: 2.3354294300079346\n",
      "[step: 65] loss: 2.331077814102173\n",
      "[step: 66] loss: 2.3238277435302734\n",
      "[step: 67] loss: 2.31708025932312\n",
      "[step: 68] loss: 2.313140392303467\n",
      "[step: 69] loss: 2.310878276824951\n",
      "[step: 70] loss: 2.307480573654175\n",
      "[step: 71] loss: 2.3017749786376953\n",
      "[step: 72] loss: 2.2952895164489746\n",
      "[step: 73] loss: 2.2901604175567627\n",
      "[step: 74] loss: 2.286550521850586\n",
      "[step: 75] loss: 2.2826614379882812\n",
      "[step: 76] loss: 2.2771501541137695\n",
      "[step: 77] loss: 2.270740032196045\n",
      "[step: 78] loss: 2.265014410018921\n",
      "[step: 79] loss: 2.2602529525756836\n",
      "[step: 80] loss: 2.25522780418396\n",
      "[step: 81] loss: 2.248985767364502\n",
      "[step: 82] loss: 2.242051124572754\n",
      "[step: 83] loss: 2.2355029582977295\n",
      "[step: 84] loss: 2.2293782234191895\n",
      "[step: 85] loss: 2.2226948738098145\n",
      "[step: 86] loss: 2.2149696350097656\n",
      "[step: 87] loss: 2.206845760345459\n",
      "[step: 88] loss: 2.1989328861236572\n",
      "[step: 89] loss: 2.190803050994873\n",
      "[step: 90] loss: 2.1817145347595215\n",
      "[step: 91] loss: 2.1717610359191895\n",
      "[step: 92] loss: 2.161543846130371\n",
      "[step: 93] loss: 2.1510000228881836\n",
      "[step: 94] loss: 2.1394686698913574\n",
      "[step: 95] loss: 2.12682843208313\n",
      "[step: 96] loss: 2.1135363578796387\n",
      "[step: 97] loss: 2.0995147228240967\n",
      "[step: 98] loss: 2.08410906791687\n",
      "[step: 99] loss: 2.0671887397766113\n",
      "[step: 100] loss: 2.049067258834839\n",
      "[step: 101] loss: 2.029423713684082\n",
      "[step: 102] loss: 2.007668972015381\n",
      "[step: 103] loss: 1.9839253425598145\n",
      "[step: 104] loss: 1.9580950736999512\n",
      "[step: 105] loss: 1.929266095161438\n",
      "[step: 106] loss: 1.8973183631896973\n",
      "[step: 107] loss: 1.8621456623077393\n",
      "[step: 108] loss: 1.8227319717407227\n",
      "[step: 109] loss: 1.7790727615356445\n",
      "[step: 110] loss: 1.7302827835083008\n",
      "[step: 111] loss: 1.6762375831604004\n",
      "[step: 112] loss: 1.6167974472045898\n",
      "[step: 113] loss: 1.5527112483978271\n",
      "[step: 114] loss: 1.484476923942566\n",
      "[step: 115] loss: 1.4148962497711182\n",
      "[step: 116] loss: 1.3470861911773682\n",
      "[step: 117] loss: 1.2844359874725342\n",
      "[step: 118] loss: 1.2547746896743774\n",
      "[step: 119] loss: 2.0306437015533447\n",
      "[step: 120] loss: 8.283214569091797\n",
      "[step: 121] loss: 3.9275753498077393\n",
      "[step: 122] loss: 3.2713332176208496\n",
      "[step: 123] loss: 1.4559966325759888\n",
      "[step: 124] loss: 2.6793739795684814\n",
      "[step: 125] loss: 3.152381181716919\n",
      "[step: 126] loss: 2.440173387527466\n",
      "[step: 127] loss: 1.8104373216629028\n",
      "[step: 128] loss: 2.1172540187835693\n",
      "[step: 129] loss: 2.661062479019165\n",
      "[step: 130] loss: 2.427830457687378\n",
      "[step: 131] loss: 1.9613630771636963\n",
      "[step: 132] loss: 1.9658126831054688\n",
      "[step: 133] loss: 2.2442877292633057\n",
      "[step: 134] loss: 2.3544092178344727\n",
      "[step: 135] loss: 2.17561936378479\n",
      "[step: 136] loss: 1.9259017705917358\n",
      "[step: 137] loss: 1.8675099611282349\n",
      "[step: 138] loss: 2.002796173095703\n",
      "[step: 139] loss: 2.0827863216400146\n",
      "[step: 140] loss: 1.9669042825698853\n",
      "[step: 141] loss: 1.7987622022628784\n",
      "[step: 142] loss: 1.7480158805847168\n",
      "[step: 143] loss: 1.7990955114364624\n",
      "[step: 144] loss: 1.8287683725357056\n",
      "[step: 145] loss: 1.7659531831741333\n",
      "[step: 146] loss: 1.6504520177841187\n",
      "[step: 147] loss: 1.571950912475586\n",
      "[step: 148] loss: 1.565417766571045\n",
      "[step: 149] loss: 1.572799563407898\n",
      "[step: 150] loss: 1.5234960317611694\n",
      "[step: 151] loss: 1.4298628568649292\n",
      "[step: 152] loss: 1.359670639038086\n",
      "[step: 153] loss: 1.3400237560272217\n",
      "[step: 154] loss: 1.3340551853179932\n",
      "[step: 155] loss: 1.2990801334381104\n",
      "[step: 156] loss: 1.2388715744018555\n",
      "[step: 157] loss: 1.1940183639526367\n",
      "[step: 158] loss: 1.1881496906280518\n",
      "[step: 159] loss: 1.1979621648788452\n",
      "[step: 160] loss: 1.1899645328521729\n",
      "[step: 161] loss: 1.169827938079834\n",
      "[step: 162] loss: 1.1649746894836426\n",
      "[step: 163] loss: 1.173496961593628\n",
      "[step: 164] loss: 1.1692085266113281\n",
      "[step: 165] loss: 1.1525399684906006\n",
      "[step: 166] loss: 1.1499782800674438\n",
      "[step: 167] loss: 1.1544926166534424\n",
      "[step: 168] loss: 1.1396939754486084\n",
      "[step: 169] loss: 1.1231714487075806\n",
      "[step: 170] loss: 1.1205732822418213\n",
      "[step: 171] loss: 1.114876627922058\n",
      "[step: 172] loss: 1.100325345993042\n",
      "[step: 173] loss: 1.0913056135177612\n",
      "[step: 174] loss: 1.090749979019165\n",
      "[step: 175] loss: 1.0865994691848755\n",
      "[step: 176] loss: 1.0775110721588135\n",
      "[step: 177] loss: 1.0733846426010132\n",
      "[step: 178] loss: 1.0744071006774902\n",
      "[step: 179] loss: 1.0720226764678955\n",
      "[step: 180] loss: 1.0660059452056885\n",
      "[step: 181] loss: 1.0630460977554321\n",
      "[step: 182] loss: 1.0619878768920898\n",
      "[step: 183] loss: 1.056959867477417\n",
      "[step: 184] loss: 1.0504286289215088\n",
      "[step: 185] loss: 1.0470318794250488\n",
      "[step: 186] loss: 1.0434978008270264\n",
      "[step: 187] loss: 1.0377907752990723\n",
      "[step: 188] loss: 1.034360647201538\n",
      "[step: 189] loss: 1.0327785015106201\n",
      "[step: 190] loss: 1.0290447473526\n",
      "[step: 191] loss: 1.0261174440383911\n",
      "[step: 192] loss: 1.0252851247787476\n",
      "[step: 193] loss: 1.0228800773620605\n",
      "[step: 194] loss: 1.0204836130142212\n",
      "[step: 195] loss: 1.019769310951233\n",
      "[step: 196] loss: 1.0178660154342651\n",
      "[step: 197] loss: 1.0158406496047974\n",
      "[step: 198] loss: 1.0150843858718872\n",
      "[step: 199] loss: 1.0134696960449219\n",
      "[step: 200] loss: 1.0117297172546387\n",
      "[step: 201] loss: 1.0108999013900757\n",
      "[step: 202] loss: 1.0094057321548462\n",
      "[step: 203] loss: 1.0078622102737427\n",
      "[step: 204] loss: 1.0070040225982666\n",
      "[step: 205] loss: 1.005634069442749\n",
      "[step: 206] loss: 1.004258394241333\n",
      "[step: 207] loss: 1.0033172369003296\n",
      "[step: 208] loss: 1.0018731355667114\n",
      "[step: 209] loss: 1.000413417816162\n",
      "[step: 210] loss: 0.9991670846939087\n",
      "[step: 211] loss: 0.997500479221344\n",
      "[step: 212] loss: 0.9959221482276917\n",
      "[step: 213] loss: 0.9945109486579895\n",
      "[step: 214] loss: 0.9928663372993469\n",
      "[step: 215] loss: 0.9914337396621704\n",
      "[step: 216] loss: 0.9901152849197388\n",
      "[step: 217] loss: 0.988655686378479\n",
      "[step: 218] loss: 0.9873821139335632\n",
      "[step: 219] loss: 0.9860954880714417\n",
      "[step: 220] loss: 0.98469477891922\n",
      "[step: 221] loss: 0.9834184050559998\n",
      "[step: 222] loss: 0.9820572733879089\n",
      "[step: 223] loss: 0.980663001537323\n",
      "[step: 224] loss: 0.9793603420257568\n",
      "[step: 225] loss: 0.9779651165008545\n",
      "[step: 226] loss: 0.9766026139259338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 227] loss: 0.9752663969993591\n",
      "[step: 228] loss: 0.9738543629646301\n",
      "[step: 229] loss: 0.9724928736686707\n",
      "[step: 230] loss: 0.9710965752601624\n",
      "[step: 231] loss: 0.9696711897850037\n",
      "[step: 232] loss: 0.9682826995849609\n",
      "[step: 233] loss: 0.9668461084365845\n",
      "[step: 234] loss: 0.9654258489608765\n",
      "[step: 235] loss: 0.9640041589736938\n",
      "[step: 236] loss: 0.9625478982925415\n",
      "[step: 237] loss: 0.961115300655365\n",
      "[step: 238] loss: 0.9596561193466187\n",
      "[step: 239] loss: 0.9582009315490723\n",
      "[step: 240] loss: 0.9567559957504272\n",
      "[step: 241] loss: 0.9552955627441406\n",
      "[step: 242] loss: 0.9538596272468567\n",
      "[step: 243] loss: 0.9524118900299072\n",
      "[step: 244] loss: 0.9509688019752502\n",
      "[step: 245] loss: 0.9495257139205933\n",
      "[step: 246] loss: 0.9480689167976379\n",
      "[step: 247] loss: 0.9466190934181213\n",
      "[step: 248] loss: 0.9451534748077393\n",
      "[step: 249] loss: 0.9436912536621094\n",
      "[step: 250] loss: 0.9422202706336975\n",
      "[step: 251] loss: 0.9407450556755066\n",
      "[step: 252] loss: 0.9392681121826172\n",
      "[step: 253] loss: 0.9377798438072205\n",
      "[step: 254] loss: 0.9362916946411133\n",
      "[step: 255] loss: 0.9347911477088928\n",
      "[step: 256] loss: 0.9332887530326843\n",
      "[step: 257] loss: 0.9317758679389954\n",
      "[step: 258] loss: 0.9302566051483154\n",
      "[step: 259] loss: 0.9287284016609192\n",
      "[step: 260] loss: 0.9271900057792664\n",
      "[step: 261] loss: 0.9256440997123718\n",
      "[step: 262] loss: 0.9240861535072327\n",
      "[step: 263] loss: 0.9225214719772339\n",
      "[step: 264] loss: 0.9209451675415039\n",
      "[step: 265] loss: 0.91936194896698\n",
      "[step: 266] loss: 0.917767345905304\n",
      "[step: 267] loss: 0.9161641597747803\n",
      "[step: 268] loss: 0.9145492911338806\n",
      "[step: 269] loss: 0.9129252433776855\n",
      "[step: 270] loss: 0.9112904071807861\n",
      "[step: 271] loss: 0.9096462726593018\n",
      "[step: 272] loss: 0.907991886138916\n",
      "[step: 273] loss: 0.9063283205032349\n",
      "[step: 274] loss: 0.9046545624732971\n",
      "[step: 275] loss: 0.9029712080955505\n",
      "[step: 276] loss: 0.9012781381607056\n",
      "[step: 277] loss: 0.8995758295059204\n",
      "[step: 278] loss: 0.8978639245033264\n",
      "[step: 279] loss: 0.8961430788040161\n",
      "[step: 280] loss: 0.8944128751754761\n",
      "[step: 281] loss: 0.8926740288734436\n",
      "[step: 282] loss: 0.89092618227005\n",
      "[step: 283] loss: 0.8891700506210327\n",
      "[step: 284] loss: 0.887405276298523\n",
      "[step: 285] loss: 0.8856328725814819\n",
      "[step: 286] loss: 0.8838517069816589\n",
      "[step: 287] loss: 0.8820626139640808\n",
      "[step: 288] loss: 0.8802648186683655\n",
      "[step: 289] loss: 0.8784587979316711\n",
      "[step: 290] loss: 0.8766443133354187\n",
      "[step: 291] loss: 0.8748210072517395\n",
      "[step: 292] loss: 0.8729895353317261\n",
      "[step: 293] loss: 0.8711502552032471\n",
      "[step: 294] loss: 0.869303286075592\n",
      "[step: 295] loss: 0.8674487471580505\n",
      "[step: 296] loss: 0.8655874133110046\n",
      "[step: 297] loss: 0.8637201189994812\n",
      "[step: 298] loss: 0.8618466258049011\n",
      "[step: 299] loss: 0.8599680066108704\n",
      "[step: 300] loss: 0.8580840826034546\n",
      "[step: 301] loss: 0.8561956286430359\n",
      "[step: 302] loss: 0.8543027639389038\n",
      "[step: 303] loss: 0.8524060249328613\n",
      "[step: 304] loss: 0.8505046367645264\n",
      "[step: 305] loss: 0.8485988974571228\n",
      "[step: 306] loss: 0.8466890454292297\n",
      "[step: 307] loss: 0.844774603843689\n",
      "[step: 308] loss: 0.8428559303283691\n",
      "[step: 309] loss: 0.840933084487915\n",
      "[step: 310] loss: 0.8390055298805237\n",
      "[step: 311] loss: 0.8370723128318787\n",
      "[step: 312] loss: 0.8351342082023621\n",
      "[step: 313] loss: 0.8331900835037231\n",
      "[step: 314] loss: 0.8312394022941589\n",
      "[step: 315] loss: 0.8292814493179321\n",
      "[step: 316] loss: 0.8273151516914368\n",
      "[step: 317] loss: 0.8253402709960938\n",
      "[step: 318] loss: 0.823355495929718\n",
      "[step: 319] loss: 0.8213599920272827\n",
      "[step: 320] loss: 0.8193528056144714\n",
      "[step: 321] loss: 0.8173328638076782\n",
      "[step: 322] loss: 0.8152992129325867\n",
      "[step: 323] loss: 0.8132510781288147\n",
      "[step: 324] loss: 0.8111886382102966\n",
      "[step: 325] loss: 0.8091115355491638\n",
      "[step: 326] loss: 0.8070207834243774\n",
      "[step: 327] loss: 0.8049159646034241\n",
      "[step: 328] loss: 0.8027963638305664\n",
      "[step: 329] loss: 0.8006607294082642\n",
      "[step: 330] loss: 0.7985069155693054\n",
      "[step: 331] loss: 0.7963358163833618\n",
      "[step: 332] loss: 0.7941464781761169\n",
      "[step: 333] loss: 0.7919395565986633\n",
      "[step: 334] loss: 0.7897169589996338\n",
      "[step: 335] loss: 0.7874892950057983\n",
      "[step: 336] loss: 0.7852972745895386\n",
      "[step: 337] loss: 0.7833704948425293\n",
      "[step: 338] loss: 0.7829583287239075\n",
      "[step: 339] loss: 0.7917954921722412\n",
      "[step: 340] loss: 0.8575507998466492\n",
      "[step: 341] loss: 1.2278794050216675\n",
      "[step: 342] loss: 2.1479508876800537\n",
      "[step: 343] loss: 1.6751991510391235\n",
      "[step: 344] loss: 0.9627193808555603\n",
      "[step: 345] loss: 1.6764543056488037\n",
      "[step: 346] loss: 1.1108235120773315\n",
      "[step: 347] loss: 1.2945393323898315\n",
      "[step: 348] loss: 0.892894446849823\n",
      "[step: 349] loss: 1.169119119644165\n",
      "[step: 350] loss: 1.1818583011627197\n",
      "[step: 351] loss: 0.9064163565635681\n",
      "[step: 352] loss: 1.0298948287963867\n",
      "[step: 353] loss: 1.1265084743499756\n",
      "[step: 354] loss: 0.9630640149116516\n",
      "[step: 355] loss: 0.8957690000534058\n",
      "[step: 356] loss: 1.0095981359481812\n",
      "[step: 357] loss: 0.9929060935974121\n",
      "[step: 358] loss: 0.8889899849891663\n",
      "[step: 359] loss: 0.903866171836853\n",
      "[step: 360] loss: 0.9610586166381836\n",
      "[step: 361] loss: 0.934680700302124\n",
      "[step: 362] loss: 0.8776008486747742\n",
      "[step: 363] loss: 0.8888242244720459\n",
      "[step: 364] loss: 0.9206584692001343\n",
      "[step: 365] loss: 0.891788899898529\n",
      "[step: 366] loss: 0.8554673194885254\n",
      "[step: 367] loss: 0.8678905963897705\n",
      "[step: 368] loss: 0.8803486824035645\n",
      "[step: 369] loss: 0.8545106649398804\n",
      "[step: 370] loss: 0.8336552381515503\n",
      "[step: 371] loss: 0.8468740582466125\n",
      "[step: 372] loss: 0.8501701951026917\n",
      "[step: 373] loss: 0.8269343972206116\n",
      "[step: 374] loss: 0.8203080296516418\n",
      "[step: 375] loss: 0.8318983316421509\n",
      "[step: 376] loss: 0.8262126445770264\n",
      "[step: 377] loss: 0.8098925948143005\n",
      "[step: 378] loss: 0.8109596371650696\n",
      "[step: 379] loss: 0.8151819109916687\n",
      "[step: 380] loss: 0.803167998790741\n",
      "[step: 381] loss: 0.7955453991889954\n",
      "[step: 382] loss: 0.8000516891479492\n",
      "[step: 383] loss: 0.7965842485427856\n",
      "[step: 384] loss: 0.7868635058403015\n",
      "[step: 385] loss: 0.7867835760116577\n",
      "[step: 386] loss: 0.7875458598136902\n",
      "[step: 387] loss: 0.7798800468444824\n",
      "[step: 388] loss: 0.7759132385253906\n",
      "[step: 389] loss: 0.77705979347229\n",
      "[step: 390] loss: 0.7729474902153015\n",
      "[step: 391] loss: 0.7675751447677612\n",
      "[step: 392] loss: 0.7673898935317993\n",
      "[step: 393] loss: 0.7655505537986755\n",
      "[step: 394] loss: 0.7602002620697021\n",
      "[step: 395] loss: 0.7580451369285583\n",
      "[step: 396] loss: 0.7569073438644409\n",
      "[step: 397] loss: 0.752676248550415\n",
      "[step: 398] loss: 0.7497445344924927\n",
      "[step: 399] loss: 0.7488898634910583\n",
      "[step: 400] loss: 0.7457783818244934\n",
      "[step: 401] loss: 0.742563009262085\n",
      "[step: 402] loss: 0.7412676215171814\n",
      "[step: 403] loss: 0.7386918067932129\n",
      "[step: 404] loss: 0.7355313897132874\n",
      "[step: 405] loss: 0.7340109944343567\n",
      "[step: 406] loss: 0.7319303154945374\n",
      "[step: 407] loss: 0.729118287563324\n",
      "[step: 408] loss: 0.727493166923523\n",
      "[step: 409] loss: 0.7256165146827698\n",
      "[step: 410] loss: 0.7230382561683655\n",
      "[step: 411] loss: 0.7213250398635864\n",
      "[step: 412] loss: 0.7195344567298889\n",
      "[step: 413] loss: 0.7171807289123535\n",
      "[step: 414] loss: 0.7154794931411743\n",
      "[step: 415] loss: 0.7137158513069153\n",
      "[step: 416] loss: 0.7115345001220703\n",
      "[step: 417] loss: 0.7098610401153564\n",
      "[step: 418] loss: 0.7080774903297424\n",
      "[step: 419] loss: 0.7060192823410034\n",
      "[step: 420] loss: 0.704368531703949\n",
      "[step: 421] loss: 0.7025412917137146\n",
      "[step: 422] loss: 0.7005949020385742\n",
      "[step: 423] loss: 0.6989579200744629\n",
      "[step: 424] loss: 0.6971074342727661\n",
      "[step: 425] loss: 0.6952959895133972\n",
      "[step: 426] loss: 0.6936646699905396\n",
      "[step: 427] loss: 0.6918284893035889\n",
      "[step: 428] loss: 0.6901367902755737\n",
      "[step: 429] loss: 0.6884691715240479\n",
      "[step: 430] loss: 0.686686098575592\n",
      "[step: 431] loss: 0.685072124004364\n",
      "[step: 432] loss: 0.6833808422088623\n",
      "[step: 433] loss: 0.6817240715026855\n",
      "[step: 434] loss: 0.6801545023918152\n",
      "[step: 435] loss: 0.6785027384757996\n",
      "[step: 436] loss: 0.6769682168960571\n",
      "[step: 437] loss: 0.6754185557365417\n",
      "[step: 438] loss: 0.6738906502723694\n",
      "[step: 439] loss: 0.6724348664283752\n",
      "[step: 440] loss: 0.6709326505661011\n",
      "[step: 441] loss: 0.669504702091217\n",
      "[step: 442] loss: 0.6680582761764526\n",
      "[step: 443] loss: 0.6666358709335327\n",
      "[step: 444] loss: 0.6652475595474243\n",
      "[step: 445] loss: 0.6638378500938416\n",
      "[step: 446] loss: 0.662476658821106\n",
      "[step: 447] loss: 0.6610918045043945\n",
      "[step: 448] loss: 0.6597409844398499\n",
      "[step: 449] loss: 0.658393383026123\n",
      "[step: 450] loss: 0.6570549607276917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 451] loss: 0.6557409167289734\n",
      "[step: 452] loss: 0.6544209718704224\n",
      "[step: 453] loss: 0.6531333923339844\n",
      "[step: 454] loss: 0.6518402695655823\n",
      "[step: 455] loss: 0.6505742073059082\n",
      "[step: 456] loss: 0.6493076682090759\n",
      "[step: 457] loss: 0.6480568051338196\n",
      "[step: 458] loss: 0.6468110084533691\n",
      "[step: 459] loss: 0.645571768283844\n",
      "[step: 460] loss: 0.6443415880203247\n",
      "[step: 461] loss: 0.6431121826171875\n",
      "[step: 462] loss: 0.6418929100036621\n",
      "[step: 463] loss: 0.6406702995300293\n",
      "[step: 464] loss: 0.6394556760787964\n",
      "[step: 465] loss: 0.6382337808609009\n",
      "[step: 466] loss: 0.6370176076889038\n",
      "[step: 467] loss: 0.6357982158660889\n",
      "[step: 468] loss: 0.6345849633216858\n",
      "[step: 469] loss: 0.6333680748939514\n",
      "[step: 470] loss: 0.6321561932563782\n",
      "[step: 471] loss: 0.6309410333633423\n",
      "[step: 472] loss: 0.6297296285629272\n",
      "[step: 473] loss: 0.62851482629776\n",
      "[step: 474] loss: 0.6273024678230286\n",
      "[step: 475] loss: 0.6260876655578613\n",
      "[step: 476] loss: 0.6248736381530762\n",
      "[step: 477] loss: 0.6236562728881836\n",
      "[step: 478] loss: 0.6224385499954224\n",
      "[step: 479] loss: 0.6212162971496582\n",
      "[step: 480] loss: 0.6199928522109985\n",
      "[step: 481] loss: 0.618765652179718\n",
      "[step: 482] loss: 0.6175366044044495\n",
      "[step: 483] loss: 0.6163031458854675\n",
      "[step: 484] loss: 0.6150663495063782\n",
      "[step: 485] loss: 0.6138244867324829\n",
      "[step: 486] loss: 0.612577497959137\n",
      "[step: 487] loss: 0.6113249063491821\n",
      "[step: 488] loss: 0.6100651621818542\n",
      "[step: 489] loss: 0.608799397945404\n",
      "[step: 490] loss: 0.6075257062911987\n",
      "[step: 491] loss: 0.6062459349632263\n",
      "[step: 492] loss: 0.6049590110778809\n",
      "[step: 493] loss: 0.6036648750305176\n",
      "[step: 494] loss: 0.6023634672164917\n",
      "[step: 495] loss: 0.601054310798645\n",
      "[step: 496] loss: 0.5997365117073059\n",
      "[step: 497] loss: 0.598409116268158\n",
      "[step: 498] loss: 0.5970725417137146\n",
      "[step: 499] loss: 0.5957258343696594\n",
      "[step: 500] loss: 0.5943703651428223\n",
      "[step: 501] loss: 0.5930073857307434\n",
      "[step: 502] loss: 0.5916370749473572\n",
      "[step: 503] loss: 0.5902597904205322\n",
      "[step: 504] loss: 0.5888754725456238\n",
      "[step: 505] loss: 0.5874841213226318\n",
      "[step: 506] loss: 0.5860861539840698\n",
      "[step: 507] loss: 0.584682285785675\n",
      "[step: 508] loss: 0.5832719206809998\n",
      "[step: 509] loss: 0.581856369972229\n",
      "[step: 510] loss: 0.5804359316825867\n",
      "[step: 511] loss: 0.5790107250213623\n",
      "[step: 512] loss: 0.5775810480117798\n",
      "[step: 513] loss: 0.5761463046073914\n",
      "[step: 514] loss: 0.5747057199478149\n",
      "[step: 515] loss: 0.5732594132423401\n",
      "[step: 516] loss: 0.5718058943748474\n",
      "[step: 517] loss: 0.5703451633453369\n",
      "[step: 518] loss: 0.568877100944519\n",
      "[step: 519] loss: 0.5674015879631042\n",
      "[step: 520] loss: 0.5659187436103821\n",
      "[step: 521] loss: 0.5644276142120361\n",
      "[step: 522] loss: 0.5629276633262634\n",
      "[step: 523] loss: 0.561417818069458\n",
      "[step: 524] loss: 0.5598987340927124\n",
      "[step: 525] loss: 0.55837082862854\n",
      "[step: 526] loss: 0.5568342208862305\n",
      "[step: 527] loss: 0.5552900433540344\n",
      "[step: 528] loss: 0.553737461566925\n",
      "[step: 529] loss: 0.5521762371063232\n",
      "[step: 530] loss: 0.5506048202514648\n",
      "[step: 531] loss: 0.5490240454673767\n",
      "[step: 532] loss: 0.5474358201026917\n",
      "[step: 533] loss: 0.5458411574363708\n",
      "[step: 534] loss: 0.5442402362823486\n",
      "[step: 535] loss: 0.5426359176635742\n",
      "[step: 536] loss: 0.5410285592079163\n",
      "[step: 537] loss: 0.5394182801246643\n",
      "[step: 538] loss: 0.5378021597862244\n",
      "[step: 539] loss: 0.5361785888671875\n",
      "[step: 540] loss: 0.5345488786697388\n",
      "[step: 541] loss: 0.5329142212867737\n",
      "[step: 542] loss: 0.5312798619270325\n",
      "[step: 543] loss: 0.5296655893325806\n",
      "[step: 544] loss: 0.5281565189361572\n",
      "[step: 545] loss: 0.5271908640861511\n",
      "[step: 546] loss: 0.5291949510574341\n",
      "[step: 547] loss: 0.5486044883728027\n",
      "[step: 548] loss: 0.6686022281646729\n",
      "[step: 549] loss: 1.2669172286987305\n",
      "[step: 550] loss: 2.22019362449646\n",
      "[step: 551] loss: 1.313293695449829\n",
      "[step: 552] loss: 0.953800618648529\n",
      "[step: 553] loss: 1.1293940544128418\n",
      "[step: 554] loss: 0.8453658819198608\n",
      "[step: 555] loss: 0.7241731882095337\n",
      "[step: 556] loss: 0.9347040057182312\n",
      "[step: 557] loss: 0.7993968725204468\n",
      "[step: 558] loss: 0.8909029364585876\n",
      "[step: 559] loss: 0.7131465673446655\n",
      "[step: 560] loss: 0.7567436695098877\n",
      "[step: 561] loss: 0.706298291683197\n",
      "[step: 562] loss: 0.7506956458091736\n",
      "[step: 563] loss: 0.7678512334823608\n",
      "[step: 564] loss: 0.7155166268348694\n",
      "[step: 565] loss: 0.7285745739936829\n",
      "[step: 566] loss: 0.6963107585906982\n",
      "[step: 567] loss: 0.6846883296966553\n",
      "[step: 568] loss: 0.709334671497345\n",
      "[step: 569] loss: 0.6812824010848999\n",
      "[step: 570] loss: 0.6677137017250061\n",
      "[step: 571] loss: 0.671815037727356\n",
      "[step: 572] loss: 0.6638273596763611\n",
      "[step: 573] loss: 0.662726640701294\n",
      "[step: 574] loss: 0.6547849774360657\n",
      "[step: 575] loss: 0.6394984722137451\n",
      "[step: 576] loss: 0.63584965467453\n",
      "[step: 577] loss: 0.6287468075752258\n",
      "[step: 578] loss: 0.6214037537574768\n",
      "[step: 579] loss: 0.6232205033302307\n",
      "[step: 580] loss: 0.6210204362869263\n",
      "[step: 581] loss: 0.6149729490280151\n",
      "[step: 582] loss: 0.6058759093284607\n",
      "[step: 583] loss: 0.5930936336517334\n",
      "[step: 584] loss: 0.5871996879577637\n",
      "[step: 585] loss: 0.5862987041473389\n",
      "[step: 586] loss: 0.5824572443962097\n",
      "[step: 587] loss: 0.5779775381088257\n",
      "[step: 588] loss: 0.5741217136383057\n",
      "[step: 589] loss: 0.5714941024780273\n",
      "[step: 590] loss: 0.5693843960762024\n",
      "[step: 591] loss: 0.5641140341758728\n",
      "[step: 592] loss: 0.5584495663642883\n",
      "[step: 593] loss: 0.5557405948638916\n",
      "[step: 594] loss: 0.55338454246521\n",
      "[step: 595] loss: 0.5499381422996521\n",
      "[step: 596] loss: 0.5456791520118713\n",
      "[step: 597] loss: 0.5424231886863708\n",
      "[step: 598] loss: 0.5404390692710876\n",
      "[step: 599] loss: 0.5378678441047668\n",
      "[step: 600] loss: 0.5361398458480835\n",
      "[step: 601] loss: 0.53448486328125\n",
      "[step: 602] loss: 0.5315167307853699\n",
      "[step: 603] loss: 0.5285430550575256\n",
      "[step: 604] loss: 0.5262574553489685\n",
      "[step: 605] loss: 0.5247415900230408\n",
      "[step: 606] loss: 0.5226650834083557\n",
      "[step: 607] loss: 0.5210205316543579\n",
      "[step: 608] loss: 0.5197160243988037\n",
      "[step: 609] loss: 0.5179409384727478\n",
      "[step: 610] loss: 0.5162758827209473\n",
      "[step: 611] loss: 0.5145021080970764\n",
      "[step: 612] loss: 0.5127605199813843\n",
      "[step: 613] loss: 0.5111305713653564\n",
      "[step: 614] loss: 0.5101243257522583\n",
      "[step: 615] loss: 0.5084714889526367\n",
      "[step: 616] loss: 0.5068236589431763\n",
      "[step: 617] loss: 0.505415678024292\n",
      "[step: 618] loss: 0.504132091999054\n",
      "[step: 619] loss: 0.5026379823684692\n",
      "[step: 620] loss: 0.5013719201087952\n",
      "[step: 621] loss: 0.49995335936546326\n",
      "[step: 622] loss: 0.4985826015472412\n",
      "[step: 623] loss: 0.4973774254322052\n",
      "[step: 624] loss: 0.49606308341026306\n",
      "[step: 625] loss: 0.49470508098602295\n",
      "[step: 626] loss: 0.49350520968437195\n",
      "[step: 627] loss: 0.49230077862739563\n",
      "[step: 628] loss: 0.4910430908203125\n",
      "[step: 629] loss: 0.4898819923400879\n",
      "[step: 630] loss: 0.4886435866355896\n",
      "[step: 631] loss: 0.48744505643844604\n",
      "[step: 632] loss: 0.4862329065799713\n",
      "[step: 633] loss: 0.4850294589996338\n",
      "[step: 634] loss: 0.48389488458633423\n",
      "[step: 635] loss: 0.4827716052532196\n",
      "[step: 636] loss: 0.4815654158592224\n",
      "[step: 637] loss: 0.48042386770248413\n",
      "[step: 638] loss: 0.4792685806751251\n",
      "[step: 639] loss: 0.4781486988067627\n",
      "[step: 640] loss: 0.47702816128730774\n",
      "[step: 641] loss: 0.47588303685188293\n",
      "[step: 642] loss: 0.47475653886795044\n",
      "[step: 643] loss: 0.473646879196167\n",
      "[step: 644] loss: 0.472514808177948\n",
      "[step: 645] loss: 0.47141551971435547\n",
      "[step: 646] loss: 0.4703100025653839\n",
      "[step: 647] loss: 0.46922191977500916\n",
      "[step: 648] loss: 0.4681241512298584\n",
      "[step: 649] loss: 0.46702563762664795\n",
      "[step: 650] loss: 0.465947687625885\n",
      "[step: 651] loss: 0.46485745906829834\n",
      "[step: 652] loss: 0.46378037333488464\n",
      "[step: 653] loss: 0.4627017676830292\n",
      "[step: 654] loss: 0.46162521839141846\n",
      "[step: 655] loss: 0.46054622530937195\n",
      "[step: 656] loss: 0.4594665467739105\n",
      "[step: 657] loss: 0.4583952724933624\n",
      "[step: 658] loss: 0.4573204815387726\n",
      "[step: 659] loss: 0.4562424123287201\n",
      "[step: 660] loss: 0.4551640748977661\n",
      "[step: 661] loss: 0.45408955216407776\n",
      "[step: 662] loss: 0.4530160427093506\n",
      "[step: 663] loss: 0.4519388675689697\n",
      "[step: 664] loss: 0.4508650004863739\n",
      "[step: 665] loss: 0.449788898229599\n",
      "[step: 666] loss: 0.44871407747268677\n",
      "[step: 667] loss: 0.447637677192688\n",
      "[step: 668] loss: 0.446563184261322\n",
      "[step: 669] loss: 0.4454842805862427\n",
      "[step: 670] loss: 0.4444064497947693\n",
      "[step: 671] loss: 0.4433242976665497\n",
      "[step: 672] loss: 0.442241370677948\n",
      "[step: 673] loss: 0.44115445017814636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 674] loss: 0.44006696343421936\n",
      "[step: 675] loss: 0.4389769434928894\n",
      "[step: 676] loss: 0.4378853440284729\n",
      "[step: 677] loss: 0.43679338693618774\n",
      "[step: 678] loss: 0.43570151925086975\n",
      "[step: 679] loss: 0.43460750579833984\n",
      "[step: 680] loss: 0.4335130453109741\n",
      "[step: 681] loss: 0.43241915106773376\n",
      "[step: 682] loss: 0.4313254952430725\n",
      "[step: 683] loss: 0.43023091554641724\n",
      "[step: 684] loss: 0.42913520336151123\n",
      "[step: 685] loss: 0.42803749442100525\n",
      "[step: 686] loss: 0.42693981528282166\n",
      "[step: 687] loss: 0.4258403182029724\n",
      "[step: 688] loss: 0.42473867535591125\n",
      "[step: 689] loss: 0.4236338436603546\n",
      "[step: 690] loss: 0.4225262999534607\n",
      "[step: 691] loss: 0.4214158356189728\n",
      "[step: 692] loss: 0.420301228761673\n",
      "[step: 693] loss: 0.41918399930000305\n",
      "[step: 694] loss: 0.4180646240711212\n",
      "[step: 695] loss: 0.416943222284317\n",
      "[step: 696] loss: 0.41581931710243225\n",
      "[step: 697] loss: 0.4146929085254669\n",
      "[step: 698] loss: 0.4135635197162628\n",
      "[step: 699] loss: 0.41243186593055725\n",
      "[step: 700] loss: 0.4112979471683502\n",
      "[step: 701] loss: 0.4101615846157074\n",
      "[step: 702] loss: 0.40902358293533325\n",
      "[step: 703] loss: 0.40788453817367554\n",
      "[step: 704] loss: 0.40674522519111633\n",
      "[step: 705] loss: 0.40560653805732727\n",
      "[step: 706] loss: 0.40446770191192627\n",
      "[step: 707] loss: 0.40332841873168945\n",
      "[step: 708] loss: 0.40218859910964966\n",
      "[step: 709] loss: 0.40104806423187256\n",
      "[step: 710] loss: 0.39990732073783875\n",
      "[step: 711] loss: 0.3987659215927124\n",
      "[step: 712] loss: 0.3976227939128876\n",
      "[step: 713] loss: 0.3964773714542389\n",
      "[step: 714] loss: 0.3953290283679962\n",
      "[step: 715] loss: 0.3941764533519745\n",
      "[step: 716] loss: 0.3930203914642334\n",
      "[step: 717] loss: 0.39185991883277893\n",
      "[step: 718] loss: 0.3906952738761902\n",
      "[step: 719] loss: 0.3895267844200134\n",
      "[step: 720] loss: 0.38835448026657104\n",
      "[step: 721] loss: 0.38717854022979736\n",
      "[step: 722] loss: 0.38599923253059387\n",
      "[step: 723] loss: 0.3848174810409546\n",
      "[step: 724] loss: 0.38363200426101685\n",
      "[step: 725] loss: 0.3824443221092224\n",
      "[step: 726] loss: 0.3812572956085205\n",
      "[step: 727] loss: 0.38008391857147217\n",
      "[step: 728] loss: 0.37900277972221375\n",
      "[step: 729] loss: 0.37852007150650024\n",
      "[step: 730] loss: 0.38212859630584717\n",
      "[step: 731] loss: 0.4146406352519989\n",
      "[step: 732] loss: 0.6469498872756958\n",
      "[step: 733] loss: 1.6020203828811646\n",
      "[step: 734] loss: 2.4130163192749023\n",
      "[step: 735] loss: 8.805347442626953\n",
      "[step: 736] loss: 10.960765838623047\n",
      "[step: 737] loss: 38.59629440307617\n",
      "[step: 738] loss: 39.6589469909668\n",
      "[step: 739] loss: 7.763294696807861\n",
      "[step: 740] loss: 12.306591033935547\n",
      "[step: 741] loss: 4.045598030090332\n",
      "[step: 742] loss: 2.9131011962890625\n",
      "[step: 743] loss: 5.524507522583008\n",
      "[step: 744] loss: 10.462920188903809\n",
      "[step: 745] loss: 6.347891330718994\n",
      "[step: 746] loss: 3.575273275375366\n",
      "[step: 747] loss: 7.327553749084473\n",
      "[step: 748] loss: 4.470046520233154\n",
      "[step: 749] loss: 4.159382343292236\n",
      "[step: 750] loss: 5.064986228942871\n",
      "[step: 751] loss: 2.695634603500366\n",
      "[step: 752] loss: 3.9839396476745605\n",
      "[step: 753] loss: 4.020396709442139\n",
      "[step: 754] loss: 2.5499138832092285\n",
      "[step: 755] loss: 3.3575234413146973\n",
      "[step: 756] loss: 3.5513439178466797\n",
      "[step: 757] loss: 2.513756513595581\n",
      "[step: 758] loss: 2.9150309562683105\n",
      "[step: 759] loss: 3.2829415798187256\n",
      "[step: 760] loss: 2.607968807220459\n",
      "[step: 761] loss: 2.53661847114563\n",
      "[step: 762] loss: 2.982421398162842\n",
      "[step: 763] loss: 2.707085609436035\n",
      "[step: 764] loss: 2.3869802951812744\n",
      "[step: 765] loss: 2.6392033100128174\n",
      "[step: 766] loss: 2.7242319583892822\n",
      "[step: 767] loss: 2.4331326484680176\n",
      "[step: 768] loss: 2.3827433586120605\n",
      "[step: 769] loss: 2.5728185176849365\n",
      "[step: 770] loss: 2.5087974071502686\n",
      "[step: 771] loss: 2.32677960395813\n",
      "[step: 772] loss: 2.3769638538360596\n",
      "[step: 773] loss: 2.471702814102173\n",
      "[step: 774] loss: 2.377436399459839\n",
      "[step: 775] loss: 2.2829477787017822\n",
      "[step: 776] loss: 2.3438220024108887\n",
      "[step: 777] loss: 2.376476526260376\n",
      "[step: 778] loss: 2.2891860008239746\n",
      "[step: 779] loss: 2.25243878364563\n",
      "[step: 780] loss: 2.303955316543579\n",
      "[step: 781] loss: 2.29671049118042\n",
      "[step: 782] loss: 2.230020761489868\n",
      "[step: 783] loss: 2.2258832454681396\n",
      "[step: 784] loss: 2.256593704223633\n",
      "[step: 785] loss: 2.2265284061431885\n",
      "[step: 786] loss: 2.1864736080169678\n",
      "[step: 787] loss: 2.2015416622161865\n",
      "[step: 788] loss: 2.2073845863342285\n",
      "[step: 789] loss: 2.1705472469329834\n",
      "[step: 790] loss: 2.1579837799072266\n",
      "[step: 791] loss: 2.1718976497650146\n",
      "[step: 792] loss: 2.154531717300415\n",
      "[step: 793] loss: 2.1298186779022217\n",
      "[step: 794] loss: 2.1354432106018066\n",
      "[step: 795] loss: 2.1315770149230957\n",
      "[step: 796] loss: 2.107266426086426\n",
      "[step: 797] loss: 2.1026105880737305\n",
      "[step: 798] loss: 2.1031196117401123\n",
      "[step: 799] loss: 2.083749532699585\n",
      "[step: 800] loss: 2.0726654529571533\n",
      "[step: 801] loss: 2.072300672531128\n",
      "[step: 802] loss: 2.0570476055145264\n",
      "[step: 803] loss: 2.043415069580078\n",
      "[step: 804] loss: 2.040609359741211\n",
      "[step: 805] loss: 2.0275466442108154\n",
      "[step: 806] loss: 2.0132768154144287\n",
      "[step: 807] loss: 2.0081546306610107\n",
      "[step: 808] loss: 1.9955651760101318\n",
      "[step: 809] loss: 1.981518268585205\n",
      "[step: 810] loss: 1.9745067358016968\n",
      "[step: 811] loss: 1.9611066579818726\n",
      "[step: 812] loss: 1.9475795030593872\n",
      "[step: 813] loss: 1.9388375282287598\n",
      "[step: 814] loss: 1.9240118265151978\n",
      "[step: 815] loss: 1.9112128019332886\n",
      "[step: 816] loss: 1.899929165840149\n",
      "[step: 817] loss: 1.8838884830474854\n",
      "[step: 818] loss: 1.8719159364700317\n",
      "[step: 819] loss: 1.8569798469543457\n",
      "[step: 820] loss: 1.8413313627243042\n",
      "[step: 821] loss: 1.8273069858551025\n",
      "[step: 822] loss: 1.8095850944519043\n",
      "[step: 823] loss: 1.7947417497634888\n",
      "[step: 824] loss: 1.775716781616211\n",
      "[step: 825] loss: 1.7588155269622803\n",
      "[step: 826] loss: 1.738497257232666\n",
      "[step: 827] loss: 1.7198179960250854\n",
      "[step: 828] loss: 1.6974698305130005\n",
      "[step: 829] loss: 1.6767905950546265\n",
      "[step: 830] loss: 1.652577519416809\n",
      "[step: 831] loss: 1.6285433769226074\n",
      "[step: 832] loss: 1.6044361591339111\n",
      "[step: 833] loss: 1.577476978302002\n",
      "[step: 834] loss: 1.550004482269287\n",
      "[step: 835] loss: 1.5233113765716553\n",
      "[step: 836] loss: 1.4982473850250244\n",
      "[step: 837] loss: 1.4812376499176025\n",
      "[step: 838] loss: 1.5134177207946777\n",
      "[step: 839] loss: 1.6387619972229004\n",
      "[step: 840] loss: 1.7786805629730225\n",
      "[step: 841] loss: 1.413859248161316\n",
      "[step: 842] loss: 1.505635380744934\n",
      "[step: 843] loss: 1.5764963626861572\n",
      "[step: 844] loss: 1.3371096849441528\n",
      "[step: 845] loss: 1.5461856126785278\n",
      "[step: 846] loss: 1.3325854539871216\n",
      "[step: 847] loss: 1.4150856733322144\n",
      "[step: 848] loss: 1.3215677738189697\n",
      "[step: 849] loss: 1.332029938697815\n",
      "[step: 850] loss: 1.3071621656417847\n",
      "[step: 851] loss: 1.2788021564483643\n",
      "[step: 852] loss: 1.2781652212142944\n",
      "[step: 853] loss: 1.2359395027160645\n",
      "[step: 854] loss: 1.2523449659347534\n",
      "[step: 855] loss: 1.2103910446166992\n",
      "[step: 856] loss: 1.2285102605819702\n",
      "[step: 857] loss: 1.1882073879241943\n",
      "[step: 858] loss: 1.2068488597869873\n",
      "[step: 859] loss: 1.1740515232086182\n",
      "[step: 860] loss: 1.1899151802062988\n",
      "[step: 861] loss: 1.1624815464019775\n",
      "[step: 862] loss: 1.17533540725708\n",
      "[step: 863] loss: 1.154394268989563\n",
      "[step: 864] loss: 1.1647595167160034\n",
      "[step: 865] loss: 1.1471912860870361\n",
      "[step: 866] loss: 1.1552815437316895\n",
      "[step: 867] loss: 1.140570044517517\n",
      "[step: 868] loss: 1.1474899053573608\n",
      "[step: 869] loss: 1.133759617805481\n",
      "[step: 870] loss: 1.1402571201324463\n",
      "[step: 871] loss: 1.1275148391723633\n",
      "[step: 872] loss: 1.1341321468353271\n",
      "[step: 873] loss: 1.1218055486679077\n",
      "[step: 874] loss: 1.1282211542129517\n",
      "[step: 875] loss: 1.1169939041137695\n",
      "[step: 876] loss: 1.1227638721466064\n",
      "[step: 877] loss: 1.113024115562439\n",
      "[step: 878] loss: 1.1171543598175049\n",
      "[step: 879] loss: 1.1097885370254517\n",
      "[step: 880] loss: 1.1116994619369507\n",
      "[step: 881] loss: 1.107063889503479\n",
      "[step: 882] loss: 1.1062684059143066\n",
      "[step: 883] loss: 1.1043909788131714\n",
      "[step: 884] loss: 1.1012588739395142\n",
      "[step: 885] loss: 1.1014695167541504\n",
      "[step: 886] loss: 1.0971074104309082\n",
      "[step: 887] loss: 1.0981472730636597\n",
      "[step: 888] loss: 1.0939821004867554\n",
      "[step: 889] loss: 1.0943167209625244\n",
      "[step: 890] loss: 1.0916136503219604\n",
      "[step: 891] loss: 1.0903877019882202\n",
      "[step: 892] loss: 1.0894219875335693\n",
      "[step: 893] loss: 1.086902141571045\n",
      "[step: 894] loss: 1.0868529081344604\n",
      "[step: 895] loss: 1.0842159986495972\n",
      "[step: 896] loss: 1.0837218761444092\n",
      "[step: 897] loss: 1.0821013450622559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 898] loss: 1.0805213451385498\n",
      "[step: 899] loss: 1.079923152923584\n",
      "[step: 900] loss: 1.0778956413269043\n",
      "[step: 901] loss: 1.0772837400436401\n",
      "[step: 902] loss: 1.0758864879608154\n",
      "[step: 903] loss: 1.074500560760498\n",
      "[step: 904] loss: 1.0738173723220825\n",
      "[step: 905] loss: 1.0721945762634277\n",
      "[step: 906] loss: 1.0712848901748657\n",
      "[step: 907] loss: 1.0702482461929321\n",
      "[step: 908] loss: 1.0688070058822632\n",
      "[step: 909] loss: 1.0680017471313477\n",
      "[step: 910] loss: 1.0667788982391357\n",
      "[step: 911] loss: 1.065549373626709\n",
      "[step: 912] loss: 1.0647026300430298\n",
      "[step: 913] loss: 1.0634675025939941\n",
      "[step: 914] loss: 1.0623666048049927\n",
      "[step: 915] loss: 1.0614780187606812\n",
      "[step: 916] loss: 1.060293436050415\n",
      "[step: 917] loss: 1.0592362880706787\n",
      "[step: 918] loss: 1.058323860168457\n",
      "[step: 919] loss: 1.0571962594985962\n",
      "[step: 920] loss: 1.0561387538909912\n",
      "[step: 921] loss: 1.0552165508270264\n",
      "[step: 922] loss: 1.0541521310806274\n",
      "[step: 923] loss: 1.0530831813812256\n",
      "[step: 924] loss: 1.0521440505981445\n",
      "[step: 925] loss: 1.051146388053894\n",
      "[step: 926] loss: 1.0500836372375488\n",
      "[step: 927] loss: 1.049107551574707\n",
      "[step: 928] loss: 1.0481575727462769\n",
      "[step: 929] loss: 1.0471354722976685\n",
      "[step: 930] loss: 1.0461182594299316\n",
      "[step: 931] loss: 1.0451630353927612\n",
      "[step: 932] loss: 1.0441982746124268\n",
      "[step: 933] loss: 1.0431898832321167\n",
      "[step: 934] loss: 1.0421947240829468\n",
      "[step: 935] loss: 1.0412379503250122\n",
      "[step: 936] loss: 1.0402764081954956\n",
      "[step: 937] loss: 1.039286494255066\n",
      "[step: 938] loss: 1.0382965803146362\n",
      "[step: 939] loss: 1.0373305082321167\n",
      "[step: 940] loss: 1.036374568939209\n",
      "[step: 941] loss: 1.0354050397872925\n",
      "[step: 942] loss: 1.0344229936599731\n",
      "[step: 943] loss: 1.0334458351135254\n",
      "[step: 944] loss: 1.0324819087982178\n",
      "[step: 945] loss: 1.0315238237380981\n",
      "[step: 946] loss: 1.0305612087249756\n",
      "[step: 947] loss: 1.0295902490615845\n",
      "[step: 948] loss: 1.0286157131195068\n",
      "[step: 949] loss: 1.0276440382003784\n",
      "[step: 950] loss: 1.0266770124435425\n",
      "[step: 951] loss: 1.0257136821746826\n",
      "[step: 952] loss: 1.0247509479522705\n",
      "[step: 953] loss: 1.023787260055542\n",
      "[step: 954] loss: 1.0228215456008911\n",
      "[step: 955] loss: 1.021854281425476\n",
      "[step: 956] loss: 1.0208864212036133\n",
      "[step: 957] loss: 1.0199183225631714\n",
      "[step: 958] loss: 1.0189517736434937\n",
      "[step: 959] loss: 1.0179890394210815\n",
      "[step: 960] loss: 1.0170340538024902\n",
      "[step: 961] loss: 1.016097903251648\n",
      "[step: 962] loss: 1.0152047872543335\n",
      "[step: 963] loss: 1.014416217803955\n",
      "[step: 964] loss: 1.013891577720642\n",
      "[step: 965] loss: 1.0140703916549683\n",
      "[step: 966] loss: 1.0161336660385132\n",
      "[step: 967] loss: 1.0234770774841309\n",
      "[step: 968] loss: 1.0437967777252197\n",
      "[step: 969] loss: 1.0925037860870361\n",
      "[step: 970] loss: 1.1593818664550781\n",
      "[step: 971] loss: 1.1810517311096191\n",
      "[step: 972] loss: 1.0771071910858154\n",
      "[step: 973] loss: 1.0059905052185059\n",
      "[step: 974] loss: 1.0693849325180054\n",
      "[step: 975] loss: 1.0832749605178833\n",
      "[step: 976] loss: 1.0110234022140503\n",
      "[step: 977] loss: 1.030809998512268\n",
      "[step: 978] loss: 1.057998776435852\n",
      "[step: 979] loss: 1.0081186294555664\n",
      "[step: 980] loss: 1.0227857828140259\n",
      "[step: 981] loss: 1.03738534450531\n",
      "[step: 982] loss: 1.002013087272644\n",
      "[step: 983] loss: 1.0211964845657349\n",
      "[step: 984] loss: 1.0202726125717163\n",
      "[step: 985] loss: 0.999290406703949\n",
      "[step: 986] loss: 1.0183250904083252\n",
      "[step: 987] loss: 1.006649136543274\n",
      "[step: 988] loss: 0.9996711611747742\n",
      "[step: 989] loss: 1.0122662782669067\n",
      "[step: 990] loss: 0.9977667927742004\n",
      "[step: 991] loss: 1.0001312494277954\n",
      "[step: 992] loss: 1.0045678615570068\n",
      "[step: 993] loss: 0.992956280708313\n",
      "[step: 994] loss: 0.9988150596618652\n",
      "[step: 995] loss: 0.9974917769432068\n",
      "[step: 996] loss: 0.990275502204895\n",
      "[step: 997] loss: 0.9957929253578186\n",
      "[step: 998] loss: 0.9919918179512024\n",
      "[step: 999] loss: 0.988040566444397\n",
      "[step: 1000] loss: 0.9919986128807068\n",
      "[step: 1001] loss: 0.98786461353302\n",
      "[step: 1002] loss: 0.9855021238327026\n",
      "[step: 1003] loss: 0.988097071647644\n",
      "[step: 1004] loss: 0.9845513105392456\n",
      "[step: 1005] loss: 0.9826104044914246\n",
      "[step: 1006] loss: 0.9843263030052185\n",
      "[step: 1007] loss: 0.9816663265228271\n",
      "[step: 1008] loss: 0.9795466065406799\n",
      "[step: 1009] loss: 0.9806277751922607\n",
      "[step: 1010] loss: 0.9790108799934387\n",
      "[step: 1011] loss: 0.9765899181365967\n",
      "[step: 1012] loss: 0.97687828540802\n",
      "[step: 1013] loss: 0.9763110876083374\n",
      "[step: 1014] loss: 0.974001944065094\n",
      "[step: 1015] loss: 0.9731970429420471\n",
      "[step: 1016] loss: 0.9732012748718262\n",
      "[step: 1017] loss: 0.9717200398445129\n",
      "[step: 1018] loss: 0.9700204730033875\n",
      "[step: 1019] loss: 0.9695685505867004\n",
      "[step: 1020] loss: 0.969089925289154\n",
      "[step: 1021] loss: 0.967619776725769\n",
      "[step: 1022] loss: 0.9662107825279236\n",
      "[step: 1023] loss: 0.9655866622924805\n",
      "[step: 1024] loss: 0.9649951457977295\n",
      "[step: 1025] loss: 0.963797390460968\n",
      "[step: 1026] loss: 0.962420642375946\n",
      "[step: 1027] loss: 0.9614598751068115\n",
      "[step: 1028] loss: 0.9607992768287659\n",
      "[step: 1029] loss: 0.9599606990814209\n",
      "[step: 1030] loss: 0.9588014483451843\n",
      "[step: 1031] loss: 0.9575676321983337\n",
      "[step: 1032] loss: 0.9565228819847107\n",
      "[step: 1033] loss: 0.9556781649589539\n",
      "[step: 1034] loss: 0.9548705220222473\n",
      "[step: 1035] loss: 0.9539620876312256\n",
      "[step: 1036] loss: 0.9529227018356323\n",
      "[step: 1037] loss: 0.9518141746520996\n",
      "[step: 1038] loss: 0.9507037997245789\n",
      "[step: 1039] loss: 0.9496371746063232\n",
      "[step: 1040] loss: 0.9486199021339417\n",
      "[step: 1041] loss: 0.9476441144943237\n",
      "[step: 1042] loss: 0.9467002153396606\n",
      "[step: 1043] loss: 0.9457860589027405\n",
      "[step: 1044] loss: 0.9449241161346436\n",
      "[step: 1045] loss: 0.9441629648208618\n",
      "[step: 1046] loss: 0.9436343312263489\n",
      "[step: 1047] loss: 0.9436437487602234\n",
      "[step: 1048] loss: 0.9449501037597656\n",
      "[step: 1049] loss: 0.9496049880981445\n",
      "[step: 1050] loss: 0.9626297950744629\n",
      "[step: 1051] loss: 0.9978687167167664\n",
      "[step: 1052] loss: 1.0793485641479492\n",
      "[step: 1053] loss: 1.2373344898223877\n",
      "[step: 1054] loss: 1.3540459871292114\n",
      "[step: 1055] loss: 1.2215406894683838\n",
      "[step: 1056] loss: 0.957488477230072\n",
      "[step: 1057] loss: 1.0247912406921387\n",
      "[step: 1058] loss: 1.1377049684524536\n",
      "[step: 1059] loss: 0.9701021909713745\n",
      "[step: 1060] loss: 0.9901765584945679\n",
      "[step: 1061] loss: 1.0602508783340454\n",
      "[step: 1062] loss: 0.9408699870109558\n",
      "[step: 1063] loss: 1.0057141780853271\n",
      "[step: 1064] loss: 0.9903520941734314\n",
      "[step: 1065] loss: 0.9435412883758545\n",
      "[step: 1066] loss: 1.0026181936264038\n",
      "[step: 1067] loss: 0.9394348859786987\n",
      "[step: 1068] loss: 0.973107099533081\n",
      "[step: 1069] loss: 0.9607247114181519\n",
      "[step: 1070] loss: 0.9423558712005615\n",
      "[step: 1071] loss: 0.969907283782959\n",
      "[step: 1072] loss: 0.9324324727058411\n",
      "[step: 1073] loss: 0.9596666097640991\n",
      "[step: 1074] loss: 0.9385011792182922\n",
      "[step: 1075] loss: 0.9419387578964233\n",
      "[step: 1076] loss: 0.9463538527488708\n",
      "[step: 1077] loss: 0.9295891523361206\n",
      "[step: 1078] loss: 0.9459360837936401\n",
      "[step: 1079] loss: 0.9276757836341858\n",
      "[step: 1080] loss: 0.9365835785865784\n",
      "[step: 1081] loss: 0.9318301677703857\n",
      "[step: 1082] loss: 0.9261125326156616\n",
      "[step: 1083] loss: 0.9334257245063782\n",
      "[step: 1084] loss: 0.921659529209137\n",
      "[step: 1085] loss: 0.9283121228218079\n",
      "[step: 1086] loss: 0.9232962131500244\n",
      "[step: 1087] loss: 0.9204074740409851\n",
      "[step: 1088] loss: 0.9242464900016785\n",
      "[step: 1089] loss: 0.9166471362113953\n",
      "[step: 1090] loss: 0.9199300408363342\n",
      "[step: 1091] loss: 0.917557418346405\n",
      "[step: 1092] loss: 0.9140161871910095\n",
      "[step: 1093] loss: 0.9168952703475952\n",
      "[step: 1094] loss: 0.9123284220695496\n",
      "[step: 1095] loss: 0.9121495485305786\n",
      "[step: 1096] loss: 0.9126925468444824\n",
      "[step: 1097] loss: 0.9087466597557068\n",
      "[step: 1098] loss: 0.9096323251724243\n",
      "[step: 1099] loss: 0.9086781740188599\n",
      "[step: 1100] loss: 0.9058308005332947\n",
      "[step: 1101] loss: 0.9066065549850464\n",
      "[step: 1102] loss: 0.9051756858825684\n",
      "[step: 1103] loss: 0.9030081629753113\n",
      "[step: 1104] loss: 0.9033954739570618\n",
      "[step: 1105] loss: 0.9020391702651978\n",
      "[step: 1106] loss: 0.9001367688179016\n",
      "[step: 1107] loss: 0.9001408219337463\n",
      "[step: 1108] loss: 0.8991003036499023\n",
      "[step: 1109] loss: 0.8972856998443604\n",
      "[step: 1110] loss: 0.8968746066093445\n",
      "[step: 1111] loss: 0.8961985111236572\n",
      "[step: 1112] loss: 0.8945404887199402\n",
      "[step: 1113] loss: 0.8936386704444885\n",
      "[step: 1114] loss: 0.8931630253791809\n",
      "[step: 1115] loss: 0.8918879628181458\n",
      "[step: 1116] loss: 0.890580952167511\n",
      "[step: 1117] loss: 0.8899245858192444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1118] loss: 0.8891146183013916\n",
      "[step: 1119] loss: 0.8878172039985657\n",
      "[step: 1120] loss: 0.8867123126983643\n",
      "[step: 1121] loss: 0.8859764933586121\n",
      "[step: 1122] loss: 0.8850716352462769\n",
      "[step: 1123] loss: 0.8838610649108887\n",
      "[step: 1124] loss: 0.8827561140060425\n",
      "[step: 1125] loss: 0.8818999528884888\n",
      "[step: 1126] loss: 0.8810117244720459\n",
      "[step: 1127] loss: 0.8799181580543518\n",
      "[step: 1128] loss: 0.8787640929222107\n",
      "[step: 1129] loss: 0.877740204334259\n",
      "[step: 1130] loss: 0.87682044506073\n",
      "[step: 1131] loss: 0.8758543133735657\n",
      "[step: 1132] loss: 0.8747783303260803\n",
      "[step: 1133] loss: 0.8736448287963867\n",
      "[step: 1134] loss: 0.8725452423095703\n",
      "[step: 1135] loss: 0.8715072274208069\n",
      "[step: 1136] loss: 0.8704990148544312\n",
      "[step: 1137] loss: 0.8694778084754944\n",
      "[step: 1138] loss: 0.8684130311012268\n",
      "[step: 1139] loss: 0.8673122525215149\n",
      "[step: 1140] loss: 0.8661841154098511\n",
      "[step: 1141] loss: 0.8650485873222351\n",
      "[step: 1142] loss: 0.8639090657234192\n",
      "[step: 1143] loss: 0.8627721667289734\n",
      "[step: 1144] loss: 0.86163330078125\n",
      "[step: 1145] loss: 0.8604938387870789\n",
      "[step: 1146] loss: 0.8593495488166809\n",
      "[step: 1147] loss: 0.8582033514976501\n",
      "[step: 1148] loss: 0.8570584654808044\n",
      "[step: 1149] loss: 0.855931282043457\n",
      "[step: 1150] loss: 0.8548593521118164\n",
      "[step: 1151] loss: 0.8539708256721497\n",
      "[step: 1152] loss: 0.8536271452903748\n",
      "[step: 1153] loss: 0.8550504446029663\n",
      "[step: 1154] loss: 0.8622658848762512\n",
      "[step: 1155] loss: 0.889460027217865\n",
      "[step: 1156] loss: 0.9847140312194824\n",
      "[step: 1157] loss: 1.2842762470245361\n",
      "[step: 1158] loss: 1.9110628366470337\n",
      "[step: 1159] loss: 2.231599807739258\n",
      "[step: 1160] loss: 1.1642366647720337\n",
      "[step: 1161] loss: 1.080697774887085\n",
      "[step: 1162] loss: 1.5037566423416138\n",
      "[step: 1163] loss: 0.8702938556671143\n",
      "[step: 1164] loss: 1.3417909145355225\n",
      "[step: 1165] loss: 0.9243953227996826\n",
      "[step: 1166] loss: 1.1733784675598145\n",
      "[step: 1167] loss: 0.9313812851905823\n",
      "[step: 1168] loss: 1.107513427734375\n",
      "[step: 1169] loss: 0.9157067537307739\n",
      "[step: 1170] loss: 1.0733760595321655\n",
      "[step: 1171] loss: 0.892870306968689\n",
      "[step: 1172] loss: 1.0485676527023315\n",
      "[step: 1173] loss: 0.8882328271865845\n",
      "[step: 1174] loss: 1.0073987245559692\n",
      "[step: 1175] loss: 0.9133288860321045\n",
      "[step: 1176] loss: 0.9561084508895874\n",
      "[step: 1177] loss: 0.9383748769760132\n",
      "[step: 1178] loss: 0.9094928503036499\n",
      "[step: 1179] loss: 0.9528847932815552\n",
      "[step: 1180] loss: 0.887427568435669\n",
      "[step: 1181] loss: 0.9423611760139465\n",
      "[step: 1182] loss: 0.8870285153388977\n",
      "[step: 1183] loss: 0.9188071489334106\n",
      "[step: 1184] loss: 0.8986188173294067\n",
      "[step: 1185] loss: 0.8935062885284424\n",
      "[step: 1186] loss: 0.9030542969703674\n",
      "[step: 1187] loss: 0.8771479725837708\n",
      "[step: 1188] loss: 0.9006912112236023\n",
      "[step: 1189] loss: 0.8704140186309814\n",
      "[step: 1190] loss: 0.8906061053276062\n",
      "[step: 1191] loss: 0.869716227054596\n",
      "[step: 1192] loss: 0.8792588710784912\n",
      "[step: 1193] loss: 0.8707204461097717\n",
      "[step: 1194] loss: 0.8681555390357971\n",
      "[step: 1195] loss: 0.8692678809165955\n",
      "[step: 1196] loss: 0.8606751561164856\n",
      "[step: 1197] loss: 0.866704523563385\n",
      "[step: 1198] loss: 0.8552036285400391\n",
      "[step: 1199] loss: 0.8625519871711731\n",
      "[step: 1200] loss: 0.8516660928726196\n",
      "[step: 1201] loss: 0.8583170771598816\n",
      "[step: 1202] loss: 0.8485416173934937\n",
      "[step: 1203] loss: 0.8537476658821106\n",
      "[step: 1204] loss: 0.8458773493766785\n",
      "[step: 1205] loss: 0.8499693870544434\n",
      "[step: 1206] loss: 0.8433243632316589\n",
      "[step: 1207] loss: 0.8462934494018555\n",
      "[step: 1208] loss: 0.8407622575759888\n",
      "[step: 1209] loss: 0.8430293798446655\n",
      "[step: 1210] loss: 0.8381100296974182\n",
      "[step: 1211] loss: 0.8398375511169434\n",
      "[step: 1212] loss: 0.835582971572876\n",
      "[step: 1213] loss: 0.8369758129119873\n",
      "[step: 1214] loss: 0.8330946564674377\n",
      "[step: 1215] loss: 0.8339977264404297\n",
      "[step: 1216] loss: 0.8307066559791565\n",
      "[step: 1217] loss: 0.8310902714729309\n",
      "[step: 1218] loss: 0.8283951282501221\n",
      "[step: 1219] loss: 0.8281305432319641\n",
      "[step: 1220] loss: 0.8262215256690979\n",
      "[step: 1221] loss: 0.8252416849136353\n",
      "[step: 1222] loss: 0.8239845633506775\n",
      "[step: 1223] loss: 0.8223826289176941\n",
      "[step: 1224] loss: 0.8217062950134277\n",
      "[step: 1225] loss: 0.8197417855262756\n",
      "[step: 1226] loss: 0.8192489743232727\n",
      "[step: 1227] loss: 0.8173116445541382\n",
      "[step: 1228] loss: 0.8166388273239136\n",
      "[step: 1229] loss: 0.8150424957275391\n",
      "[step: 1230] loss: 0.8139195442199707\n",
      "[step: 1231] loss: 0.812818169593811\n",
      "[step: 1232] loss: 0.8113132119178772\n",
      "[step: 1233] loss: 0.8104548454284668\n",
      "[step: 1234] loss: 0.8088816404342651\n",
      "[step: 1235] loss: 0.8079240322113037\n",
      "[step: 1236] loss: 0.8066073656082153\n",
      "[step: 1237] loss: 0.8053329586982727\n",
      "[step: 1238] loss: 0.8043086528778076\n",
      "[step: 1239] loss: 0.8028809428215027\n",
      "[step: 1240] loss: 0.8018413782119751\n",
      "[step: 1241] loss: 0.8005672097206116\n",
      "[step: 1242] loss: 0.7993109822273254\n",
      "[step: 1243] loss: 0.7982315421104431\n",
      "[step: 1244] loss: 0.7968930006027222\n",
      "[step: 1245] loss: 0.7957608103752136\n",
      "[step: 1246] loss: 0.7945733666419983\n",
      "[step: 1247] loss: 0.7932848930358887\n",
      "[step: 1248] loss: 0.7921724319458008\n",
      "[step: 1249] loss: 0.7909333109855652\n",
      "[step: 1250] loss: 0.7897003889083862\n",
      "[step: 1251] loss: 0.7885655164718628\n",
      "[step: 1252] loss: 0.7873167991638184\n",
      "[step: 1253] loss: 0.7861089706420898\n",
      "[step: 1254] loss: 0.7849549055099487\n",
      "[step: 1255] loss: 0.7837158441543579\n",
      "[step: 1256] loss: 0.7825145721435547\n",
      "[step: 1257] loss: 0.7813518643379211\n",
      "[step: 1258] loss: 0.780127763748169\n",
      "[step: 1259] loss: 0.7789222002029419\n",
      "[step: 1260] loss: 0.7777565121650696\n",
      "[step: 1261] loss: 0.7765508890151978\n",
      "[step: 1262] loss: 0.7753399014472961\n",
      "[step: 1263] loss: 0.7741653919219971\n",
      "[step: 1264] loss: 0.7729790210723877\n",
      "[step: 1265] loss: 0.7717701196670532\n",
      "[step: 1266] loss: 0.7705795764923096\n",
      "[step: 1267] loss: 0.7694023251533508\n",
      "[step: 1268] loss: 0.7682087421417236\n",
      "[step: 1269] loss: 0.7670060992240906\n",
      "[step: 1270] loss: 0.7658175230026245\n",
      "[step: 1271] loss: 0.764636754989624\n",
      "[step: 1272] loss: 0.7634435296058655\n",
      "[step: 1273] loss: 0.7622432708740234\n",
      "[step: 1274] loss: 0.761048436164856\n",
      "[step: 1275] loss: 0.7598612308502197\n",
      "[step: 1276] loss: 0.7586721181869507\n",
      "[step: 1277] loss: 0.7574760317802429\n",
      "[step: 1278] loss: 0.7562775015830994\n",
      "[step: 1279] loss: 0.7550815343856812\n",
      "[step: 1280] loss: 0.753890335559845\n",
      "[step: 1281] loss: 0.7527019381523132\n",
      "[step: 1282] loss: 0.7515121698379517\n",
      "[step: 1283] loss: 0.7503207921981812\n",
      "[step: 1284] loss: 0.7491276264190674\n",
      "[step: 1285] loss: 0.7479347586631775\n",
      "[step: 1286] loss: 0.7467424869537354\n",
      "[step: 1287] loss: 0.7455517649650574\n",
      "[step: 1288] loss: 0.7443615198135376\n",
      "[step: 1289] loss: 0.7431730628013611\n",
      "[step: 1290] loss: 0.7419850826263428\n",
      "[step: 1291] loss: 0.7407976388931274\n",
      "[step: 1292] loss: 0.7396115660667419\n",
      "[step: 1293] loss: 0.738426923751831\n",
      "[step: 1294] loss: 0.737244725227356\n",
      "[step: 1295] loss: 0.7360691428184509\n",
      "[step: 1296] loss: 0.7349130511283875\n",
      "[step: 1297] loss: 0.7338254451751709\n",
      "[step: 1298] loss: 0.732973575592041\n",
      "[step: 1299] loss: 0.7330286502838135\n",
      "[step: 1300] loss: 0.736666738986969\n",
      "[step: 1301] loss: 0.7553911209106445\n",
      "[step: 1302] loss: 0.8389779329299927\n",
      "[step: 1303] loss: 1.1927069425582886\n",
      "[step: 1304] loss: 2.338473320007324\n",
      "[step: 1305] loss: 3.908665895462036\n",
      "[step: 1306] loss: 1.4859181642532349\n",
      "[step: 1307] loss: 1.3039562702178955\n",
      "[step: 1308] loss: 1.8524882793426514\n",
      "[step: 1309] loss: 1.036915898323059\n",
      "[step: 1310] loss: 1.3651924133300781\n",
      "[step: 1311] loss: 1.0191049575805664\n",
      "[step: 1312] loss: 1.168821096420288\n",
      "[step: 1313] loss: 1.2357627153396606\n",
      "[step: 1314] loss: 0.9853327870368958\n",
      "[step: 1315] loss: 1.2667622566223145\n",
      "[step: 1316] loss: 0.9286282062530518\n",
      "[step: 1317] loss: 1.0662599802017212\n",
      "[step: 1318] loss: 1.0644341707229614\n",
      "[step: 1319] loss: 0.9036816358566284\n",
      "[step: 1320] loss: 1.058510661125183\n",
      "[step: 1321] loss: 0.9369692206382751\n",
      "[step: 1322] loss: 0.9291616082191467\n",
      "[step: 1323] loss: 1.0134798288345337\n",
      "[step: 1324] loss: 0.8993109464645386\n",
      "[step: 1325] loss: 0.9423722624778748\n",
      "[step: 1326] loss: 0.9349698424339294\n",
      "[step: 1327] loss: 0.8557602167129517\n",
      "[step: 1328] loss: 0.9199559092521667\n",
      "[step: 1329] loss: 0.8759506344795227\n",
      "[step: 1330] loss: 0.8602176308631897\n",
      "[step: 1331] loss: 0.8897268772125244\n",
      "[step: 1332] loss: 0.8348267674446106\n",
      "[step: 1333] loss: 0.8643399477005005\n",
      "[step: 1334] loss: 0.857193112373352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1335] loss: 0.8313513398170471\n",
      "[step: 1336] loss: 0.8526769280433655\n",
      "[step: 1337] loss: 0.8195310235023499\n",
      "[step: 1338] loss: 0.8291734457015991\n",
      "[step: 1339] loss: 0.8308810591697693\n",
      "[step: 1340] loss: 0.8110452890396118\n",
      "[step: 1341] loss: 0.8243883848190308\n",
      "[step: 1342] loss: 0.8064067363739014\n",
      "[step: 1343] loss: 0.810251772403717\n",
      "[step: 1344] loss: 0.8117710947990417\n",
      "[step: 1345] loss: 0.797797441482544\n",
      "[step: 1346] loss: 0.8044729232788086\n",
      "[step: 1347] loss: 0.7928877472877502\n",
      "[step: 1348] loss: 0.7951351404190063\n",
      "[step: 1349] loss: 0.7943729162216187\n",
      "[step: 1350] loss: 0.7859318852424622\n",
      "[step: 1351] loss: 0.7900186777114868\n",
      "[step: 1352] loss: 0.7823379635810852\n",
      "[step: 1353] loss: 0.7846376299858093\n",
      "[step: 1354] loss: 0.7813168168067932\n",
      "[step: 1355] loss: 0.7770106196403503\n",
      "[step: 1356] loss: 0.7780131101608276\n",
      "[step: 1357] loss: 0.7728305459022522\n",
      "[step: 1358] loss: 0.7744516134262085\n",
      "[step: 1359] loss: 0.769855260848999\n",
      "[step: 1360] loss: 0.7693161964416504\n",
      "[step: 1361] loss: 0.7680108547210693\n",
      "[step: 1362] loss: 0.7655607461929321\n",
      "[step: 1363] loss: 0.7652809023857117\n",
      "[step: 1364] loss: 0.7615321278572083\n",
      "[step: 1365] loss: 0.7617768049240112\n",
      "[step: 1366] loss: 0.7587617039680481\n",
      "[step: 1367] loss: 0.7581508159637451\n",
      "[step: 1368] loss: 0.7558252811431885\n",
      "[step: 1369] loss: 0.7546842694282532\n",
      "[step: 1370] loss: 0.7534878253936768\n",
      "[step: 1371] loss: 0.7516660094261169\n",
      "[step: 1372] loss: 0.7507075667381287\n",
      "[step: 1373] loss: 0.7487281560897827\n",
      "[step: 1374] loss: 0.7481178045272827\n",
      "[step: 1375] loss: 0.7460657954216003\n",
      "[step: 1376] loss: 0.7453133463859558\n",
      "[step: 1377] loss: 0.7434993386268616\n",
      "[step: 1378] loss: 0.7428067326545715\n",
      "[step: 1379] loss: 0.7410891652107239\n",
      "[step: 1380] loss: 0.7402054071426392\n",
      "[step: 1381] loss: 0.7387025952339172\n",
      "[step: 1382] loss: 0.7378106713294983\n",
      "[step: 1383] loss: 0.7363786101341248\n",
      "[step: 1384] loss: 0.7353993654251099\n",
      "[step: 1385] loss: 0.7341342568397522\n",
      "[step: 1386] loss: 0.7331910133361816\n",
      "[step: 1387] loss: 0.7319521307945251\n",
      "[step: 1388] loss: 0.7309994697570801\n",
      "[step: 1389] loss: 0.7298617362976074\n",
      "[step: 1390] loss: 0.7289283871650696\n",
      "[step: 1391] loss: 0.7277928590774536\n",
      "[step: 1392] loss: 0.7268821597099304\n",
      "[step: 1393] loss: 0.7258129119873047\n",
      "[step: 1394] loss: 0.7249165177345276\n",
      "[step: 1395] loss: 0.7238626480102539\n",
      "[step: 1396] loss: 0.723010241985321\n",
      "[step: 1397] loss: 0.7220109701156616\n",
      "[step: 1398] loss: 0.721162736415863\n",
      "[step: 1399] loss: 0.7201935648918152\n",
      "[step: 1400] loss: 0.7193703651428223\n",
      "[step: 1401] loss: 0.7184395790100098\n",
      "[step: 1402] loss: 0.7176064848899841\n",
      "[step: 1403] loss: 0.7167210578918457\n",
      "[step: 1404] loss: 0.7158979773521423\n",
      "[step: 1405] loss: 0.7150484919548035\n",
      "[step: 1406] loss: 0.7142189741134644\n",
      "[step: 1407] loss: 0.7134114503860474\n",
      "[step: 1408] loss: 0.7125848531723022\n",
      "[step: 1409] loss: 0.7117969393730164\n",
      "[step: 1410] loss: 0.710980236530304\n",
      "[step: 1411] loss: 0.7102144360542297\n",
      "[step: 1412] loss: 0.709413468837738\n",
      "[step: 1413] loss: 0.7086493968963623\n",
      "[step: 1414] loss: 0.7078732848167419\n",
      "[step: 1415] loss: 0.7071096301078796\n",
      "[step: 1416] loss: 0.7063514590263367\n",
      "[step: 1417] loss: 0.7055895924568176\n",
      "[step: 1418] loss: 0.704849898815155\n",
      "[step: 1419] loss: 0.704095184803009\n",
      "[step: 1420] loss: 0.7033617496490479\n",
      "[step: 1421] loss: 0.7026222944259644\n",
      "[step: 1422] loss: 0.7018907070159912\n",
      "[step: 1423] loss: 0.7011645436286926\n",
      "[step: 1424] loss: 0.7004362344741821\n",
      "[step: 1425] loss: 0.6997215151786804\n",
      "[step: 1426] loss: 0.6989999413490295\n",
      "[step: 1427] loss: 0.6982883214950562\n",
      "[step: 1428] loss: 0.6975783109664917\n",
      "[step: 1429] loss: 0.6968687772750854\n",
      "[step: 1430] loss: 0.6961671710014343\n",
      "[step: 1431] loss: 0.6954630017280579\n",
      "[step: 1432] loss: 0.6947659254074097\n",
      "[step: 1433] loss: 0.6940692067146301\n",
      "[step: 1434] loss: 0.6933740377426147\n",
      "[step: 1435] loss: 0.6926839351654053\n",
      "[step: 1436] loss: 0.6919926404953003\n",
      "[step: 1437] loss: 0.6913055181503296\n",
      "[step: 1438] loss: 0.6906202435493469\n",
      "[step: 1439] loss: 0.6899350881576538\n",
      "[step: 1440] loss: 0.6892537474632263\n",
      "[step: 1441] loss: 0.6885725259780884\n",
      "[step: 1442] loss: 0.6878929138183594\n",
      "[step: 1443] loss: 0.6872153878211975\n",
      "[step: 1444] loss: 0.6865381002426147\n",
      "[step: 1445] loss: 0.6858631372451782\n",
      "[step: 1446] loss: 0.6851893663406372\n",
      "[step: 1447] loss: 0.6845154762268066\n",
      "[step: 1448] loss: 0.6838437914848328\n",
      "[step: 1449] loss: 0.6831721067428589\n",
      "[step: 1450] loss: 0.6825006604194641\n",
      "[step: 1451] loss: 0.6818307638168335\n",
      "[step: 1452] loss: 0.6811606884002686\n",
      "[step: 1453] loss: 0.6804912090301514\n",
      "[step: 1454] loss: 0.6798224449157715\n",
      "[step: 1455] loss: 0.6791530847549438\n",
      "[step: 1456] loss: 0.6784846186637878\n",
      "[step: 1457] loss: 0.6778163313865662\n",
      "[step: 1458] loss: 0.6771483421325684\n",
      "[step: 1459] loss: 0.676479697227478\n",
      "[step: 1460] loss: 0.6758120059967041\n",
      "[step: 1461] loss: 0.6751435995101929\n",
      "[step: 1462] loss: 0.6744753122329712\n",
      "[step: 1463] loss: 0.6738064885139465\n",
      "[step: 1464] loss: 0.6731373071670532\n",
      "[step: 1465] loss: 0.6724675893783569\n",
      "[step: 1466] loss: 0.6717972755432129\n",
      "[step: 1467] loss: 0.6711264848709106\n",
      "[step: 1468] loss: 0.6704548597335815\n",
      "[step: 1469] loss: 0.6697824597358704\n",
      "[step: 1470] loss: 0.6691091656684875\n",
      "[step: 1471] loss: 0.6684349179267883\n",
      "[step: 1472] loss: 0.667759895324707\n",
      "[step: 1473] loss: 0.6670841574668884\n",
      "[step: 1474] loss: 0.6664069294929504\n",
      "[step: 1475] loss: 0.665727972984314\n",
      "[step: 1476] loss: 0.6650477051734924\n",
      "[step: 1477] loss: 0.6643655896186829\n",
      "[step: 1478] loss: 0.6636815071105957\n",
      "[step: 1479] loss: 0.6629949808120728\n",
      "[step: 1480] loss: 0.6623063087463379\n",
      "[step: 1481] loss: 0.6616151332855225\n",
      "[step: 1482] loss: 0.6609217524528503\n",
      "[step: 1483] loss: 0.6602252721786499\n",
      "[step: 1484] loss: 0.6595262885093689\n",
      "[step: 1485] loss: 0.6588245630264282\n",
      "[step: 1486] loss: 0.6581194996833801\n",
      "[step: 1487] loss: 0.6574136018753052\n",
      "[step: 1488] loss: 0.6567084789276123\n",
      "[step: 1489] loss: 0.6560050249099731\n",
      "[step: 1490] loss: 0.6553019881248474\n",
      "[step: 1491] loss: 0.6546009182929993\n",
      "[step: 1492] loss: 0.6539003252983093\n",
      "[step: 1493] loss: 0.6532009243965149\n",
      "[step: 1494] loss: 0.6525022387504578\n",
      "[step: 1495] loss: 0.6518046855926514\n",
      "[step: 1496] loss: 0.6511098742485046\n",
      "[step: 1497] loss: 0.6504189968109131\n",
      "[step: 1498] loss: 0.6497318744659424\n",
      "[step: 1499] loss: 0.6490482687950134\n",
      "[step: 1500] loss: 0.6483680009841919\n",
      "[step: 1501] loss: 0.647691011428833\n",
      "[step: 1502] loss: 0.6470168828964233\n",
      "[step: 1503] loss: 0.6463449001312256\n",
      "[step: 1504] loss: 0.6456760168075562\n",
      "[step: 1505] loss: 0.6450093388557434\n",
      "[step: 1506] loss: 0.644345223903656\n",
      "[step: 1507] loss: 0.6436827778816223\n",
      "[step: 1508] loss: 0.6430227160453796\n",
      "[step: 1509] loss: 0.642363965511322\n",
      "[step: 1510] loss: 0.6417062878608704\n",
      "[step: 1511] loss: 0.6410499811172485\n",
      "[step: 1512] loss: 0.640394389629364\n",
      "[step: 1513] loss: 0.6397385597229004\n",
      "[step: 1514] loss: 0.6390842795372009\n",
      "[step: 1515] loss: 0.6384300589561462\n",
      "[step: 1516] loss: 0.6377766132354736\n",
      "[step: 1517] loss: 0.6371232867240906\n",
      "[step: 1518] loss: 0.6364699006080627\n",
      "[step: 1519] loss: 0.6358171105384827\n",
      "[step: 1520] loss: 0.6351641416549683\n",
      "[step: 1521] loss: 0.6345118880271912\n",
      "[step: 1522] loss: 0.6338598728179932\n",
      "[step: 1523] loss: 0.6332091689109802\n",
      "[step: 1524] loss: 0.6325611472129822\n",
      "[step: 1525] loss: 0.6319217681884766\n",
      "[step: 1526] loss: 0.631307065486908\n",
      "[step: 1527] loss: 0.6307677030563354\n",
      "[step: 1528] loss: 0.6304596066474915\n",
      "[step: 1529] loss: 0.6308871507644653\n",
      "[step: 1530] loss: 0.6337227821350098\n",
      "[step: 1531] loss: 0.6447398066520691\n",
      "[step: 1532] loss: 0.6833664178848267\n",
      "[step: 1533] loss: 0.8166781067848206\n",
      "[step: 1534] loss: 1.2049601078033447\n",
      "[step: 1535] loss: 2.06496000289917\n",
      "[step: 1536] loss: 2.0200672149658203\n",
      "[step: 1537] loss: 1.1536376476287842\n",
      "[step: 1538] loss: 1.0176600217819214\n",
      "[step: 1539] loss: 1.2972118854522705\n",
      "[step: 1540] loss: 0.7163600325584412\n",
      "[step: 1541] loss: 1.1168956756591797\n",
      "[step: 1542] loss: 0.8908848762512207\n",
      "[step: 1543] loss: 0.9070056676864624\n",
      "[step: 1544] loss: 0.8498895764350891\n",
      "[step: 1545] loss: 0.8661028742790222\n",
      "[step: 1546] loss: 0.8020755648612976\n",
      "[step: 1547] loss: 0.8717893362045288\n",
      "[step: 1548] loss: 0.7622737884521484\n",
      "[step: 1549] loss: 0.8570814728736877\n",
      "[step: 1550] loss: 0.7099775075912476\n",
      "[step: 1551] loss: 0.8034979104995728\n",
      "[step: 1552] loss: 0.7045233249664307\n",
      "[step: 1553] loss: 0.8021981120109558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1554] loss: 0.7087688446044922\n",
      "[step: 1555] loss: 0.7759873867034912\n",
      "[step: 1556] loss: 0.7018002271652222\n",
      "[step: 1557] loss: 0.7459796071052551\n",
      "[step: 1558] loss: 0.6943639516830444\n",
      "[step: 1559] loss: 0.7293142676353455\n",
      "[step: 1560] loss: 0.7016360759735107\n",
      "[step: 1561] loss: 0.7172163724899292\n",
      "[step: 1562] loss: 0.7013932466506958\n",
      "[step: 1563] loss: 0.6985953450202942\n",
      "[step: 1564] loss: 0.6916354298591614\n",
      "[step: 1565] loss: 0.6825239658355713\n",
      "[step: 1566] loss: 0.6868599653244019\n",
      "[step: 1567] loss: 0.677264392375946\n",
      "[step: 1568] loss: 0.6830055117607117\n",
      "[step: 1569] loss: 0.6725268363952637\n",
      "[step: 1570] loss: 0.676547646522522\n",
      "[step: 1571] loss: 0.6651793718338013\n",
      "[step: 1572] loss: 0.6696350574493408\n",
      "[step: 1573] loss: 0.6617909073829651\n",
      "[step: 1574] loss: 0.6659363508224487\n",
      "[step: 1575] loss: 0.6587874889373779\n",
      "[step: 1576] loss: 0.6606031060218811\n",
      "[step: 1577] loss: 0.6545188426971436\n",
      "[step: 1578] loss: 0.6542750000953674\n",
      "[step: 1579] loss: 0.6514165997505188\n",
      "[step: 1580] loss: 0.6498679518699646\n",
      "[step: 1581] loss: 0.6500279307365417\n",
      "[step: 1582] loss: 0.6451833844184875\n",
      "[step: 1583] loss: 0.6471144556999207\n",
      "[step: 1584] loss: 0.640706479549408\n",
      "[step: 1585] loss: 0.6436148285865784\n",
      "[step: 1586] loss: 0.6387052536010742\n",
      "[step: 1587] loss: 0.6404150724411011\n",
      "[step: 1588] loss: 0.6374641060829163\n",
      "[step: 1589] loss: 0.6361635327339172\n",
      "[step: 1590] loss: 0.6359037160873413\n",
      "[step: 1591] loss: 0.6326864361763\n",
      "[step: 1592] loss: 0.6340456008911133\n",
      "[step: 1593] loss: 0.6311415433883667\n",
      "[step: 1594] loss: 0.6307452917098999\n",
      "[step: 1595] loss: 0.6299266219139099\n",
      "[step: 1596] loss: 0.6277590990066528\n",
      "[step: 1597] loss: 0.6283161044120789\n",
      "[step: 1598] loss: 0.626438319683075\n",
      "[step: 1599] loss: 0.6259937286376953\n",
      "[step: 1600] loss: 0.6254239678382874\n",
      "[step: 1601] loss: 0.6237644553184509\n",
      "[step: 1602] loss: 0.623810887336731\n",
      "[step: 1603] loss: 0.6226160526275635\n",
      "[step: 1604] loss: 0.6217480301856995\n",
      "[step: 1605] loss: 0.6214980483055115\n",
      "[step: 1606] loss: 0.6201857924461365\n",
      "[step: 1607] loss: 0.6197386384010315\n",
      "[step: 1608] loss: 0.6192160844802856\n",
      "[step: 1609] loss: 0.6181506514549255\n",
      "[step: 1610] loss: 0.6178106069564819\n",
      "[step: 1611] loss: 0.6170910000801086\n",
      "[step: 1612] loss: 0.6162347197532654\n",
      "[step: 1613] loss: 0.6158962249755859\n",
      "[step: 1614] loss: 0.615153431892395\n",
      "[step: 1615] loss: 0.6143999695777893\n",
      "[step: 1616] loss: 0.6139820218086243\n",
      "[step: 1617] loss: 0.6132654547691345\n",
      "[step: 1618] loss: 0.6125996112823486\n",
      "[step: 1619] loss: 0.6121512055397034\n",
      "[step: 1620] loss: 0.6114541292190552\n",
      "[step: 1621] loss: 0.6107938289642334\n",
      "[step: 1622] loss: 0.6103108525276184\n",
      "[step: 1623] loss: 0.6096646189689636\n",
      "[step: 1624] loss: 0.6090213060379028\n",
      "[step: 1625] loss: 0.6085085868835449\n",
      "[step: 1626] loss: 0.6078824996948242\n",
      "[step: 1627] loss: 0.6072365045547485\n",
      "[step: 1628] loss: 0.6067087054252625\n",
      "[step: 1629] loss: 0.6061221361160278\n",
      "[step: 1630] loss: 0.6054837703704834\n",
      "[step: 1631] loss: 0.6049267053604126\n",
      "[step: 1632] loss: 0.6043614149093628\n",
      "[step: 1633] loss: 0.6037407517433167\n",
      "[step: 1634] loss: 0.6031611561775208\n",
      "[step: 1635] loss: 0.6026081442832947\n",
      "[step: 1636] loss: 0.6020114421844482\n",
      "[step: 1637] loss: 0.6014203429222107\n",
      "[step: 1638] loss: 0.6008687615394592\n",
      "[step: 1639] loss: 0.6003008484840393\n",
      "[step: 1640] loss: 0.5997115969657898\n",
      "[step: 1641] loss: 0.5991465449333191\n",
      "[step: 1642] loss: 0.5985934138298035\n",
      "[step: 1643] loss: 0.5980187058448792\n",
      "[step: 1644] loss: 0.5974361896514893\n",
      "[step: 1645] loss: 0.5968711972236633\n",
      "[step: 1646] loss: 0.596307098865509\n",
      "[step: 1647] loss: 0.5957293510437012\n",
      "[step: 1648] loss: 0.5951530933380127\n",
      "[step: 1649] loss: 0.5945896506309509\n",
      "[step: 1650] loss: 0.594027578830719\n",
      "[step: 1651] loss: 0.593457818031311\n",
      "[step: 1652] loss: 0.5928882360458374\n",
      "[step: 1653] loss: 0.5923249125480652\n",
      "[step: 1654] loss: 0.5917624831199646\n",
      "[step: 1655] loss: 0.5911954045295715\n",
      "[step: 1656] loss: 0.5906273126602173\n",
      "[step: 1657] loss: 0.5900638699531555\n",
      "[step: 1658] loss: 0.5895054936408997\n",
      "[step: 1659] loss: 0.5889465808868408\n",
      "[step: 1660] loss: 0.5883846282958984\n",
      "[step: 1661] loss: 0.5878225564956665\n",
      "[step: 1662] loss: 0.5872626900672913\n",
      "[step: 1663] loss: 0.5867058634757996\n",
      "[step: 1664] loss: 0.5861499905586243\n",
      "[step: 1665] loss: 0.5855944752693176\n",
      "[step: 1666] loss: 0.5850384831428528\n",
      "[step: 1667] loss: 0.5844832062721252\n",
      "[step: 1668] loss: 0.5839299559593201\n",
      "[step: 1669] loss: 0.5833780765533447\n",
      "[step: 1670] loss: 0.582827627658844\n",
      "[step: 1671] loss: 0.5822771191596985\n",
      "[step: 1672] loss: 0.5817267894744873\n",
      "[step: 1673] loss: 0.581176221370697\n",
      "[step: 1674] loss: 0.5806271433830261\n",
      "[step: 1675] loss: 0.5800795555114746\n",
      "[step: 1676] loss: 0.5795323252677917\n",
      "[step: 1677] loss: 0.5789850950241089\n",
      "[step: 1678] loss: 0.5784376859664917\n",
      "[step: 1679] loss: 0.5778904557228088\n",
      "[step: 1680] loss: 0.5773441195487976\n",
      "[step: 1681] loss: 0.5767967104911804\n",
      "[step: 1682] loss: 0.576248824596405\n",
      "[step: 1683] loss: 0.5757012963294983\n",
      "[step: 1684] loss: 0.5751535892486572\n",
      "[step: 1685] loss: 0.5746052861213684\n",
      "[step: 1686] loss: 0.5740579962730408\n",
      "[step: 1687] loss: 0.573510468006134\n",
      "[step: 1688] loss: 0.5729649662971497\n",
      "[step: 1689] loss: 0.5724225640296936\n",
      "[step: 1690] loss: 0.5718891620635986\n",
      "[step: 1691] loss: 0.5713769197463989\n",
      "[step: 1692] loss: 0.5709149241447449\n",
      "[step: 1693] loss: 0.5705758333206177\n",
      "[step: 1694] loss: 0.5705549120903015\n",
      "[step: 1695] loss: 0.5713607668876648\n",
      "[step: 1696] loss: 0.5744381546974182\n",
      "[step: 1697] loss: 0.5837909579277039\n",
      "[step: 1698] loss: 0.6114014983177185\n",
      "[step: 1699] loss: 0.6893587708473206\n",
      "[step: 1700] loss: 0.9055497050285339\n",
      "[step: 1701] loss: 1.345834493637085\n",
      "[step: 1702] loss: 1.9201185703277588\n",
      "[step: 1703] loss: 1.206343173980713\n",
      "[step: 1704] loss: 0.6993661522865295\n",
      "[step: 1705] loss: 1.067344307899475\n",
      "[step: 1706] loss: 0.7387745976448059\n",
      "[step: 1707] loss: 0.7086564302444458\n",
      "[step: 1708] loss: 0.9874529242515564\n",
      "[step: 1709] loss: 0.7027117609977722\n",
      "[step: 1710] loss: 0.7274492383003235\n",
      "[step: 1711] loss: 0.7566997408866882\n",
      "[step: 1712] loss: 0.7048121094703674\n",
      "[step: 1713] loss: 0.7461473941802979\n",
      "[step: 1714] loss: 0.6524360179901123\n",
      "[step: 1715] loss: 0.7237125039100647\n",
      "[step: 1716] loss: 0.6288381218910217\n",
      "[step: 1717] loss: 0.7221997976303101\n",
      "[step: 1718] loss: 0.6201333999633789\n",
      "[step: 1719] loss: 0.6907826662063599\n",
      "[step: 1720] loss: 0.6179648041725159\n",
      "[step: 1721] loss: 0.662793755531311\n",
      "[step: 1722] loss: 0.622246503829956\n",
      "[step: 1723] loss: 0.6482957601547241\n",
      "[step: 1724] loss: 0.6207889914512634\n",
      "[step: 1725] loss: 0.6173359751701355\n",
      "[step: 1726] loss: 0.6251060366630554\n",
      "[step: 1727] loss: 0.6017272472381592\n",
      "[step: 1728] loss: 0.6306978464126587\n",
      "[step: 1729] loss: 0.592680811882019\n",
      "[step: 1730] loss: 0.6125162243843079\n",
      "[step: 1731] loss: 0.5941401720046997\n",
      "[step: 1732] loss: 0.5982749462127686\n",
      "[step: 1733] loss: 0.6021084785461426\n",
      "[step: 1734] loss: 0.5886372327804565\n",
      "[step: 1735] loss: 0.5993557572364807\n",
      "[step: 1736] loss: 0.5836560130119324\n",
      "[step: 1737] loss: 0.5925484299659729\n",
      "[step: 1738] loss: 0.5886412262916565\n",
      "[step: 1739] loss: 0.5818784236907959\n",
      "[step: 1740] loss: 0.5887598991394043\n",
      "[step: 1741] loss: 0.5783481597900391\n",
      "[step: 1742] loss: 0.5824983716011047\n",
      "[step: 1743] loss: 0.5817674994468689\n",
      "[step: 1744] loss: 0.574721097946167\n",
      "[step: 1745] loss: 0.5795150995254517\n",
      "[step: 1746] loss: 0.5745226740837097\n",
      "[step: 1747] loss: 0.5730215311050415\n",
      "[step: 1748] loss: 0.5762550234794617\n",
      "[step: 1749] loss: 0.5704491138458252\n",
      "[step: 1750] loss: 0.570690393447876\n",
      "[step: 1751] loss: 0.5717372894287109\n",
      "[step: 1752] loss: 0.5675446391105652\n",
      "[step: 1753] loss: 0.5687463283538818\n",
      "[step: 1754] loss: 0.5681143403053284\n",
      "[step: 1755] loss: 0.5651152729988098\n",
      "[step: 1756] loss: 0.5662121772766113\n",
      "[step: 1757] loss: 0.5655937194824219\n",
      "[step: 1758] loss: 0.5630965828895569\n",
      "[step: 1759] loss: 0.5634555816650391\n",
      "[step: 1760] loss: 0.5630897283554077\n",
      "[step: 1761] loss: 0.5612187385559082\n",
      "[step: 1762] loss: 0.5614193081855774\n",
      "[step: 1763] loss: 0.560827910900116\n",
      "[step: 1764] loss: 0.5592013597488403\n",
      "[step: 1765] loss: 0.5590831637382507\n",
      "[step: 1766] loss: 0.5588073134422302\n",
      "[step: 1767] loss: 0.5574073791503906\n",
      "[step: 1768] loss: 0.5567911267280579\n",
      "[step: 1769] loss: 0.5567269325256348\n",
      "[step: 1770] loss: 0.5557091236114502\n",
      "[step: 1771] loss: 0.5548768639564514\n",
      "[step: 1772] loss: 0.5546332001686096\n",
      "[step: 1773] loss: 0.5540410280227661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1774] loss: 0.553134560585022\n",
      "[step: 1775] loss: 0.5527009963989258\n",
      "[step: 1776] loss: 0.5523087382316589\n",
      "[step: 1777] loss: 0.5515283942222595\n",
      "[step: 1778] loss: 0.5508483052253723\n",
      "[step: 1779] loss: 0.5504566431045532\n",
      "[step: 1780] loss: 0.5499376058578491\n",
      "[step: 1781] loss: 0.5492016077041626\n",
      "[step: 1782] loss: 0.5486301183700562\n",
      "[step: 1783] loss: 0.5481930375099182\n",
      "[step: 1784] loss: 0.5476454496383667\n",
      "[step: 1785] loss: 0.5469710230827332\n",
      "[step: 1786] loss: 0.5464180707931519\n",
      "[step: 1787] loss: 0.5459490418434143\n",
      "[step: 1788] loss: 0.5454174876213074\n",
      "[step: 1789] loss: 0.5447922945022583\n",
      "[step: 1790] loss: 0.5442212224006653\n",
      "[step: 1791] loss: 0.5437167882919312\n",
      "[step: 1792] loss: 0.5432127118110657\n",
      "[step: 1793] loss: 0.5426406264305115\n",
      "[step: 1794] loss: 0.5420605540275574\n",
      "[step: 1795] loss: 0.5415129661560059\n",
      "[step: 1796] loss: 0.5410045385360718\n",
      "[step: 1797] loss: 0.5404824018478394\n",
      "[step: 1798] loss: 0.5399295091629028\n",
      "[step: 1799] loss: 0.5393568873405457\n",
      "[step: 1800] loss: 0.5388085246086121\n",
      "[step: 1801] loss: 0.5382832288742065\n",
      "[step: 1802] loss: 0.5377644896507263\n",
      "[step: 1803] loss: 0.5372240543365479\n",
      "[step: 1804] loss: 0.5366711616516113\n",
      "[step: 1805] loss: 0.5361143350601196\n",
      "[step: 1806] loss: 0.5355666875839233\n",
      "[step: 1807] loss: 0.5350261330604553\n",
      "[step: 1808] loss: 0.5344944596290588\n",
      "[step: 1809] loss: 0.5339608192443848\n",
      "[step: 1810] loss: 0.533420979976654\n",
      "[step: 1811] loss: 0.5328736901283264\n",
      "[step: 1812] loss: 0.5323227047920227\n",
      "[step: 1813] loss: 0.5317693948745728\n",
      "[step: 1814] loss: 0.531215488910675\n",
      "[step: 1815] loss: 0.5306618809700012\n",
      "[step: 1816] loss: 0.5301080346107483\n",
      "[step: 1817] loss: 0.5295556783676147\n",
      "[step: 1818] loss: 0.5290024876594543\n",
      "[step: 1819] loss: 0.5284481048583984\n",
      "[step: 1820] loss: 0.5278928875923157\n",
      "[step: 1821] loss: 0.5273379683494568\n",
      "[step: 1822] loss: 0.5267813205718994\n",
      "[step: 1823] loss: 0.5262254476547241\n",
      "[step: 1824] loss: 0.5256723761558533\n",
      "[step: 1825] loss: 0.5251266956329346\n",
      "[step: 1826] loss: 0.5245998501777649\n",
      "[step: 1827] loss: 0.524120569229126\n",
      "[step: 1828] loss: 0.5237643718719482\n",
      "[step: 1829] loss: 0.5237275958061218\n",
      "[step: 1830] loss: 0.5245725512504578\n",
      "[step: 1831] loss: 0.5278658270835876\n",
      "[step: 1832] loss: 0.5383560061454773\n",
      "[step: 1833] loss: 0.5694233775138855\n",
      "[step: 1834] loss: 0.6624956130981445\n",
      "[step: 1835] loss: 0.9087172746658325\n",
      "[step: 1836] loss: 1.4778547286987305\n",
      "[step: 1837] loss: 1.7556037902832031\n",
      "[step: 1838] loss: 1.370586633682251\n",
      "[step: 1839] loss: 0.70784592628479\n",
      "[step: 1840] loss: 1.4153302907943726\n",
      "[step: 1841] loss: 1.0529189109802246\n",
      "[step: 1842] loss: 0.7746509313583374\n",
      "[step: 1843] loss: 0.9096859097480774\n",
      "[step: 1844] loss: 0.7275245189666748\n",
      "[step: 1845] loss: 0.8300096392631531\n",
      "[step: 1846] loss: 0.7846861481666565\n",
      "[step: 1847] loss: 0.767277181148529\n",
      "[step: 1848] loss: 0.7211747169494629\n",
      "[step: 1849] loss: 0.7022953629493713\n",
      "[step: 1850] loss: 0.7157765030860901\n",
      "[step: 1851] loss: 0.7114043831825256\n",
      "[step: 1852] loss: 0.6725125908851624\n",
      "[step: 1853] loss: 0.644129753112793\n",
      "[step: 1854] loss: 0.6640422344207764\n",
      "[step: 1855] loss: 0.6721771955490112\n",
      "[step: 1856] loss: 0.657076358795166\n",
      "[step: 1857] loss: 0.6214548945426941\n",
      "[step: 1858] loss: 0.6295475959777832\n",
      "[step: 1859] loss: 0.6375073790550232\n",
      "[step: 1860] loss: 0.6290119290351868\n",
      "[step: 1861] loss: 0.5959190726280212\n",
      "[step: 1862] loss: 0.5897960066795349\n",
      "[step: 1863] loss: 0.5934340953826904\n",
      "[step: 1864] loss: 0.5966438055038452\n",
      "[step: 1865] loss: 0.5832693576812744\n",
      "[step: 1866] loss: 0.5746281743049622\n",
      "[step: 1867] loss: 0.5769196152687073\n",
      "[step: 1868] loss: 0.5740205645561218\n",
      "[step: 1869] loss: 0.5677570700645447\n",
      "[step: 1870] loss: 0.5652651190757751\n",
      "[step: 1871] loss: 0.5657534003257751\n",
      "[step: 1872] loss: 0.5567079782485962\n",
      "[step: 1873] loss: 0.5535726547241211\n",
      "[step: 1874] loss: 0.5585864186286926\n",
      "[step: 1875] loss: 0.555860698223114\n",
      "[step: 1876] loss: 0.5488753914833069\n",
      "[step: 1877] loss: 0.5461620092391968\n",
      "[step: 1878] loss: 0.5475547313690186\n",
      "[step: 1879] loss: 0.5451332330703735\n",
      "[step: 1880] loss: 0.5398855209350586\n",
      "[step: 1881] loss: 0.5404200553894043\n",
      "[step: 1882] loss: 0.5398970246315002\n",
      "[step: 1883] loss: 0.5376157760620117\n",
      "[step: 1884] loss: 0.5387375354766846\n",
      "[step: 1885] loss: 0.5367593169212341\n",
      "[step: 1886] loss: 0.5335277318954468\n",
      "[step: 1887] loss: 0.5336870551109314\n",
      "[step: 1888] loss: 0.5330109000205994\n",
      "[step: 1889] loss: 0.5309455990791321\n",
      "[step: 1890] loss: 0.5300090312957764\n",
      "[step: 1891] loss: 0.5293407440185547\n",
      "[step: 1892] loss: 0.5281744003295898\n",
      "[step: 1893] loss: 0.5271695256233215\n",
      "[step: 1894] loss: 0.5261748433113098\n",
      "[step: 1895] loss: 0.524999737739563\n",
      "[step: 1896] loss: 0.5242780447006226\n",
      "[step: 1897] loss: 0.5239391326904297\n",
      "[step: 1898] loss: 0.5229912996292114\n",
      "[step: 1899] loss: 0.5220714211463928\n",
      "[step: 1900] loss: 0.521597683429718\n",
      "[step: 1901] loss: 0.5207651853561401\n",
      "[step: 1902] loss: 0.5196985006332397\n",
      "[step: 1903] loss: 0.518945038318634\n",
      "[step: 1904] loss: 0.5181503891944885\n",
      "[step: 1905] loss: 0.5173869729042053\n",
      "[step: 1906] loss: 0.5167474746704102\n",
      "[step: 1907] loss: 0.5159719586372375\n",
      "[step: 1908] loss: 0.5152908563613892\n",
      "[step: 1909] loss: 0.5146870017051697\n",
      "[step: 1910] loss: 0.5139369964599609\n",
      "[step: 1911] loss: 0.5132482647895813\n",
      "[step: 1912] loss: 0.5126009583473206\n",
      "[step: 1913] loss: 0.5119450092315674\n",
      "[step: 1914] loss: 0.5113933086395264\n",
      "[step: 1915] loss: 0.5107257962226868\n",
      "[step: 1916] loss: 0.5099984407424927\n",
      "[step: 1917] loss: 0.509475827217102\n",
      "[step: 1918] loss: 0.5088891386985779\n",
      "[step: 1919] loss: 0.5081749558448792\n",
      "[step: 1920] loss: 0.5076026320457458\n",
      "[step: 1921] loss: 0.5070435404777527\n",
      "[step: 1922] loss: 0.5063970685005188\n",
      "[step: 1923] loss: 0.5057799816131592\n",
      "[step: 1924] loss: 0.5051785111427307\n",
      "[step: 1925] loss: 0.5046064853668213\n",
      "[step: 1926] loss: 0.5040384531021118\n",
      "[step: 1927] loss: 0.5034074187278748\n",
      "[step: 1928] loss: 0.5028166770935059\n",
      "[step: 1929] loss: 0.502271294593811\n",
      "[step: 1930] loss: 0.5016778707504272\n",
      "[step: 1931] loss: 0.5010876059532166\n",
      "[step: 1932] loss: 0.5005148649215698\n",
      "[step: 1933] loss: 0.49993962049484253\n",
      "[step: 1934] loss: 0.49938109517097473\n",
      "[step: 1935] loss: 0.49880602955818176\n",
      "[step: 1936] loss: 0.4982259273529053\n",
      "[step: 1937] loss: 0.49766576290130615\n",
      "[step: 1938] loss: 0.49709779024124146\n",
      "[step: 1939] loss: 0.4965364336967468\n",
      "[step: 1940] loss: 0.49598413705825806\n",
      "[step: 1941] loss: 0.4954141676425934\n",
      "[step: 1942] loss: 0.4948462247848511\n",
      "[step: 1943] loss: 0.4942830801010132\n",
      "[step: 1944] loss: 0.49372079968452454\n",
      "[step: 1945] loss: 0.4931654930114746\n",
      "[step: 1946] loss: 0.4926053285598755\n",
      "[step: 1947] loss: 0.49204564094543457\n",
      "[step: 1948] loss: 0.49148911237716675\n",
      "[step: 1949] loss: 0.49092555046081543\n",
      "[step: 1950] loss: 0.49036651849746704\n",
      "[step: 1951] loss: 0.4898111820220947\n",
      "[step: 1952] loss: 0.48925483226776123\n",
      "[step: 1953] loss: 0.48870202898979187\n",
      "[step: 1954] loss: 0.48814913630485535\n",
      "[step: 1955] loss: 0.48760029673576355\n",
      "[step: 1956] loss: 0.4870530664920807\n",
      "[step: 1957] loss: 0.4865075647830963\n",
      "[step: 1958] loss: 0.48596763610839844\n",
      "[step: 1959] loss: 0.4854303002357483\n",
      "[step: 1960] loss: 0.48489880561828613\n",
      "[step: 1961] loss: 0.48438018560409546\n",
      "[step: 1962] loss: 0.48388364911079407\n",
      "[step: 1963] loss: 0.48343947529792786\n",
      "[step: 1964] loss: 0.48311570286750793\n",
      "[step: 1965] loss: 0.48309075832366943\n",
      "[step: 1966] loss: 0.48383188247680664\n",
      "[step: 1967] loss: 0.486612468957901\n",
      "[step: 1968] loss: 0.49501705169677734\n",
      "[step: 1969] loss: 0.519320011138916\n",
      "[step: 1970] loss: 0.5879971385002136\n",
      "[step: 1971] loss: 0.7717039585113525\n",
      "[step: 1972] loss: 1.1769294738769531\n",
      "[step: 1973] loss: 1.5426839590072632\n",
      "[step: 1974] loss: 1.3541260957717896\n",
      "[step: 1975] loss: 0.6406160593032837\n",
      "[step: 1976] loss: 1.1995676755905151\n",
      "[step: 1977] loss: 1.1852151155471802\n",
      "[step: 1978] loss: 0.6082372665405273\n",
      "[step: 1979] loss: 0.8639900088310242\n",
      "[step: 1980] loss: 0.7511966228485107\n",
      "[step: 1981] loss: 0.7462765574455261\n",
      "[step: 1982] loss: 0.6425675749778748\n",
      "[step: 1983] loss: 0.6397936940193176\n",
      "[step: 1984] loss: 0.7642263174057007\n",
      "[step: 1985] loss: 0.6386523246765137\n",
      "[step: 1986] loss: 0.5872310400009155\n",
      "[step: 1987] loss: 0.6479361057281494\n",
      "[step: 1988] loss: 0.5915253758430481\n",
      "[step: 1989] loss: 0.6398029923439026\n",
      "[step: 1990] loss: 0.5844342708587646\n",
      "[step: 1991] loss: 0.602156937122345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1992] loss: 0.5697758793830872\n",
      "[step: 1993] loss: 0.5571776628494263\n",
      "[step: 1994] loss: 0.5907232761383057\n",
      "[step: 1995] loss: 0.5360484719276428\n",
      "[step: 1996] loss: 0.5526626706123352\n",
      "[step: 1997] loss: 0.525736391544342\n",
      "[step: 1998] loss: 0.5446315407752991\n",
      "[step: 1999] loss: 0.5404843688011169\n",
      "[step: 2000] loss: 0.525349497795105\n",
      "[step: 2001] loss: 0.5381546020507812\n",
      "[step: 2002] loss: 0.5185127854347229\n",
      "[step: 2003] loss: 0.5336285829544067\n",
      "[step: 2004] loss: 0.5250815749168396\n",
      "[step: 2005] loss: 0.5178235769271851\n",
      "[step: 2006] loss: 0.5149324536323547\n",
      "[step: 2007] loss: 0.5108702182769775\n",
      "[step: 2008] loss: 0.5180655717849731\n",
      "[step: 2009] loss: 0.5044527053833008\n",
      "[step: 2010] loss: 0.5051928758621216\n",
      "[step: 2011] loss: 0.5071583986282349\n",
      "[step: 2012] loss: 0.5011067986488342\n",
      "[step: 2013] loss: 0.5054869651794434\n",
      "[step: 2014] loss: 0.5013677477836609\n",
      "[step: 2015] loss: 0.4980785846710205\n",
      "[step: 2016] loss: 0.49991869926452637\n",
      "[step: 2017] loss: 0.496960312128067\n",
      "[step: 2018] loss: 0.4967482388019562\n",
      "[step: 2019] loss: 0.49468880891799927\n",
      "[step: 2020] loss: 0.4930974245071411\n",
      "[step: 2021] loss: 0.4936591386795044\n",
      "[step: 2022] loss: 0.49072712659835815\n",
      "[step: 2023] loss: 0.4904806613922119\n",
      "[step: 2024] loss: 0.48999959230422974\n",
      "[step: 2025] loss: 0.48772329092025757\n",
      "[step: 2026] loss: 0.4884890913963318\n",
      "[step: 2027] loss: 0.48702484369277954\n",
      "[step: 2028] loss: 0.485141396522522\n",
      "[step: 2029] loss: 0.48555871844291687\n",
      "[step: 2030] loss: 0.4840571880340576\n",
      "[step: 2031] loss: 0.48286065459251404\n",
      "[step: 2032] loss: 0.4829101860523224\n",
      "[step: 2033] loss: 0.4814434051513672\n",
      "[step: 2034] loss: 0.48073267936706543\n",
      "[step: 2035] loss: 0.4808022677898407\n",
      "[step: 2036] loss: 0.47942259907722473\n",
      "[step: 2037] loss: 0.4785151183605194\n",
      "[step: 2038] loss: 0.4784649610519409\n",
      "[step: 2039] loss: 0.47758805751800537\n",
      "[step: 2040] loss: 0.476681113243103\n",
      "[step: 2041] loss: 0.4762627184391022\n",
      "[step: 2042] loss: 0.47565141320228577\n",
      "[step: 2043] loss: 0.47483575344085693\n",
      "[step: 2044] loss: 0.47415056824684143\n",
      "[step: 2045] loss: 0.4736880362033844\n",
      "[step: 2046] loss: 0.4730982482433319\n",
      "[step: 2047] loss: 0.47235429286956787\n",
      "[step: 2048] loss: 0.4718486964702606\n",
      "[step: 2049] loss: 0.47130319476127625\n",
      "[step: 2050] loss: 0.47058629989624023\n",
      "[step: 2051] loss: 0.47003793716430664\n",
      "[step: 2052] loss: 0.46953636407852173\n",
      "[step: 2053] loss: 0.4689490795135498\n",
      "[step: 2054] loss: 0.46835610270500183\n",
      "[step: 2055] loss: 0.46777015924453735\n",
      "[step: 2056] loss: 0.46726876497268677\n",
      "[step: 2057] loss: 0.4667431712150574\n",
      "[step: 2058] loss: 0.46611323952674866\n",
      "[step: 2059] loss: 0.4655593931674957\n",
      "[step: 2060] loss: 0.4650707244873047\n",
      "[step: 2061] loss: 0.4645448625087738\n",
      "[step: 2062] loss: 0.46397116780281067\n",
      "[step: 2063] loss: 0.4633871912956238\n",
      "[step: 2064] loss: 0.46287551522254944\n",
      "[step: 2065] loss: 0.4623793363571167\n",
      "[step: 2066] loss: 0.4618419408798218\n",
      "[step: 2067] loss: 0.4613024890422821\n",
      "[step: 2068] loss: 0.4607482850551605\n",
      "[step: 2069] loss: 0.4602055847644806\n",
      "[step: 2070] loss: 0.4596920907497406\n",
      "[step: 2071] loss: 0.4591771364212036\n",
      "[step: 2072] loss: 0.4586597979068756\n",
      "[step: 2073] loss: 0.45812806487083435\n",
      "[step: 2074] loss: 0.45758944749832153\n",
      "[step: 2075] loss: 0.45705723762512207\n",
      "[step: 2076] loss: 0.45652318000793457\n",
      "[step: 2077] loss: 0.45600056648254395\n",
      "[step: 2078] loss: 0.455479234457016\n",
      "[step: 2079] loss: 0.45495858788490295\n",
      "[step: 2080] loss: 0.4544459283351898\n",
      "[step: 2081] loss: 0.45392924547195435\n",
      "[step: 2082] loss: 0.453418105840683\n",
      "[step: 2083] loss: 0.45291611552238464\n",
      "[step: 2084] loss: 0.4524274170398712\n",
      "[step: 2085] loss: 0.45196956396102905\n",
      "[step: 2086] loss: 0.45157667994499207\n",
      "[step: 2087] loss: 0.4513269066810608\n",
      "[step: 2088] loss: 0.45140859484672546\n",
      "[step: 2089] loss: 0.45229724049568176\n",
      "[step: 2090] loss: 0.4552387297153473\n",
      "[step: 2091] loss: 0.4635178744792938\n",
      "[step: 2092] loss: 0.48617154359817505\n",
      "[step: 2093] loss: 0.5475382804870605\n",
      "[step: 2094] loss: 0.706682562828064\n",
      "[step: 2095] loss: 1.0519953966140747\n",
      "[step: 2096] loss: 1.5208439826965332\n",
      "[step: 2097] loss: 1.1588892936706543\n",
      "[step: 2098] loss: 0.6037129759788513\n",
      "[step: 2099] loss: 0.8556245565414429\n",
      "[step: 2100] loss: 0.7715781331062317\n",
      "[step: 2101] loss: 0.5733502507209778\n",
      "[step: 2102] loss: 0.762129008769989\n",
      "[step: 2103] loss: 0.6442619562149048\n",
      "[step: 2104] loss: 0.5741816759109497\n",
      "[step: 2105] loss: 0.6850204467773438\n",
      "[step: 2106] loss: 0.5274662375450134\n",
      "[step: 2107] loss: 0.5522801280021667\n",
      "[step: 2108] loss: 0.6316462755203247\n",
      "[step: 2109] loss: 0.5469696521759033\n",
      "[step: 2110] loss: 0.5329086780548096\n",
      "[step: 2111] loss: 0.5608944892883301\n",
      "[step: 2112] loss: 0.5052358508110046\n",
      "[step: 2113] loss: 0.5594111084938049\n",
      "[step: 2114] loss: 0.5255223512649536\n",
      "[step: 2115] loss: 0.4835027754306793\n",
      "[step: 2116] loss: 0.538121223449707\n",
      "[step: 2117] loss: 0.49758562445640564\n",
      "[step: 2118] loss: 0.48861849308013916\n",
      "[step: 2119] loss: 0.5070056319236755\n",
      "[step: 2120] loss: 0.48395252227783203\n",
      "[step: 2121] loss: 0.49770843982696533\n",
      "[step: 2122] loss: 0.48302486538887024\n",
      "[step: 2123] loss: 0.48226481676101685\n",
      "[step: 2124] loss: 0.49188411235809326\n",
      "[step: 2125] loss: 0.471707284450531\n",
      "[step: 2126] loss: 0.47951167821884155\n",
      "[step: 2127] loss: 0.47721296548843384\n",
      "[step: 2128] loss: 0.46748343110084534\n",
      "[step: 2129] loss: 0.4760902523994446\n",
      "[step: 2130] loss: 0.46700984239578247\n",
      "[step: 2131] loss: 0.4658099412918091\n",
      "[step: 2132] loss: 0.471831351518631\n",
      "[step: 2133] loss: 0.4624292552471161\n",
      "[step: 2134] loss: 0.4633883833885193\n",
      "[step: 2135] loss: 0.4660762548446655\n",
      "[step: 2136] loss: 0.4582720398902893\n",
      "[step: 2137] loss: 0.4610940217971802\n",
      "[step: 2138] loss: 0.4615378975868225\n",
      "[step: 2139] loss: 0.45489445328712463\n",
      "[step: 2140] loss: 0.4571804106235504\n",
      "[step: 2141] loss: 0.4573700428009033\n",
      "[step: 2142] loss: 0.45302706956863403\n",
      "[step: 2143] loss: 0.4545811116695404\n",
      "[step: 2144] loss: 0.4543142020702362\n",
      "[step: 2145] loss: 0.4510956108570099\n",
      "[step: 2146] loss: 0.451547235250473\n",
      "[step: 2147] loss: 0.4510972499847412\n",
      "[step: 2148] loss: 0.4489617943763733\n",
      "[step: 2149] loss: 0.4490499794483185\n",
      "[step: 2150] loss: 0.44861799478530884\n",
      "[step: 2151] loss: 0.4471490681171417\n",
      "[step: 2152] loss: 0.446562260389328\n",
      "[step: 2153] loss: 0.44614914059638977\n",
      "[step: 2154] loss: 0.4454846680164337\n",
      "[step: 2155] loss: 0.4444335699081421\n",
      "[step: 2156] loss: 0.44384145736694336\n",
      "[step: 2157] loss: 0.4437009394168854\n",
      "[step: 2158] loss: 0.44260209798812866\n",
      "[step: 2159] loss: 0.44159314036369324\n",
      "[step: 2160] loss: 0.4414574205875397\n",
      "[step: 2161] loss: 0.4410027861595154\n",
      "[step: 2162] loss: 0.4400436580181122\n",
      "[step: 2163] loss: 0.4391942322254181\n",
      "[step: 2164] loss: 0.43893036246299744\n",
      "[step: 2165] loss: 0.43853697180747986\n",
      "[step: 2166] loss: 0.43752482533454895\n",
      "[step: 2167] loss: 0.43683016300201416\n",
      "[step: 2168] loss: 0.4364347755908966\n",
      "[step: 2169] loss: 0.4359198212623596\n",
      "[step: 2170] loss: 0.4353550672531128\n",
      "[step: 2171] loss: 0.43461930751800537\n",
      "[step: 2172] loss: 0.43393731117248535\n",
      "[step: 2173] loss: 0.43346184492111206\n",
      "[step: 2174] loss: 0.43301692605018616\n",
      "[step: 2175] loss: 0.43246498703956604\n",
      "[step: 2176] loss: 0.43180716037750244\n",
      "[step: 2177] loss: 0.4311777949333191\n",
      "[step: 2178] loss: 0.4305606782436371\n",
      "[step: 2179] loss: 0.4300358295440674\n",
      "[step: 2180] loss: 0.4295540750026703\n",
      "[step: 2181] loss: 0.4290262758731842\n",
      "[step: 2182] loss: 0.42852285504341125\n",
      "[step: 2183] loss: 0.4279567003250122\n",
      "[step: 2184] loss: 0.4273836612701416\n",
      "[step: 2185] loss: 0.4268237352371216\n",
      "[step: 2186] loss: 0.4262368679046631\n",
      "[step: 2187] loss: 0.42567718029022217\n",
      "[step: 2188] loss: 0.4251249432563782\n",
      "[step: 2189] loss: 0.42457452416419983\n",
      "[step: 2190] loss: 0.42403823137283325\n",
      "[step: 2191] loss: 0.42352163791656494\n",
      "[step: 2192] loss: 0.4230179488658905\n",
      "[step: 2193] loss: 0.4225706458091736\n",
      "[step: 2194] loss: 0.42225250601768494\n",
      "[step: 2195] loss: 0.42221686244010925\n",
      "[step: 2196] loss: 0.422946035861969\n",
      "[step: 2197] loss: 0.4256313145160675\n",
      "[step: 2198] loss: 0.4336319863796234\n",
      "[step: 2199] loss: 0.45698559284210205\n",
      "[step: 2200] loss: 0.5232963562011719\n",
      "[step: 2201] loss: 0.7046389579772949\n",
      "[step: 2202] loss: 1.1096885204315186\n",
      "[step: 2203] loss: 1.5074034929275513\n",
      "[step: 2204] loss: 1.300318717956543\n",
      "[step: 2205] loss: 0.5891769528388977\n",
      "[step: 2206] loss: 1.1671595573425293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2207] loss: 1.1184431314468384\n",
      "[step: 2208] loss: 0.5602659583091736\n",
      "[step: 2209] loss: 0.7638294696807861\n",
      "[step: 2210] loss: 0.716427206993103\n",
      "[step: 2211] loss: 0.7142154574394226\n",
      "[step: 2212] loss: 0.6267900466918945\n",
      "[step: 2213] loss: 0.5644239187240601\n",
      "[step: 2214] loss: 0.6565937995910645\n",
      "[step: 2215] loss: 0.5504260063171387\n",
      "[step: 2216] loss: 0.5724890828132629\n",
      "[step: 2217] loss: 0.5594007968902588\n",
      "[step: 2218] loss: 0.5291825532913208\n",
      "[step: 2219] loss: 0.5830274820327759\n",
      "[step: 2220] loss: 0.5306479930877686\n",
      "[step: 2221] loss: 0.5075762867927551\n",
      "[step: 2222] loss: 0.49142807722091675\n",
      "[step: 2223] loss: 0.5248655676841736\n",
      "[step: 2224] loss: 0.5009765625\n",
      "[step: 2225] loss: 0.4743175208568573\n",
      "[step: 2226] loss: 0.4921170175075531\n",
      "[step: 2227] loss: 0.4907219111919403\n",
      "[step: 2228] loss: 0.4839403033256531\n",
      "[step: 2229] loss: 0.471126914024353\n",
      "[step: 2230] loss: 0.4645395278930664\n",
      "[step: 2231] loss: 0.47368788719177246\n",
      "[step: 2232] loss: 0.4701208472251892\n",
      "[step: 2233] loss: 0.46432605385780334\n",
      "[step: 2234] loss: 0.46070683002471924\n",
      "[step: 2235] loss: 0.4586058557033539\n",
      "[step: 2236] loss: 0.46162763237953186\n",
      "[step: 2237] loss: 0.45234209299087524\n",
      "[step: 2238] loss: 0.45029735565185547\n",
      "[step: 2239] loss: 0.4527193307876587\n",
      "[step: 2240] loss: 0.4518267512321472\n",
      "[step: 2241] loss: 0.4488891661167145\n",
      "[step: 2242] loss: 0.44552624225616455\n",
      "[step: 2243] loss: 0.44724318385124207\n",
      "[step: 2244] loss: 0.4458697736263275\n",
      "[step: 2245] loss: 0.4422529637813568\n",
      "[step: 2246] loss: 0.44043290615081787\n",
      "[step: 2247] loss: 0.44023534655570984\n",
      "[step: 2248] loss: 0.43910759687423706\n",
      "[step: 2249] loss: 0.435968816280365\n",
      "[step: 2250] loss: 0.4351305067539215\n",
      "[step: 2251] loss: 0.4363310933113098\n",
      "[step: 2252] loss: 0.43417197465896606\n",
      "[step: 2253] loss: 0.43178269267082214\n",
      "[step: 2254] loss: 0.43256354331970215\n",
      "[step: 2255] loss: 0.4312855899333954\n",
      "[step: 2256] loss: 0.4290940761566162\n",
      "[step: 2257] loss: 0.4288683533668518\n",
      "[step: 2258] loss: 0.4281565845012665\n",
      "[step: 2259] loss: 0.42703139781951904\n",
      "[step: 2260] loss: 0.4260324537754059\n",
      "[step: 2261] loss: 0.4250648617744446\n",
      "[step: 2262] loss: 0.42462053894996643\n",
      "[step: 2263] loss: 0.4237024486064911\n",
      "[step: 2264] loss: 0.42253047227859497\n",
      "[step: 2265] loss: 0.4222210943698883\n",
      "[step: 2266] loss: 0.42143434286117554\n",
      "[step: 2267] loss: 0.42019733786582947\n",
      "[step: 2268] loss: 0.41971325874328613\n",
      "[step: 2269] loss: 0.419090211391449\n",
      "[step: 2270] loss: 0.41811904311180115\n",
      "[step: 2271] loss: 0.41745898127555847\n",
      "[step: 2272] loss: 0.41688069701194763\n",
      "[step: 2273] loss: 0.4161095917224884\n",
      "[step: 2274] loss: 0.41537848114967346\n",
      "[step: 2275] loss: 0.41479161381721497\n",
      "[step: 2276] loss: 0.4140970706939697\n",
      "[step: 2277] loss: 0.41343432664871216\n",
      "[step: 2278] loss: 0.41286012530326843\n",
      "[step: 2279] loss: 0.41212067008018494\n",
      "[step: 2280] loss: 0.4114973545074463\n",
      "[step: 2281] loss: 0.4110054671764374\n",
      "[step: 2282] loss: 0.4102729260921478\n",
      "[step: 2283] loss: 0.40962663292884827\n",
      "[step: 2284] loss: 0.4091060757637024\n",
      "[step: 2285] loss: 0.40847349166870117\n",
      "[step: 2286] loss: 0.4078550636768341\n",
      "[step: 2287] loss: 0.4072740972042084\n",
      "[step: 2288] loss: 0.4066750705242157\n",
      "[step: 2289] loss: 0.4061034321784973\n",
      "[step: 2290] loss: 0.4055668115615845\n",
      "[step: 2291] loss: 0.4049700200557709\n",
      "[step: 2292] loss: 0.4043692648410797\n",
      "[step: 2293] loss: 0.4038316607475281\n",
      "[step: 2294] loss: 0.403250515460968\n",
      "[step: 2295] loss: 0.40270259976387024\n",
      "[step: 2296] loss: 0.4021815061569214\n",
      "[step: 2297] loss: 0.4016028940677643\n",
      "[step: 2298] loss: 0.40105780959129333\n",
      "[step: 2299] loss: 0.4005153477191925\n",
      "[step: 2300] loss: 0.39994949102401733\n",
      "[step: 2301] loss: 0.39941781759262085\n",
      "[step: 2302] loss: 0.3988863229751587\n",
      "[step: 2303] loss: 0.3983413279056549\n",
      "[step: 2304] loss: 0.3978222608566284\n",
      "[step: 2305] loss: 0.3972986042499542\n",
      "[step: 2306] loss: 0.3967672884464264\n",
      "[step: 2307] loss: 0.3962656557559967\n",
      "[step: 2308] loss: 0.39575818181037903\n",
      "[step: 2309] loss: 0.3952745497226715\n",
      "[step: 2310] loss: 0.39484846591949463\n",
      "[step: 2311] loss: 0.3945067226886749\n",
      "[step: 2312] loss: 0.39440155029296875\n",
      "[step: 2313] loss: 0.3948307931423187\n",
      "[step: 2314] loss: 0.39656326174736023\n",
      "[step: 2315] loss: 0.4017345607280731\n",
      "[step: 2316] loss: 0.4161226451396942\n",
      "[step: 2317] loss: 0.4558478593826294\n",
      "[step: 2318] loss: 0.5615920424461365\n",
      "[step: 2319] loss: 0.8116950988769531\n",
      "[step: 2320] loss: 1.2570950984954834\n",
      "[step: 2321] loss: 1.3078490495681763\n",
      "[step: 2322] loss: 0.7935446500778198\n",
      "[step: 2323] loss: 0.6041865944862366\n",
      "[step: 2324] loss: 1.0046073198318481\n",
      "[step: 2325] loss: 0.826743483543396\n",
      "[step: 2326] loss: 0.4728831350803375\n",
      "[step: 2327] loss: 0.6138542890548706\n",
      "[step: 2328] loss: 0.6150657534599304\n",
      "[step: 2329] loss: 0.49993470311164856\n",
      "[step: 2330] loss: 0.523177981376648\n",
      "[step: 2331] loss: 0.594389021396637\n",
      "[step: 2332] loss: 0.48864197731018066\n",
      "[step: 2333] loss: 0.47651752829551697\n",
      "[step: 2334] loss: 0.4995051324367523\n",
      "[step: 2335] loss: 0.48021993041038513\n",
      "[step: 2336] loss: 0.47634202241897583\n",
      "[step: 2337] loss: 0.4585723876953125\n",
      "[step: 2338] loss: 0.46898916363716125\n",
      "[step: 2339] loss: 0.47086650133132935\n",
      "[step: 2340] loss: 0.43915343284606934\n",
      "[step: 2341] loss: 0.44891220331192017\n",
      "[step: 2342] loss: 0.45077869296073914\n",
      "[step: 2343] loss: 0.4342225193977356\n",
      "[step: 2344] loss: 0.44272053241729736\n",
      "[step: 2345] loss: 0.4335970878601074\n",
      "[step: 2346] loss: 0.4306598901748657\n",
      "[step: 2347] loss: 0.43429598212242126\n",
      "[step: 2348] loss: 0.42277792096138\n",
      "[step: 2349] loss: 0.42566022276878357\n",
      "[step: 2350] loss: 0.42307987809181213\n",
      "[step: 2351] loss: 0.4212556481361389\n",
      "[step: 2352] loss: 0.416374534368515\n",
      "[step: 2353] loss: 0.4153136610984802\n",
      "[step: 2354] loss: 0.4175809919834137\n",
      "[step: 2355] loss: 0.40919122099876404\n",
      "[step: 2356] loss: 0.41005438566207886\n",
      "[step: 2357] loss: 0.4145486056804657\n",
      "[step: 2358] loss: 0.4060012996196747\n",
      "[step: 2359] loss: 0.40574419498443604\n",
      "[step: 2360] loss: 0.408335417509079\n",
      "[step: 2361] loss: 0.403346985578537\n",
      "[step: 2362] loss: 0.40187233686447144\n",
      "[step: 2363] loss: 0.4029153287410736\n",
      "[step: 2364] loss: 0.401079922914505\n",
      "[step: 2365] loss: 0.39875268936157227\n",
      "[step: 2366] loss: 0.39838337898254395\n",
      "[step: 2367] loss: 0.3980486989021301\n",
      "[step: 2368] loss: 0.395919531583786\n",
      "[step: 2369] loss: 0.39530614018440247\n",
      "[step: 2370] loss: 0.395436555147171\n",
      "[step: 2371] loss: 0.39378729462623596\n",
      "[step: 2372] loss: 0.3929782807826996\n",
      "[step: 2373] loss: 0.3925076127052307\n",
      "[step: 2374] loss: 0.39137667417526245\n",
      "[step: 2375] loss: 0.39046329259872437\n",
      "[step: 2376] loss: 0.39002755284309387\n",
      "[step: 2377] loss: 0.38922351598739624\n",
      "[step: 2378] loss: 0.388057678937912\n",
      "[step: 2379] loss: 0.3878212869167328\n",
      "[step: 2380] loss: 0.38698190450668335\n",
      "[step: 2381] loss: 0.38581618666648865\n",
      "[step: 2382] loss: 0.3858211934566498\n",
      "[step: 2383] loss: 0.3851555585861206\n",
      "[step: 2384] loss: 0.3840174376964569\n",
      "[step: 2385] loss: 0.3835420310497284\n",
      "[step: 2386] loss: 0.383253276348114\n",
      "[step: 2387] loss: 0.38228967785835266\n",
      "[step: 2388] loss: 0.3816536068916321\n",
      "[step: 2389] loss: 0.38124987483024597\n",
      "[step: 2390] loss: 0.3805322051048279\n",
      "[step: 2391] loss: 0.38002410531044006\n",
      "[step: 2392] loss: 0.3794154226779938\n",
      "[step: 2393] loss: 0.378737211227417\n",
      "[step: 2394] loss: 0.378221333026886\n",
      "[step: 2395] loss: 0.3778100609779358\n",
      "[step: 2396] loss: 0.3771061599254608\n",
      "[step: 2397] loss: 0.3765474557876587\n",
      "[step: 2398] loss: 0.3760007619857788\n",
      "[step: 2399] loss: 0.37543007731437683\n",
      "[step: 2400] loss: 0.3749406039714813\n",
      "[step: 2401] loss: 0.3744615912437439\n",
      "[step: 2402] loss: 0.3738567531108856\n",
      "[step: 2403] loss: 0.3733308017253876\n",
      "[step: 2404] loss: 0.37278807163238525\n",
      "[step: 2405] loss: 0.3722206950187683\n",
      "[step: 2406] loss: 0.3717156946659088\n",
      "[step: 2407] loss: 0.3712129592895508\n",
      "[step: 2408] loss: 0.3706830143928528\n",
      "[step: 2409] loss: 0.37021058797836304\n",
      "[step: 2410] loss: 0.3697083592414856\n",
      "[step: 2411] loss: 0.36920255422592163\n",
      "[step: 2412] loss: 0.3687276840209961\n",
      "[step: 2413] loss: 0.3682451844215393\n",
      "[step: 2414] loss: 0.3677810728549957\n",
      "[step: 2415] loss: 0.36738884449005127\n",
      "[step: 2416] loss: 0.3670518696308136\n",
      "[step: 2417] loss: 0.3669252097606659\n",
      "[step: 2418] loss: 0.3672085106372833\n",
      "[step: 2419] loss: 0.368406742811203\n",
      "[step: 2420] loss: 0.37184441089630127\n",
      "[step: 2421] loss: 0.3807814419269562\n",
      "[step: 2422] loss: 0.4038696885108948\n",
      "[step: 2423] loss: 0.4610913395881653\n",
      "[step: 2424] loss: 0.599199116230011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2425] loss: 0.849602460861206\n",
      "[step: 2426] loss: 1.1772162914276123\n",
      "[step: 2427] loss: 0.9060304760932922\n",
      "[step: 2428] loss: 0.46773210167884827\n",
      "[step: 2429] loss: 0.5780181288719177\n",
      "[step: 2430] loss: 0.8030757308006287\n",
      "[step: 2431] loss: 0.6009414196014404\n",
      "[step: 2432] loss: 0.44305965304374695\n",
      "[step: 2433] loss: 0.5686991810798645\n",
      "[step: 2434] loss: 0.5044631958007812\n",
      "[step: 2435] loss: 0.4187262952327728\n",
      "[step: 2436] loss: 0.5307943224906921\n",
      "[step: 2437] loss: 0.5136873722076416\n",
      "[step: 2438] loss: 0.39262184500694275\n",
      "[step: 2439] loss: 0.47991886734962463\n",
      "[step: 2440] loss: 0.4491536021232605\n",
      "[step: 2441] loss: 0.4073326885700226\n",
      "[step: 2442] loss: 0.4600541591644287\n",
      "[step: 2443] loss: 0.41937679052352905\n",
      "[step: 2444] loss: 0.40768590569496155\n",
      "[step: 2445] loss: 0.42619436979293823\n",
      "[step: 2446] loss: 0.41145071387290955\n",
      "[step: 2447] loss: 0.403418630361557\n",
      "[step: 2448] loss: 0.4051981568336487\n",
      "[step: 2449] loss: 0.4116571545600891\n",
      "[step: 2450] loss: 0.3861680030822754\n",
      "[step: 2451] loss: 0.3925327658653259\n",
      "[step: 2452] loss: 0.39408695697784424\n",
      "[step: 2453] loss: 0.38567718863487244\n",
      "[step: 2454] loss: 0.3831897974014282\n",
      "[step: 2455] loss: 0.3845330476760864\n",
      "[step: 2456] loss: 0.3844096064567566\n",
      "[step: 2457] loss: 0.376392662525177\n",
      "[step: 2458] loss: 0.37997332215309143\n",
      "[step: 2459] loss: 0.3811815679073334\n",
      "[step: 2460] loss: 0.3721734285354614\n",
      "[step: 2461] loss: 0.3763190805912018\n",
      "[step: 2462] loss: 0.3741329312324524\n",
      "[step: 2463] loss: 0.3699832856655121\n",
      "[step: 2464] loss: 0.3710595667362213\n",
      "[step: 2465] loss: 0.3704172372817993\n",
      "[step: 2466] loss: 0.367941290140152\n",
      "[step: 2467] loss: 0.36656057834625244\n",
      "[step: 2468] loss: 0.36822494864463806\n",
      "[step: 2469] loss: 0.364624559879303\n",
      "[step: 2470] loss: 0.3653327524662018\n",
      "[step: 2471] loss: 0.3638492822647095\n",
      "[step: 2472] loss: 0.3623441159725189\n",
      "[step: 2473] loss: 0.36216306686401367\n",
      "[step: 2474] loss: 0.361971914768219\n",
      "[step: 2475] loss: 0.3595189154148102\n",
      "[step: 2476] loss: 0.35987845063209534\n",
      "[step: 2477] loss: 0.35938024520874023\n",
      "[step: 2478] loss: 0.35789114236831665\n",
      "[step: 2479] loss: 0.35721370577812195\n",
      "[step: 2480] loss: 0.35721778869628906\n",
      "[step: 2481] loss: 0.3556809425354004\n",
      "[step: 2482] loss: 0.35552749037742615\n",
      "[step: 2483] loss: 0.3546706736087799\n",
      "[step: 2484] loss: 0.35413604974746704\n",
      "[step: 2485] loss: 0.35361528396606445\n",
      "[step: 2486] loss: 0.3530476987361908\n",
      "[step: 2487] loss: 0.3519294857978821\n",
      "[step: 2488] loss: 0.35176482796669006\n",
      "[step: 2489] loss: 0.35110828280448914\n",
      "[step: 2490] loss: 0.3504437506198883\n",
      "[step: 2491] loss: 0.35008612275123596\n",
      "[step: 2492] loss: 0.34916821122169495\n",
      "[step: 2493] loss: 0.348706990480423\n",
      "[step: 2494] loss: 0.3482023775577545\n",
      "[step: 2495] loss: 0.3476200997829437\n",
      "[step: 2496] loss: 0.34717270731925964\n",
      "[step: 2497] loss: 0.3466213643550873\n",
      "[step: 2498] loss: 0.34600433707237244\n",
      "[step: 2499] loss: 0.3454712927341461\n",
      "[step: 2500] loss: 0.34492263197898865\n",
      "[step: 2501] loss: 0.34431225061416626\n",
      "[step: 2502] loss: 0.343842089176178\n",
      "[step: 2503] loss: 0.3432886004447937\n",
      "[step: 2504] loss: 0.34275364875793457\n",
      "[step: 2505] loss: 0.34230050444602966\n",
      "[step: 2506] loss: 0.3417423665523529\n",
      "[step: 2507] loss: 0.34129390120506287\n",
      "[step: 2508] loss: 0.34086793661117554\n",
      "[step: 2509] loss: 0.34046846628189087\n",
      "[step: 2510] loss: 0.34031417965888977\n",
      "[step: 2511] loss: 0.3405054211616516\n",
      "[step: 2512] loss: 0.34167787432670593\n",
      "[step: 2513] loss: 0.34533941745758057\n",
      "[step: 2514] loss: 0.3552832007408142\n",
      "[step: 2515] loss: 0.3821623921394348\n",
      "[step: 2516] loss: 0.45012712478637695\n",
      "[step: 2517] loss: 0.6197413802146912\n",
      "[step: 2518] loss: 0.8996871709823608\n",
      "[step: 2519] loss: 1.222406029701233\n",
      "[step: 2520] loss: 0.7764580249786377\n",
      "[step: 2521] loss: 0.4794551730155945\n",
      "[step: 2522] loss: 0.6471725702285767\n",
      "[step: 2523] loss: 0.6035373210906982\n",
      "[step: 2524] loss: 0.42911332845687866\n",
      "[step: 2525] loss: 0.6602002382278442\n",
      "[step: 2526] loss: 0.4555550813674927\n",
      "[step: 2527] loss: 0.48104771971702576\n",
      "[step: 2528] loss: 0.4700589179992676\n",
      "[step: 2529] loss: 0.415870726108551\n",
      "[step: 2530] loss: 0.4411815404891968\n",
      "[step: 2531] loss: 0.4637905955314636\n",
      "[step: 2532] loss: 0.39200735092163086\n",
      "[step: 2533] loss: 0.44736576080322266\n",
      "[step: 2534] loss: 0.4183281362056732\n",
      "[step: 2535] loss: 0.3690919280052185\n",
      "[step: 2536] loss: 0.4313714802265167\n",
      "[step: 2537] loss: 0.38261422514915466\n",
      "[step: 2538] loss: 0.4012393355369568\n",
      "[step: 2539] loss: 0.3994123339653015\n",
      "[step: 2540] loss: 0.38675934076309204\n",
      "[step: 2541] loss: 0.3769777715206146\n",
      "[step: 2542] loss: 0.3780866861343384\n",
      "[step: 2543] loss: 0.3855666518211365\n",
      "[step: 2544] loss: 0.36565524339675903\n",
      "[step: 2545] loss: 0.3738130033016205\n",
      "[step: 2546] loss: 0.3757738471031189\n",
      "[step: 2547] loss: 0.3556050956249237\n",
      "[step: 2548] loss: 0.3676154613494873\n",
      "[step: 2549] loss: 0.367053359746933\n",
      "[step: 2550] loss: 0.35794195532798767\n",
      "[step: 2551] loss: 0.35608652234077454\n",
      "[step: 2552] loss: 0.35783660411834717\n",
      "[step: 2553] loss: 0.35013148188591003\n",
      "[step: 2554] loss: 0.35028040409088135\n",
      "[step: 2555] loss: 0.353598415851593\n",
      "[step: 2556] loss: 0.34634050726890564\n",
      "[step: 2557] loss: 0.34834328293800354\n",
      "[step: 2558] loss: 0.3466202914714813\n",
      "[step: 2559] loss: 0.3435663878917694\n",
      "[step: 2560] loss: 0.3438785970211029\n",
      "[step: 2561] loss: 0.3449992537498474\n",
      "[step: 2562] loss: 0.3396899998188019\n",
      "[step: 2563] loss: 0.33975598216056824\n",
      "[step: 2564] loss: 0.3403504192829132\n",
      "[step: 2565] loss: 0.33803218603134155\n",
      "[step: 2566] loss: 0.3366502523422241\n",
      "[step: 2567] loss: 0.33810847997665405\n",
      "[step: 2568] loss: 0.3347966969013214\n",
      "[step: 2569] loss: 0.334344744682312\n",
      "[step: 2570] loss: 0.3343330919742584\n",
      "[step: 2571] loss: 0.3337853252887726\n",
      "[step: 2572] loss: 0.33214759826660156\n",
      "[step: 2573] loss: 0.33174410462379456\n",
      "[step: 2574] loss: 0.33054691553115845\n",
      "[step: 2575] loss: 0.3305444121360779\n",
      "[step: 2576] loss: 0.3298684060573578\n",
      "[step: 2577] loss: 0.32818564772605896\n",
      "[step: 2578] loss: 0.32820433378219604\n",
      "[step: 2579] loss: 0.3273589313030243\n",
      "[step: 2580] loss: 0.3268546760082245\n",
      "[step: 2581] loss: 0.3263675570487976\n",
      "[step: 2582] loss: 0.32526397705078125\n",
      "[step: 2583] loss: 0.32480376958847046\n",
      "[step: 2584] loss: 0.32432299852371216\n",
      "[step: 2585] loss: 0.32377299666404724\n",
      "[step: 2586] loss: 0.3232850432395935\n",
      "[step: 2587] loss: 0.32255828380584717\n",
      "[step: 2588] loss: 0.3218468129634857\n",
      "[step: 2589] loss: 0.3212890326976776\n",
      "[step: 2590] loss: 0.32071471214294434\n",
      "[step: 2591] loss: 0.32020339369773865\n",
      "[step: 2592] loss: 0.3197026252746582\n",
      "[step: 2593] loss: 0.3191779553890228\n",
      "[step: 2594] loss: 0.31876781582832336\n",
      "[step: 2595] loss: 0.31828299164772034\n",
      "[step: 2596] loss: 0.31782829761505127\n",
      "[step: 2597] loss: 0.31751105189323425\n",
      "[step: 2598] loss: 0.3172169625759125\n",
      "[step: 2599] loss: 0.31731754541397095\n",
      "[step: 2600] loss: 0.31803497672080994\n",
      "[step: 2601] loss: 0.32002875208854675\n",
      "[step: 2602] loss: 0.3252829313278198\n",
      "[step: 2603] loss: 0.3380977511405945\n",
      "[step: 2604] loss: 0.36787816882133484\n",
      "[step: 2605] loss: 0.43757808208465576\n",
      "[step: 2606] loss: 0.5710555911064148\n",
      "[step: 2607] loss: 0.7853588461875916\n",
      "[step: 2608] loss: 0.8226077556610107\n",
      "[step: 2609] loss: 0.6546856164932251\n",
      "[step: 2610] loss: 0.3843024671077728\n",
      "[step: 2611] loss: 0.581428587436676\n",
      "[step: 2612] loss: 0.7919616103172302\n",
      "[step: 2613] loss: 0.43604952096939087\n",
      "[step: 2614] loss: 0.48836880922317505\n",
      "[step: 2615] loss: 0.5800675749778748\n",
      "[step: 2616] loss: 0.41373410820961\n",
      "[step: 2617] loss: 0.4558064639568329\n",
      "[step: 2618] loss: 0.6987423896789551\n",
      "[step: 2619] loss: 0.6381248831748962\n",
      "[step: 2620] loss: 0.7153791785240173\n",
      "[step: 2621] loss: 0.6845577359199524\n",
      "[step: 2622] loss: 0.6899229288101196\n",
      "[step: 2623] loss: 0.751758873462677\n",
      "[step: 2624] loss: 0.7888799905776978\n",
      "[step: 2625] loss: 0.7124244570732117\n",
      "[step: 2626] loss: 0.6087509989738464\n",
      "[step: 2627] loss: 0.5728856921195984\n",
      "[step: 2628] loss: 0.5929964184761047\n",
      "[step: 2629] loss: 0.61247718334198\n",
      "[step: 2630] loss: 0.6004183888435364\n",
      "[step: 2631] loss: 0.5870279669761658\n",
      "[step: 2632] loss: 0.5599822998046875\n",
      "[step: 2633] loss: 0.5228919386863708\n",
      "[step: 2634] loss: 0.5178197622299194\n",
      "[step: 2635] loss: 0.522719144821167\n",
      "[step: 2636] loss: 0.5192074179649353\n",
      "[step: 2637] loss: 0.5228212475776672\n",
      "[step: 2638] loss: 0.5216161608695984\n",
      "[step: 2639] loss: 0.5052621364593506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2640] loss: 0.4951070249080658\n",
      "[step: 2641] loss: 0.49436789751052856\n",
      "[step: 2642] loss: 0.4897620677947998\n",
      "[step: 2643] loss: 0.486528217792511\n",
      "[step: 2644] loss: 0.48601800203323364\n",
      "[step: 2645] loss: 0.47871965169906616\n",
      "[step: 2646] loss: 0.4755564033985138\n",
      "[step: 2647] loss: 0.4766474962234497\n",
      "[step: 2648] loss: 0.4721704125404358\n",
      "[step: 2649] loss: 0.4670850336551666\n",
      "[step: 2650] loss: 0.4631573557853699\n",
      "[step: 2651] loss: 0.4573853611946106\n",
      "[step: 2652] loss: 0.45376086235046387\n",
      "[step: 2653] loss: 0.4535190165042877\n",
      "[step: 2654] loss: 0.45187821984291077\n",
      "[step: 2655] loss: 0.4495805501937866\n",
      "[step: 2656] loss: 0.448489785194397\n",
      "[step: 2657] loss: 0.4463675618171692\n",
      "[step: 2658] loss: 0.44366011023521423\n",
      "[step: 2659] loss: 0.44220736622810364\n",
      "[step: 2660] loss: 0.4409959614276886\n",
      "[step: 2661] loss: 0.4386844038963318\n",
      "[step: 2662] loss: 0.43624693155288696\n",
      "[step: 2663] loss: 0.4347365200519562\n",
      "[step: 2664] loss: 0.43350309133529663\n",
      "[step: 2665] loss: 0.43199679255485535\n",
      "[step: 2666] loss: 0.43065959215164185\n",
      "[step: 2667] loss: 0.42928874492645264\n",
      "[step: 2668] loss: 0.4273935556411743\n",
      "[step: 2669] loss: 0.42551204562187195\n",
      "[step: 2670] loss: 0.4241847097873688\n",
      "[step: 2671] loss: 0.42297524213790894\n",
      "[step: 2672] loss: 0.4215417802333832\n",
      "[step: 2673] loss: 0.4199625849723816\n",
      "[step: 2674] loss: 0.418494313955307\n",
      "[step: 2675] loss: 0.4173223674297333\n",
      "[step: 2676] loss: 0.4162413775920868\n",
      "[step: 2677] loss: 0.41500169038772583\n",
      "[step: 2678] loss: 0.4136044979095459\n",
      "[step: 2679] loss: 0.4121783673763275\n",
      "[step: 2680] loss: 0.4108838737010956\n",
      "[step: 2681] loss: 0.409758985042572\n",
      "[step: 2682] loss: 0.40861237049102783\n",
      "[step: 2683] loss: 0.4073619544506073\n",
      "[step: 2684] loss: 0.4060867428779602\n",
      "[step: 2685] loss: 0.4048328399658203\n",
      "[step: 2686] loss: 0.40363118052482605\n",
      "[step: 2687] loss: 0.4024462103843689\n",
      "[step: 2688] loss: 0.4012537896633148\n",
      "[step: 2689] loss: 0.40007397532463074\n",
      "[step: 2690] loss: 0.3989028334617615\n",
      "[step: 2691] loss: 0.39776429533958435\n",
      "[step: 2692] loss: 0.39667561650276184\n",
      "[step: 2693] loss: 0.3956076502799988\n",
      "[step: 2694] loss: 0.39459675550460815\n",
      "[step: 2695] loss: 0.39378926157951355\n",
      "[step: 2696] loss: 0.39352086186408997\n",
      "[step: 2697] loss: 0.39476150274276733\n",
      "[step: 2698] loss: 0.4005061089992523\n",
      "[step: 2699] loss: 0.4208223819732666\n",
      "[step: 2700] loss: 0.48854976892471313\n",
      "[step: 2701] loss: 0.7041571736335754\n",
      "[step: 2702] loss: 1.2398130893707275\n",
      "[step: 2703] loss: 1.790535569190979\n",
      "[step: 2704] loss: 1.575119137763977\n",
      "[step: 2705] loss: 0.5319225788116455\n",
      "[step: 2706] loss: 1.3220045566558838\n",
      "[step: 2707] loss: 1.220258355140686\n",
      "[step: 2708] loss: 0.7221964597702026\n",
      "[step: 2709] loss: 1.0116233825683594\n",
      "[step: 2710] loss: 0.7284479141235352\n",
      "[step: 2711] loss: 0.6702904105186462\n",
      "[step: 2712] loss: 0.7632449865341187\n",
      "[step: 2713] loss: 0.7861273884773254\n",
      "[step: 2714] loss: 0.764828085899353\n",
      "[step: 2715] loss: 0.7361965775489807\n",
      "[step: 2716] loss: 0.6862365007400513\n",
      "[step: 2717] loss: 0.6414303183555603\n",
      "[step: 2718] loss: 0.656772255897522\n",
      "[step: 2719] loss: 0.6107269525527954\n",
      "[step: 2720] loss: 0.5760802626609802\n",
      "[step: 2721] loss: 0.6015167236328125\n",
      "[step: 2722] loss: 0.5999494194984436\n",
      "[step: 2723] loss: 0.5625839829444885\n",
      "[step: 2724] loss: 0.5449498891830444\n",
      "[step: 2725] loss: 0.5450550317764282\n",
      "[step: 2726] loss: 0.5168303847312927\n",
      "[step: 2727] loss: 0.49024903774261475\n",
      "[step: 2728] loss: 0.48504728078842163\n",
      "[step: 2729] loss: 0.48452574014663696\n",
      "[step: 2730] loss: 0.4750801920890808\n",
      "[step: 2731] loss: 0.48252445459365845\n",
      "[step: 2732] loss: 0.48436668515205383\n",
      "[step: 2733] loss: 0.481633722782135\n",
      "[step: 2734] loss: 0.4702724516391754\n",
      "[step: 2735] loss: 0.46529123187065125\n",
      "[step: 2736] loss: 0.45236003398895264\n",
      "[step: 2737] loss: 0.4576067626476288\n",
      "[step: 2738] loss: 0.45171380043029785\n",
      "[step: 2739] loss: 0.4464699923992157\n",
      "[step: 2740] loss: 0.4452677071094513\n",
      "[step: 2741] loss: 0.44756054878234863\n",
      "[step: 2742] loss: 0.44183340668678284\n",
      "[step: 2743] loss: 0.4358656108379364\n",
      "[step: 2744] loss: 0.4378804564476013\n",
      "[step: 2745] loss: 0.4320406913757324\n",
      "[step: 2746] loss: 0.4281595051288605\n",
      "[step: 2747] loss: 0.42550164461135864\n",
      "[step: 2748] loss: 0.42504632472991943\n",
      "[step: 2749] loss: 0.42226076126098633\n",
      "[step: 2750] loss: 0.4210450351238251\n",
      "[step: 2751] loss: 0.42037275433540344\n",
      "[step: 2752] loss: 0.4187186062335968\n",
      "[step: 2753] loss: 0.4174383580684662\n",
      "[step: 2754] loss: 0.41349151730537415\n",
      "[step: 2755] loss: 0.41194188594818115\n",
      "[step: 2756] loss: 0.410495400428772\n",
      "[step: 2757] loss: 0.4091247618198395\n",
      "[step: 2758] loss: 0.40800613164901733\n",
      "[step: 2759] loss: 0.40715357661247253\n",
      "[step: 2760] loss: 0.40591806173324585\n",
      "[step: 2761] loss: 0.40333038568496704\n",
      "[step: 2762] loss: 0.4021899700164795\n",
      "[step: 2763] loss: 0.4008216857910156\n",
      "[step: 2764] loss: 0.3986891210079193\n",
      "[step: 2765] loss: 0.3977544605731964\n",
      "[step: 2766] loss: 0.3965611159801483\n",
      "[step: 2767] loss: 0.3951568007469177\n",
      "[step: 2768] loss: 0.39394640922546387\n",
      "[step: 2769] loss: 0.39237427711486816\n",
      "[step: 2770] loss: 0.3911074101924896\n",
      "[step: 2771] loss: 0.3898348808288574\n",
      "[step: 2772] loss: 0.38838961720466614\n",
      "[step: 2773] loss: 0.38733720779418945\n",
      "[step: 2774] loss: 0.3860505521297455\n",
      "[step: 2775] loss: 0.3846054971218109\n",
      "[step: 2776] loss: 0.3836473822593689\n",
      "[step: 2777] loss: 0.3825225830078125\n",
      "[step: 2778] loss: 0.38121190667152405\n",
      "[step: 2779] loss: 0.38021689653396606\n",
      "[step: 2780] loss: 0.3791300654411316\n",
      "[step: 2781] loss: 0.37792015075683594\n",
      "[step: 2782] loss: 0.3768620491027832\n",
      "[step: 2783] loss: 0.37584197521209717\n",
      "[step: 2784] loss: 0.3747834265232086\n",
      "[step: 2785] loss: 0.3737543821334839\n",
      "[step: 2786] loss: 0.37281906604766846\n",
      "[step: 2787] loss: 0.37181034684181213\n",
      "[step: 2788] loss: 0.37074270844459534\n",
      "[step: 2789] loss: 0.36980295181274414\n",
      "[step: 2790] loss: 0.3688562512397766\n",
      "[step: 2791] loss: 0.36788299679756165\n",
      "[step: 2792] loss: 0.366953581571579\n",
      "[step: 2793] loss: 0.36601749062538147\n",
      "[step: 2794] loss: 0.3651047348976135\n",
      "[step: 2795] loss: 0.3641928732395172\n",
      "[step: 2796] loss: 0.36327308416366577\n",
      "[step: 2797] loss: 0.36236050724983215\n",
      "[step: 2798] loss: 0.36147230863571167\n",
      "[step: 2799] loss: 0.3606041967868805\n",
      "[step: 2800] loss: 0.35972222685813904\n",
      "[step: 2801] loss: 0.35885900259017944\n",
      "[step: 2802] loss: 0.35799118876457214\n",
      "[step: 2803] loss: 0.3571125566959381\n",
      "[step: 2804] loss: 0.35626038908958435\n",
      "[step: 2805] loss: 0.35541173815727234\n",
      "[step: 2806] loss: 0.3545726239681244\n",
      "[step: 2807] loss: 0.3537444472312927\n",
      "[step: 2808] loss: 0.3529236912727356\n",
      "[step: 2809] loss: 0.3521065413951874\n",
      "[step: 2810] loss: 0.3513003885746002\n",
      "[step: 2811] loss: 0.35051581263542175\n",
      "[step: 2812] loss: 0.3497665524482727\n",
      "[step: 2813] loss: 0.3491099774837494\n",
      "[step: 2814] loss: 0.3486458361148834\n",
      "[step: 2815] loss: 0.3487144112586975\n",
      "[step: 2816] loss: 0.350290983915329\n",
      "[step: 2817] loss: 0.3564104735851288\n",
      "[step: 2818] loss: 0.3767780661582947\n",
      "[step: 2819] loss: 0.4432529807090759\n",
      "[step: 2820] loss: 0.6493901014328003\n",
      "[step: 2821] loss: 1.200614333152771\n",
      "[step: 2822] loss: 1.790940284729004\n",
      "[step: 2823] loss: 1.6228259801864624\n",
      "[step: 2824] loss: 0.5208048224449158\n",
      "[step: 2825] loss: 1.2659494876861572\n",
      "[step: 2826] loss: 0.8760048747062683\n",
      "[step: 2827] loss: 0.6820579767227173\n",
      "[step: 2828] loss: 0.9322594404220581\n",
      "[step: 2829] loss: 0.5674635767936707\n",
      "[step: 2830] loss: 0.6753811240196228\n",
      "[step: 2831] loss: 0.5584079623222351\n",
      "[step: 2832] loss: 0.706160843372345\n",
      "[step: 2833] loss: 0.5462712049484253\n",
      "[step: 2834] loss: 0.6616796255111694\n",
      "[step: 2835] loss: 0.49630871415138245\n",
      "[step: 2836] loss: 0.571528434753418\n",
      "[step: 2837] loss: 0.47349026799201965\n",
      "[step: 2838] loss: 0.566100001335144\n",
      "[step: 2839] loss: 0.47555235028266907\n",
      "[step: 2840] loss: 0.5304540991783142\n",
      "[step: 2841] loss: 0.48293936252593994\n",
      "[step: 2842] loss: 0.49975138902664185\n",
      "[step: 2843] loss: 0.46679186820983887\n",
      "[step: 2844] loss: 0.46756020188331604\n",
      "[step: 2845] loss: 0.45052284002304077\n",
      "[step: 2846] loss: 0.4481789171695709\n",
      "[step: 2847] loss: 0.4371490180492401\n",
      "[step: 2848] loss: 0.43700751662254333\n",
      "[step: 2849] loss: 0.4357205629348755\n",
      "[step: 2850] loss: 0.42550128698349\n",
      "[step: 2851] loss: 0.4280601143836975\n",
      "[step: 2852] loss: 0.41650456190109253\n",
      "[step: 2853] loss: 0.41996651887893677\n",
      "[step: 2854] loss: 0.4015347361564636\n",
      "[step: 2855] loss: 0.4129126965999603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2856] loss: 0.3947611153125763\n",
      "[step: 2857] loss: 0.4031354784965515\n",
      "[step: 2858] loss: 0.39090695977211\n",
      "[step: 2859] loss: 0.39704447984695435\n",
      "[step: 2860] loss: 0.38879942893981934\n",
      "[step: 2861] loss: 0.3903144896030426\n",
      "[step: 2862] loss: 0.3837968409061432\n",
      "[step: 2863] loss: 0.38299527764320374\n",
      "[step: 2864] loss: 0.38022851943969727\n",
      "[step: 2865] loss: 0.376761257648468\n",
      "[step: 2866] loss: 0.3806050717830658\n",
      "[step: 2867] loss: 0.3739748001098633\n",
      "[step: 2868] loss: 0.37521952390670776\n",
      "[step: 2869] loss: 0.3701139986515045\n",
      "[step: 2870] loss: 0.3691410720348358\n",
      "[step: 2871] loss: 0.36809250712394714\n",
      "[step: 2872] loss: 0.3655892312526703\n",
      "[step: 2873] loss: 0.366792231798172\n",
      "[step: 2874] loss: 0.36357077956199646\n",
      "[step: 2875] loss: 0.36281725764274597\n",
      "[step: 2876] loss: 0.36142951250076294\n",
      "[step: 2877] loss: 0.3592536747455597\n",
      "[step: 2878] loss: 0.3588114082813263\n",
      "[step: 2879] loss: 0.35656702518463135\n",
      "[step: 2880] loss: 0.3556572198867798\n",
      "[step: 2881] loss: 0.3548648953437805\n",
      "[step: 2882] loss: 0.35273057222366333\n",
      "[step: 2883] loss: 0.35203835368156433\n",
      "[step: 2884] loss: 0.350986123085022\n",
      "[step: 2885] loss: 0.3490910232067108\n",
      "[step: 2886] loss: 0.3485097587108612\n",
      "[step: 2887] loss: 0.34747254848480225\n",
      "[step: 2888] loss: 0.3458574414253235\n",
      "[step: 2889] loss: 0.34534135460853577\n",
      "[step: 2890] loss: 0.34422561526298523\n",
      "[step: 2891] loss: 0.34264466166496277\n",
      "[step: 2892] loss: 0.3419765532016754\n",
      "[step: 2893] loss: 0.341046005487442\n",
      "[step: 2894] loss: 0.3398562967777252\n",
      "[step: 2895] loss: 0.33903247117996216\n",
      "[step: 2896] loss: 0.3381945788860321\n",
      "[step: 2897] loss: 0.3370554745197296\n",
      "[step: 2898] loss: 0.3359813392162323\n",
      "[step: 2899] loss: 0.33532068133354187\n",
      "[step: 2900] loss: 0.3344409763813019\n",
      "[step: 2901] loss: 0.3333618640899658\n",
      "[step: 2902] loss: 0.3325079381465912\n",
      "[step: 2903] loss: 0.3317265510559082\n",
      "[step: 2904] loss: 0.3308553695678711\n",
      "[step: 2905] loss: 0.32987767457962036\n",
      "[step: 2906] loss: 0.3290175199508667\n",
      "[step: 2907] loss: 0.32824215292930603\n",
      "[step: 2908] loss: 0.32742470502853394\n",
      "[step: 2909] loss: 0.32655367255210876\n",
      "[step: 2910] loss: 0.325635701417923\n",
      "[step: 2911] loss: 0.32480481266975403\n",
      "[step: 2912] loss: 0.3240119218826294\n",
      "[step: 2913] loss: 0.32322463393211365\n",
      "[step: 2914] loss: 0.32241788506507874\n",
      "[step: 2915] loss: 0.32157590985298157\n",
      "[step: 2916] loss: 0.3207351565361023\n",
      "[step: 2917] loss: 0.31991034746170044\n",
      "[step: 2918] loss: 0.3191375434398651\n",
      "[step: 2919] loss: 0.31837862730026245\n",
      "[step: 2920] loss: 0.3176436126232147\n",
      "[step: 2921] loss: 0.31691065430641174\n",
      "[step: 2922] loss: 0.3161781430244446\n",
      "[step: 2923] loss: 0.31546375155448914\n",
      "[step: 2924] loss: 0.31477245688438416\n",
      "[step: 2925] loss: 0.31412994861602783\n",
      "[step: 2926] loss: 0.31355753540992737\n",
      "[step: 2927] loss: 0.3131408095359802\n",
      "[step: 2928] loss: 0.31304654479026794\n",
      "[step: 2929] loss: 0.31371062994003296\n",
      "[step: 2930] loss: 0.3162081241607666\n",
      "[step: 2931] loss: 0.32329490780830383\n",
      "[step: 2932] loss: 0.34265005588531494\n",
      "[step: 2933] loss: 0.3926420211791992\n",
      "[step: 2934] loss: 0.5200130939483643\n",
      "[step: 2935] loss: 0.7722184062004089\n",
      "[step: 2936] loss: 1.053371548652649\n",
      "[step: 2937] loss: 0.9598141312599182\n",
      "[step: 2938] loss: 0.4128488004207611\n",
      "[step: 2939] loss: 0.5811241865158081\n",
      "[step: 2940] loss: 0.7296324372291565\n",
      "[step: 2941] loss: 0.40860429406166077\n",
      "[step: 2942] loss: 0.5379410982131958\n",
      "[step: 2943] loss: 0.5144816637039185\n",
      "[step: 2944] loss: 0.384968101978302\n",
      "[step: 2945] loss: 0.5379695892333984\n",
      "[step: 2946] loss: 0.3864525258541107\n",
      "[step: 2947] loss: 0.42990434169769287\n",
      "[step: 2948] loss: 0.4229785203933716\n",
      "[step: 2949] loss: 0.3744560182094574\n",
      "[step: 2950] loss: 0.424231618642807\n",
      "[step: 2951] loss: 0.36627689003944397\n",
      "[step: 2952] loss: 0.37973228096961975\n",
      "[step: 2953] loss: 0.38113805651664734\n",
      "[step: 2954] loss: 0.3623785376548767\n",
      "[step: 2955] loss: 0.37616991996765137\n",
      "[step: 2956] loss: 0.35232895612716675\n",
      "[step: 2957] loss: 0.35773584246635437\n",
      "[step: 2958] loss: 0.3555753827095032\n",
      "[step: 2959] loss: 0.3431735038757324\n",
      "[step: 2960] loss: 0.3486503064632416\n",
      "[step: 2961] loss: 0.3403514623641968\n",
      "[step: 2962] loss: 0.33742451667785645\n",
      "[step: 2963] loss: 0.34104689955711365\n",
      "[step: 2964] loss: 0.3244274854660034\n",
      "[step: 2965] loss: 0.3342129588127136\n",
      "[step: 2966] loss: 0.3287643492221832\n",
      "[step: 2967] loss: 0.32344144582748413\n",
      "[step: 2968] loss: 0.3309747278690338\n",
      "[step: 2969] loss: 0.31484749913215637\n",
      "[step: 2970] loss: 0.3226620852947235\n",
      "[step: 2971] loss: 0.3198513686656952\n",
      "[step: 2972] loss: 0.3140214681625366\n",
      "[step: 2973] loss: 0.3179570734500885\n",
      "[step: 2974] loss: 0.31288567185401917\n",
      "[step: 2975] loss: 0.3119388222694397\n",
      "[step: 2976] loss: 0.31202155351638794\n",
      "[step: 2977] loss: 0.31150445342063904\n",
      "[step: 2978] loss: 0.305961549282074\n",
      "[step: 2979] loss: 0.3082093298435211\n",
      "[step: 2980] loss: 0.30784475803375244\n",
      "[step: 2981] loss: 0.30365684628486633\n",
      "[step: 2982] loss: 0.30476486682891846\n",
      "[step: 2983] loss: 0.3038296699523926\n",
      "[step: 2984] loss: 0.30233272910118103\n",
      "[step: 2985] loss: 0.3001391589641571\n",
      "[step: 2986] loss: 0.30141815543174744\n",
      "[step: 2987] loss: 0.30028480291366577\n",
      "[step: 2988] loss: 0.29798832535743713\n",
      "[step: 2989] loss: 0.29726335406303406\n",
      "[step: 2990] loss: 0.29736948013305664\n",
      "[step: 2991] loss: 0.29715442657470703\n",
      "[step: 2992] loss: 0.2949022054672241\n",
      "[step: 2993] loss: 0.29420045018196106\n",
      "[step: 2994] loss: 0.2938249707221985\n",
      "[step: 2995] loss: 0.29383569955825806\n",
      "[step: 2996] loss: 0.29248201847076416\n",
      "[step: 2997] loss: 0.2912863492965698\n",
      "[step: 2998] loss: 0.29064691066741943\n",
      "[step: 2999] loss: 0.29046598076820374\n",
      "[step: 3000] loss: 0.290160208940506\n",
      "[step: 3001] loss: 0.2892175614833832\n",
      "[step: 3002] loss: 0.28836628794670105\n",
      "[step: 3003] loss: 0.28742581605911255\n",
      "[step: 3004] loss: 0.2869844138622284\n",
      "[step: 3005] loss: 0.28642845153808594\n",
      "[step: 3006] loss: 0.2861647307872772\n",
      "[step: 3007] loss: 0.2855747640132904\n",
      "[step: 3008] loss: 0.28501468896865845\n",
      "[step: 3009] loss: 0.2842029631137848\n",
      "[step: 3010] loss: 0.2835213840007782\n",
      "[step: 3011] loss: 0.28282374143600464\n",
      "[step: 3012] loss: 0.28226718306541443\n",
      "[step: 3013] loss: 0.2817169725894928\n",
      "[step: 3014] loss: 0.2812362611293793\n",
      "[step: 3015] loss: 0.28079354763031006\n",
      "[step: 3016] loss: 0.2804168462753296\n",
      "[step: 3017] loss: 0.280115008354187\n",
      "[step: 3018] loss: 0.2799282371997833\n",
      "[step: 3019] loss: 0.28003495931625366\n",
      "[step: 3020] loss: 0.2806958556175232\n",
      "[step: 3021] loss: 0.282596617937088\n",
      "[step: 3022] loss: 0.28716614842414856\n",
      "[step: 3023] loss: 0.297791063785553\n",
      "[step: 3024] loss: 0.3216609060764313\n",
      "[step: 3025] loss: 0.37455061078071594\n",
      "[step: 3026] loss: 0.4800160229206085\n",
      "[step: 3027] loss: 0.6492369174957275\n",
      "[step: 3028] loss: 0.7787644863128662\n",
      "[step: 3029] loss: 0.6096686720848083\n",
      "[step: 3030] loss: 0.3309759199619293\n",
      "[step: 3031] loss: 0.3913690745830536\n",
      "[step: 3032] loss: 0.531524121761322\n",
      "[step: 3033] loss: 0.3919680118560791\n",
      "[step: 3034] loss: 0.3018432855606079\n",
      "[step: 3035] loss: 0.41365060210227966\n",
      "[step: 3036] loss: 0.37161049246788025\n",
      "[step: 3037] loss: 0.3058597147464752\n",
      "[step: 3038] loss: 0.3767082691192627\n",
      "[step: 3039] loss: 0.3435111939907074\n",
      "[step: 3040] loss: 0.29593780636787415\n",
      "[step: 3041] loss: 0.3533857762813568\n",
      "[step: 3042] loss: 0.3214013874530792\n",
      "[step: 3043] loss: 0.2939174473285675\n",
      "[step: 3044] loss: 0.3366561233997345\n",
      "[step: 3045] loss: 0.30407172441482544\n",
      "[step: 3046] loss: 0.293107271194458\n",
      "[step: 3047] loss: 0.323723167181015\n",
      "[step: 3048] loss: 0.29934296011924744\n",
      "[step: 3049] loss: 0.28663673996925354\n",
      "[step: 3050] loss: 0.30873221158981323\n",
      "[step: 3051] loss: 0.2919933497905731\n",
      "[step: 3052] loss: 0.28369322419166565\n",
      "[step: 3053] loss: 0.2995296120643616\n",
      "[step: 3054] loss: 0.2878100574016571\n",
      "[step: 3055] loss: 0.2786584794521332\n",
      "[step: 3056] loss: 0.2912663221359253\n",
      "[step: 3057] loss: 0.28583279252052307\n",
      "[step: 3058] loss: 0.2750537395477295\n",
      "[step: 3059] loss: 0.2832888066768646\n",
      "[step: 3060] loss: 0.28289252519607544\n",
      "[step: 3061] loss: 0.2732911705970764\n",
      "[step: 3062] loss: 0.27672070264816284\n",
      "[step: 3063] loss: 0.2805432081222534\n",
      "[step: 3064] loss: 0.273378849029541\n",
      "[step: 3065] loss: 0.27101239562034607\n",
      "[step: 3066] loss: 0.27540940046310425\n",
      "[step: 3067] loss: 0.273949533700943\n",
      "[step: 3068] loss: 0.26875489950180054\n",
      "[step: 3069] loss: 0.26944854855537415\n",
      "[step: 3070] loss: 0.2722143530845642\n",
      "[step: 3071] loss: 0.27042487263679504\n",
      "[step: 3072] loss: 0.26656460762023926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3073] loss: 0.2663848400115967\n",
      "[step: 3074] loss: 0.2680303454399109\n",
      "[step: 3075] loss: 0.26750680804252625\n",
      "[step: 3076] loss: 0.2649487853050232\n",
      "[step: 3077] loss: 0.263609379529953\n",
      "[step: 3078] loss: 0.26417309045791626\n",
      "[step: 3079] loss: 0.26485636830329895\n",
      "[step: 3080] loss: 0.2638219892978668\n",
      "[step: 3081] loss: 0.2620218098163605\n",
      "[step: 3082] loss: 0.26100119948387146\n",
      "[step: 3083] loss: 0.2612857520580292\n",
      "[step: 3084] loss: 0.2616633474826813\n",
      "[step: 3085] loss: 0.26135993003845215\n",
      "[step: 3086] loss: 0.2603299617767334\n",
      "[step: 3087] loss: 0.25912973284721375\n",
      "[step: 3088] loss: 0.25831085443496704\n",
      "[step: 3089] loss: 0.2581126093864441\n",
      "[step: 3090] loss: 0.2581954300403595\n",
      "[step: 3091] loss: 0.2581675350666046\n",
      "[step: 3092] loss: 0.2578970193862915\n",
      "[step: 3093] loss: 0.2573341429233551\n",
      "[step: 3094] loss: 0.2565712630748749\n",
      "[step: 3095] loss: 0.2557910680770874\n",
      "[step: 3096] loss: 0.25514018535614014\n",
      "[step: 3097] loss: 0.2545851171016693\n",
      "[step: 3098] loss: 0.254144549369812\n",
      "[step: 3099] loss: 0.25381073355674744\n",
      "[step: 3100] loss: 0.25351691246032715\n",
      "[step: 3101] loss: 0.25328823924064636\n",
      "[step: 3102] loss: 0.2531619071960449\n",
      "[step: 3103] loss: 0.25318530201911926\n",
      "[step: 3104] loss: 0.2534981966018677\n",
      "[step: 3105] loss: 0.2543466091156006\n",
      "[step: 3106] loss: 0.25624361634254456\n",
      "[step: 3107] loss: 0.26017940044403076\n",
      "[step: 3108] loss: 0.26853954792022705\n",
      "[step: 3109] loss: 0.2857857644557953\n",
      "[step: 3110] loss: 0.32156381011009216\n",
      "[step: 3111] loss: 0.3882645070552826\n",
      "[step: 3112] loss: 0.4985402822494507\n",
      "[step: 3113] loss: 0.6014107465744019\n",
      "[step: 3114] loss: 0.6005687117576599\n",
      "[step: 3115] loss: 0.4062308371067047\n",
      "[step: 3116] loss: 0.26708099246025085\n",
      "[step: 3117] loss: 0.36396482586860657\n",
      "[step: 3118] loss: 0.44667068123817444\n",
      "[step: 3119] loss: 0.3326705992221832\n",
      "[step: 3120] loss: 0.27025359869003296\n",
      "[step: 3121] loss: 0.3592544198036194\n",
      "[step: 3122] loss: 0.36160117387771606\n",
      "[step: 3123] loss: 0.26826727390289307\n",
      "[step: 3124] loss: 0.29581502079963684\n",
      "[step: 3125] loss: 0.34343427419662476\n",
      "[step: 3126] loss: 0.2764546275138855\n",
      "[step: 3127] loss: 0.2736331522464752\n",
      "[step: 3128] loss: 0.3101452589035034\n",
      "[step: 3129] loss: 0.2789166271686554\n",
      "[step: 3130] loss: 0.26528993248939514\n",
      "[step: 3131] loss: 0.29081228375434875\n",
      "[step: 3132] loss: 0.28347691893577576\n",
      "[step: 3133] loss: 0.25378644466400146\n",
      "[step: 3134] loss: 0.275443971157074\n",
      "[step: 3135] loss: 0.280276358127594\n",
      "[step: 3136] loss: 0.2563900947570801\n",
      "[step: 3137] loss: 0.26488444209098816\n",
      "[step: 3138] loss: 0.26824942231178284\n",
      "[step: 3139] loss: 0.2633422613143921\n",
      "[step: 3140] loss: 0.25444573163986206\n",
      "[step: 3141] loss: 0.257894903421402\n",
      "[step: 3142] loss: 0.26445841789245605\n",
      "[step: 3143] loss: 0.2501606047153473\n",
      "[step: 3144] loss: 0.25256526470184326\n",
      "[step: 3145] loss: 0.25752755999565125\n",
      "[step: 3146] loss: 0.25217071175575256\n",
      "[step: 3147] loss: 0.24959279596805573\n",
      "[step: 3148] loss: 0.2476462572813034\n",
      "[step: 3149] loss: 0.25166648626327515\n",
      "[step: 3150] loss: 0.24964480102062225\n",
      "[step: 3151] loss: 0.2442251592874527\n",
      "[step: 3152] loss: 0.24607409536838531\n",
      "[step: 3153] loss: 0.24635988473892212\n",
      "[step: 3154] loss: 0.24583479762077332\n",
      "[step: 3155] loss: 0.24384814500808716\n",
      "[step: 3156] loss: 0.24145835638046265\n",
      "[step: 3157] loss: 0.24321912229061127\n",
      "[step: 3158] loss: 0.24351099133491516\n",
      "[step: 3159] loss: 0.24202828109264374\n",
      "[step: 3160] loss: 0.2410636693239212\n",
      "[step: 3161] loss: 0.23945504426956177\n",
      "[step: 3162] loss: 0.23983408510684967\n",
      "[step: 3163] loss: 0.24049460887908936\n",
      "[step: 3164] loss: 0.23954316973686218\n",
      "[step: 3165] loss: 0.2389720231294632\n",
      "[step: 3166] loss: 0.23785057663917542\n",
      "[step: 3167] loss: 0.2368742972612381\n",
      "[step: 3168] loss: 0.2371240109205246\n",
      "[step: 3169] loss: 0.23696154356002808\n",
      "[step: 3170] loss: 0.2367444634437561\n",
      "[step: 3171] loss: 0.23662912845611572\n",
      "[step: 3172] loss: 0.23573799431324005\n",
      "[step: 3173] loss: 0.23500259220600128\n",
      "[step: 3174] loss: 0.23452386260032654\n",
      "[step: 3175] loss: 0.23382708430290222\n",
      "[step: 3176] loss: 0.23350690305233002\n",
      "[step: 3177] loss: 0.2333538681268692\n",
      "[step: 3178] loss: 0.2330051064491272\n",
      "[step: 3179] loss: 0.2329048067331314\n",
      "[step: 3180] loss: 0.23288996517658234\n",
      "[step: 3181] loss: 0.232809916138649\n",
      "[step: 3182] loss: 0.23307767510414124\n",
      "[step: 3183] loss: 0.23370395600795746\n",
      "[step: 3184] loss: 0.23477116227149963\n",
      "[step: 3185] loss: 0.23712457716464996\n",
      "[step: 3186] loss: 0.24167032539844513\n",
      "[step: 3187] loss: 0.25039219856262207\n",
      "[step: 3188] loss: 0.2672927677631378\n",
      "[step: 3189] loss: 0.30060142278671265\n",
      "[step: 3190] loss: 0.3587476909160614\n",
      "[step: 3191] loss: 0.4548354744911194\n",
      "[step: 3192] loss: 0.5492880940437317\n",
      "[step: 3193] loss: 0.5745818018913269\n",
      "[step: 3194] loss: 0.4379240572452545\n",
      "[step: 3195] loss: 0.26732590794563293\n",
      "[step: 3196] loss: 0.27642497420310974\n",
      "[step: 3197] loss: 0.38481345772743225\n",
      "[step: 3198] loss: 0.3539524972438812\n",
      "[step: 3199] loss: 0.2507684826850891\n",
      "[step: 3200] loss: 0.2698603570461273\n",
      "[step: 3201] loss: 0.32895025610923767\n",
      "[step: 3202] loss: 0.2864985764026642\n",
      "[step: 3203] loss: 0.24414390325546265\n",
      "[step: 3204] loss: 0.27052292227745056\n",
      "[step: 3205] loss: 0.2835365831851959\n",
      "[step: 3206] loss: 0.2528294622898102\n",
      "[step: 3207] loss: 0.2440568506717682\n",
      "[step: 3208] loss: 0.2666769027709961\n",
      "[step: 3209] loss: 0.2640041410923004\n",
      "[step: 3210] loss: 0.23818230628967285\n",
      "[step: 3211] loss: 0.24316754937171936\n",
      "[step: 3212] loss: 0.2598685920238495\n",
      "[step: 3213] loss: 0.2481292337179184\n",
      "[step: 3214] loss: 0.23216278851032257\n",
      "[step: 3215] loss: 0.23886780440807343\n",
      "[step: 3216] loss: 0.24782727658748627\n",
      "[step: 3217] loss: 0.2381620556116104\n",
      "[step: 3218] loss: 0.22842170298099518\n",
      "[step: 3219] loss: 0.23524586856365204\n",
      "[step: 3220] loss: 0.24105069041252136\n",
      "[step: 3221] loss: 0.23248706758022308\n",
      "[step: 3222] loss: 0.22658322751522064\n",
      "[step: 3223] loss: 0.23206423223018646\n",
      "[step: 3224] loss: 0.23508749902248383\n",
      "[step: 3225] loss: 0.2295689433813095\n",
      "[step: 3226] loss: 0.22467859089374542\n",
      "[step: 3227] loss: 0.22682490944862366\n",
      "[step: 3228] loss: 0.22974196076393127\n",
      "[step: 3229] loss: 0.2280794382095337\n",
      "[step: 3230] loss: 0.22432856261730194\n",
      "[step: 3231] loss: 0.2228897511959076\n",
      "[step: 3232] loss: 0.2241155356168747\n",
      "[step: 3233] loss: 0.22515471279621124\n",
      "[step: 3234] loss: 0.22439567744731903\n",
      "[step: 3235] loss: 0.22221535444259644\n",
      "[step: 3236] loss: 0.22058463096618652\n",
      "[step: 3237] loss: 0.22044454514980316\n",
      "[step: 3238] loss: 0.22144246101379395\n",
      "[step: 3239] loss: 0.22188164293766022\n",
      "[step: 3240] loss: 0.22086289525032043\n",
      "[step: 3241] loss: 0.21909965574741364\n",
      "[step: 3242] loss: 0.21797949075698853\n",
      "[step: 3243] loss: 0.21777509152889252\n",
      "[step: 3244] loss: 0.21788302063941956\n",
      "[step: 3245] loss: 0.21791693568229675\n",
      "[step: 3246] loss: 0.21776582300662994\n",
      "[step: 3247] loss: 0.2175283432006836\n",
      "[step: 3248] loss: 0.21714068949222565\n",
      "[step: 3249] loss: 0.21658030152320862\n",
      "[step: 3250] loss: 0.21584922075271606\n",
      "[step: 3251] loss: 0.2151004523038864\n",
      "[step: 3252] loss: 0.2144940048456192\n",
      "[step: 3253] loss: 0.21406203508377075\n",
      "[step: 3254] loss: 0.21370141208171844\n",
      "[step: 3255] loss: 0.2134014219045639\n",
      "[step: 3256] loss: 0.2130841761827469\n",
      "[step: 3257] loss: 0.2128344029188156\n",
      "[step: 3258] loss: 0.21271392703056335\n",
      "[step: 3259] loss: 0.21291035413742065\n",
      "[step: 3260] loss: 0.2137567698955536\n",
      "[step: 3261] loss: 0.2161010503768921\n",
      "[step: 3262] loss: 0.22163845598697662\n",
      "[step: 3263] loss: 0.2349075824022293\n",
      "[step: 3264] loss: 0.2629120945930481\n",
      "[step: 3265] loss: 0.3275260031223297\n",
      "[step: 3266] loss: 0.4315408170223236\n",
      "[step: 3267] loss: 0.6170271635055542\n",
      "[step: 3268] loss: 0.7959206104278564\n",
      "[step: 3269] loss: 0.8027610778808594\n",
      "[step: 3270] loss: 0.40455731749534607\n",
      "[step: 3271] loss: 0.34367766976356506\n",
      "[step: 3272] loss: 0.5431138277053833\n",
      "[step: 3273] loss: 0.3637387454509735\n",
      "[step: 3274] loss: 0.3539051115512848\n",
      "[step: 3275] loss: 0.35191309452056885\n",
      "[step: 3276] loss: 0.26758384704589844\n",
      "[step: 3277] loss: 0.36397644877433777\n",
      "[step: 3278] loss: 0.238067626953125\n",
      "[step: 3279] loss: 0.3256604075431824\n",
      "[step: 3280] loss: 0.2601363956928253\n",
      "[step: 3281] loss: 0.29022887349128723\n",
      "[step: 3282] loss: 0.2510959208011627\n",
      "[step: 3283] loss: 0.27196139097213745\n",
      "[step: 3284] loss: 0.24693433940410614\n",
      "[step: 3285] loss: 0.2617540955543518\n",
      "[step: 3286] loss: 0.2428545355796814\n",
      "[step: 3287] loss: 0.2536340355873108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3288] loss: 0.2378523200750351\n",
      "[step: 3289] loss: 0.24287159740924835\n",
      "[step: 3290] loss: 0.23790033161640167\n",
      "[step: 3291] loss: 0.23683717846870422\n",
      "[step: 3292] loss: 0.23599374294281006\n",
      "[step: 3293] loss: 0.23139774799346924\n",
      "[step: 3294] loss: 0.2353752851486206\n",
      "[step: 3295] loss: 0.22619350254535675\n",
      "[step: 3296] loss: 0.23196114599704742\n",
      "[step: 3297] loss: 0.222762331366539\n",
      "[step: 3298] loss: 0.23117688298225403\n",
      "[step: 3299] loss: 0.21944743394851685\n",
      "[step: 3300] loss: 0.22598260641098022\n",
      "[step: 3301] loss: 0.21895766258239746\n",
      "[step: 3302] loss: 0.2241564244031906\n",
      "[step: 3303] loss: 0.2186565101146698\n",
      "[step: 3304] loss: 0.21818086504936218\n",
      "[step: 3305] loss: 0.21777008473873138\n",
      "[step: 3306] loss: 0.21541306376457214\n",
      "[step: 3307] loss: 0.21817313134670258\n",
      "[step: 3308] loss: 0.2129090279340744\n",
      "[step: 3309] loss: 0.21483033895492554\n",
      "[step: 3310] loss: 0.21217310428619385\n",
      "[step: 3311] loss: 0.21254438161849976\n",
      "[step: 3312] loss: 0.2124316394329071\n",
      "[step: 3313] loss: 0.20985817909240723\n",
      "[step: 3314] loss: 0.21087126433849335\n",
      "[step: 3315] loss: 0.20842938125133514\n",
      "[step: 3316] loss: 0.20871776342391968\n",
      "[step: 3317] loss: 0.20852401852607727\n",
      "[step: 3318] loss: 0.20683008432388306\n",
      "[step: 3319] loss: 0.20755472779273987\n",
      "[step: 3320] loss: 0.20606517791748047\n",
      "[step: 3321] loss: 0.20543503761291504\n",
      "[step: 3322] loss: 0.20559604465961456\n",
      "[step: 3323] loss: 0.20435966551303864\n",
      "[step: 3324] loss: 0.20418119430541992\n",
      "[step: 3325] loss: 0.20426560938358307\n",
      "[step: 3326] loss: 0.20322059094905853\n",
      "[step: 3327] loss: 0.2031632661819458\n",
      "[step: 3328] loss: 0.20298254489898682\n",
      "[step: 3329] loss: 0.20209166407585144\n",
      "[step: 3330] loss: 0.20194315910339355\n",
      "[step: 3331] loss: 0.20195674896240234\n",
      "[step: 3332] loss: 0.2014816552400589\n",
      "[step: 3333] loss: 0.20155231654644012\n",
      "[step: 3334] loss: 0.20231316983699799\n",
      "[step: 3335] loss: 0.20324984192848206\n",
      "[step: 3336] loss: 0.20577967166900635\n",
      "[step: 3337] loss: 0.21137380599975586\n",
      "[step: 3338] loss: 0.2233726978302002\n",
      "[step: 3339] loss: 0.24303250014781952\n",
      "[step: 3340] loss: 0.2875061333179474\n",
      "[step: 3341] loss: 0.34064429998397827\n",
      "[step: 3342] loss: 0.4195992648601532\n",
      "[step: 3343] loss: 0.40123656392097473\n",
      "[step: 3344] loss: 0.32705968618392944\n",
      "[step: 3345] loss: 0.24019116163253784\n",
      "[step: 3346] loss: 0.2155642956495285\n",
      "[step: 3347] loss: 0.27694860100746155\n",
      "[step: 3348] loss: 0.3134618401527405\n",
      "[step: 3349] loss: 0.2548900544643402\n",
      "[step: 3350] loss: 0.2278898060321808\n",
      "[step: 3351] loss: 0.24415767192840576\n",
      "[step: 3352] loss: 0.21779084205627441\n",
      "[step: 3353] loss: 0.24319614470005035\n",
      "[step: 3354] loss: 0.26172542572021484\n",
      "[step: 3355] loss: 0.20278583467006683\n",
      "[step: 3356] loss: 0.2568757236003876\n",
      "[step: 3357] loss: 0.2586383521556854\n",
      "[step: 3358] loss: 0.21678756177425385\n",
      "[step: 3359] loss: 0.2770877778530121\n",
      "[step: 3360] loss: 0.2158837616443634\n",
      "[step: 3361] loss: 0.22772584855556488\n",
      "[step: 3362] loss: 0.24775654077529907\n",
      "[step: 3363] loss: 0.20001156628131866\n",
      "[step: 3364] loss: 0.23116165399551392\n",
      "[step: 3365] loss: 0.21028603613376617\n",
      "[step: 3366] loss: 0.20220313966274261\n",
      "[step: 3367] loss: 0.22200043499469757\n",
      "[step: 3368] loss: 0.1978960782289505\n",
      "[step: 3369] loss: 0.2137647569179535\n",
      "[step: 3370] loss: 0.21274976432323456\n",
      "[step: 3371] loss: 0.19811394810676575\n",
      "[step: 3372] loss: 0.2198425531387329\n",
      "[step: 3373] loss: 0.20276020467281342\n",
      "[step: 3374] loss: 0.20032228529453278\n",
      "[step: 3375] loss: 0.21459753811359406\n",
      "[step: 3376] loss: 0.19643612205982208\n",
      "[step: 3377] loss: 0.20066045224666595\n",
      "[step: 3378] loss: 0.20509104430675507\n",
      "[step: 3379] loss: 0.19247394800186157\n",
      "[step: 3380] loss: 0.1986074447631836\n",
      "[step: 3381] loss: 0.1981203407049179\n",
      "[step: 3382] loss: 0.1913333386182785\n",
      "[step: 3383] loss: 0.19610841572284698\n",
      "[step: 3384] loss: 0.19412103295326233\n",
      "[step: 3385] loss: 0.1900259405374527\n",
      "[step: 3386] loss: 0.1936260312795639\n",
      "[step: 3387] loss: 0.19201821088790894\n",
      "[step: 3388] loss: 0.18919937312602997\n",
      "[step: 3389] loss: 0.19202953577041626\n",
      "[step: 3390] loss: 0.19176168739795685\n",
      "[step: 3391] loss: 0.19052520394325256\n",
      "[step: 3392] loss: 0.1948927640914917\n",
      "[step: 3393] loss: 0.20151257514953613\n",
      "[step: 3394] loss: 0.21272267401218414\n",
      "[step: 3395] loss: 0.24462293088436127\n",
      "[step: 3396] loss: 0.2955256998538971\n",
      "[step: 3397] loss: 0.40820151567459106\n",
      "[step: 3398] loss: 0.49082568287849426\n",
      "[step: 3399] loss: 0.5506592988967896\n",
      "[step: 3400] loss: 0.4516633450984955\n",
      "[step: 3401] loss: 0.24854204058647156\n",
      "[step: 3402] loss: 0.2192985862493515\n",
      "[step: 3403] loss: 0.3341653048992157\n",
      "[step: 3404] loss: 0.3381490409374237\n",
      "[step: 3405] loss: 0.2671862244606018\n",
      "[step: 3406] loss: 0.23610414564609528\n",
      "[step: 3407] loss: 0.22496052086353302\n",
      "[step: 3408] loss: 0.2818695902824402\n",
      "[step: 3409] loss: 0.2524990737438202\n",
      "[step: 3410] loss: 0.20851418375968933\n",
      "[step: 3411] loss: 0.2702328562736511\n",
      "[step: 3412] loss: 0.22306965291500092\n",
      "[step: 3413] loss: 0.22854731976985931\n",
      "[step: 3414] loss: 0.218430757522583\n",
      "[step: 3415] loss: 0.208926260471344\n",
      "[step: 3416] loss: 0.2321002185344696\n",
      "[step: 3417] loss: 0.19443561136722565\n",
      "[step: 3418] loss: 0.22082822024822235\n",
      "[step: 3419] loss: 0.21259264647960663\n",
      "[step: 3420] loss: 0.19694803655147552\n",
      "[step: 3421] loss: 0.22329457104206085\n",
      "[step: 3422] loss: 0.19673259556293488\n",
      "[step: 3423] loss: 0.2012031078338623\n",
      "[step: 3424] loss: 0.2078092098236084\n",
      "[step: 3425] loss: 0.18896888196468353\n",
      "[step: 3426] loss: 0.19826434552669525\n",
      "[step: 3427] loss: 0.19222316145896912\n",
      "[step: 3428] loss: 0.18761394917964935\n",
      "[step: 3429] loss: 0.1955975890159607\n",
      "[step: 3430] loss: 0.18729938566684723\n",
      "[step: 3431] loss: 0.18772567808628082\n",
      "[step: 3432] loss: 0.19121482968330383\n",
      "[step: 3433] loss: 0.18435923755168915\n",
      "[step: 3434] loss: 0.18709325790405273\n",
      "[step: 3435] loss: 0.1885824054479599\n",
      "[step: 3436] loss: 0.18298718333244324\n",
      "[step: 3437] loss: 0.18594057857990265\n",
      "[step: 3438] loss: 0.186374694108963\n",
      "[step: 3439] loss: 0.18162569403648376\n",
      "[step: 3440] loss: 0.1839933544397354\n",
      "[step: 3441] loss: 0.18438923358917236\n",
      "[step: 3442] loss: 0.18052151799201965\n",
      "[step: 3443] loss: 0.18191373348236084\n",
      "[step: 3444] loss: 0.18257641792297363\n",
      "[step: 3445] loss: 0.1793619990348816\n",
      "[step: 3446] loss: 0.1798391044139862\n",
      "[step: 3447] loss: 0.1811194270849228\n",
      "[step: 3448] loss: 0.17873531579971313\n",
      "[step: 3449] loss: 0.1780681610107422\n",
      "[step: 3450] loss: 0.17931348085403442\n",
      "[step: 3451] loss: 0.17834390699863434\n",
      "[step: 3452] loss: 0.17691120505332947\n",
      "[step: 3453] loss: 0.1775316447019577\n",
      "[step: 3454] loss: 0.17836248874664307\n",
      "[step: 3455] loss: 0.1786157488822937\n",
      "[step: 3456] loss: 0.18048161268234253\n",
      "[step: 3457] loss: 0.187633216381073\n",
      "[step: 3458] loss: 0.2026708871126175\n",
      "[step: 3459] loss: 0.23921920359134674\n",
      "[step: 3460] loss: 0.30054497718811035\n",
      "[step: 3461] loss: 0.43900442123413086\n",
      "[step: 3462] loss: 0.4757261872291565\n",
      "[step: 3463] loss: 0.4824940860271454\n",
      "[step: 3464] loss: 0.4108753204345703\n",
      "[step: 3465] loss: 0.23180802166461945\n",
      "[step: 3466] loss: 0.21761451661586761\n",
      "[step: 3467] loss: 0.3382006883621216\n",
      "[step: 3468] loss: 0.2844184935092926\n",
      "[step: 3469] loss: 0.25776591897010803\n",
      "[step: 3470] loss: 0.26738449931144714\n",
      "[step: 3471] loss: 0.19156180322170258\n",
      "[step: 3472] loss: 0.2618989646434784\n",
      "[step: 3473] loss: 0.23294584453105927\n",
      "[step: 3474] loss: 0.21521857380867004\n",
      "[step: 3475] loss: 0.2551984190940857\n",
      "[step: 3476] loss: 0.1900377869606018\n",
      "[step: 3477] loss: 0.22911064326763153\n",
      "[step: 3478] loss: 0.18889787793159485\n",
      "[step: 3479] loss: 0.21248428523540497\n",
      "[step: 3480] loss: 0.20776960253715515\n",
      "[step: 3481] loss: 0.19194525480270386\n",
      "[step: 3482] loss: 0.21867693960666656\n",
      "[step: 3483] loss: 0.1845875084400177\n",
      "[step: 3484] loss: 0.1997164934873581\n",
      "[step: 3485] loss: 0.18991544842720032\n",
      "[step: 3486] loss: 0.18283897638320923\n",
      "[step: 3487] loss: 0.19445118308067322\n",
      "[step: 3488] loss: 0.17925234138965607\n",
      "[step: 3489] loss: 0.1929783970117569\n",
      "[step: 3490] loss: 0.18564794957637787\n",
      "[step: 3491] loss: 0.1814056932926178\n",
      "[step: 3492] loss: 0.19011902809143066\n",
      "[step: 3493] loss: 0.17713002860546112\n",
      "[step: 3494] loss: 0.18383075296878815\n",
      "[step: 3495] loss: 0.18436293303966522\n",
      "[step: 3496] loss: 0.17625315487384796\n",
      "[step: 3497] loss: 0.1823081374168396\n",
      "[step: 3498] loss: 0.17547428607940674\n",
      "[step: 3499] loss: 0.17439651489257812\n",
      "[step: 3500] loss: 0.1784297078847885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3501] loss: 0.17361481487751007\n",
      "[step: 3502] loss: 0.1759548783302307\n",
      "[step: 3503] loss: 0.17772996425628662\n",
      "[step: 3504] loss: 0.1742740273475647\n",
      "[step: 3505] loss: 0.17717666923999786\n",
      "[step: 3506] loss: 0.17944994568824768\n",
      "[step: 3507] loss: 0.17715133726596832\n",
      "[step: 3508] loss: 0.18183034658432007\n",
      "[step: 3509] loss: 0.1846497803926468\n",
      "[step: 3510] loss: 0.1856158822774887\n",
      "[step: 3511] loss: 0.19065891206264496\n",
      "[step: 3512] loss: 0.20050227642059326\n",
      "[step: 3513] loss: 0.200020894408226\n",
      "[step: 3514] loss: 0.2092680037021637\n",
      "[step: 3515] loss: 0.21475611627101898\n",
      "[step: 3516] loss: 0.21927312016487122\n",
      "[step: 3517] loss: 0.21888019144535065\n",
      "[step: 3518] loss: 0.2211155891418457\n",
      "[step: 3519] loss: 0.19708852469921112\n",
      "[step: 3520] loss: 0.1858723908662796\n",
      "[step: 3521] loss: 0.17496058344841003\n",
      "[step: 3522] loss: 0.16682223975658417\n",
      "[step: 3523] loss: 0.17469310760498047\n",
      "[step: 3524] loss: 0.1845092475414276\n",
      "[step: 3525] loss: 0.18824057281017303\n",
      "[step: 3526] loss: 0.1993071734905243\n",
      "[step: 3527] loss: 0.22988882660865784\n",
      "[step: 3528] loss: 0.22101983428001404\n",
      "[step: 3529] loss: 0.2215011864900589\n",
      "[step: 3530] loss: 0.21926923096179962\n",
      "[step: 3531] loss: 0.19548572599887848\n",
      "[step: 3532] loss: 0.17062880098819733\n",
      "[step: 3533] loss: 0.1710638403892517\n",
      "[step: 3534] loss: 0.17498962581157684\n",
      "[step: 3535] loss: 0.1795630156993866\n",
      "[step: 3536] loss: 0.1882227212190628\n",
      "[step: 3537] loss: 0.21032612025737762\n",
      "[step: 3538] loss: 0.24293196201324463\n",
      "[step: 3539] loss: 0.22369325160980225\n",
      "[step: 3540] loss: 0.2295941859483719\n",
      "[step: 3541] loss: 0.2219182401895523\n",
      "[step: 3542] loss: 0.1790396124124527\n",
      "[step: 3543] loss: 0.1685522049665451\n",
      "[step: 3544] loss: 0.186686709523201\n",
      "[step: 3545] loss: 0.18232060968875885\n",
      "[step: 3546] loss: 0.19300644099712372\n",
      "[step: 3547] loss: 0.23916007578372955\n",
      "[step: 3548] loss: 0.23461614549160004\n",
      "[step: 3549] loss: 0.23441067337989807\n",
      "[step: 3550] loss: 0.23293481767177582\n",
      "[step: 3551] loss: 0.20608972012996674\n",
      "[step: 3552] loss: 0.16823597252368927\n",
      "[step: 3553] loss: 0.17380549013614655\n",
      "[step: 3554] loss: 0.193532332777977\n",
      "[step: 3555] loss: 0.20197467505931854\n",
      "[step: 3556] loss: 0.20911654829978943\n",
      "[step: 3557] loss: 0.23347359895706177\n",
      "[step: 3558] loss: 0.2596782445907593\n",
      "[step: 3559] loss: 0.20285484194755554\n",
      "[step: 3560] loss: 0.22921693325042725\n",
      "[step: 3561] loss: 0.21168047189712524\n",
      "[step: 3562] loss: 0.17221489548683167\n",
      "[step: 3563] loss: 0.2394038587808609\n",
      "[step: 3564] loss: 0.23406429588794708\n",
      "[step: 3565] loss: 0.19696232676506042\n",
      "[step: 3566] loss: 0.2644478380680084\n",
      "[step: 3567] loss: 0.25751233100891113\n",
      "[step: 3568] loss: 0.21430671215057373\n",
      "[step: 3569] loss: 0.2632398009300232\n",
      "[step: 3570] loss: 0.1929454356431961\n",
      "[step: 3571] loss: 0.19761431217193604\n",
      "[step: 3572] loss: 0.23135046660900116\n",
      "[step: 3573] loss: 0.2017226368188858\n",
      "[step: 3574] loss: 0.229739710688591\n",
      "[step: 3575] loss: 0.19241741299629211\n",
      "[step: 3576] loss: 0.17461185157299042\n",
      "[step: 3577] loss: 0.1759924292564392\n",
      "[step: 3578] loss: 0.16493752598762512\n",
      "[step: 3579] loss: 0.18393738567829132\n",
      "[step: 3580] loss: 0.17695336043834686\n",
      "[step: 3581] loss: 0.1703260838985443\n",
      "[step: 3582] loss: 0.1727294921875\n",
      "[step: 3583] loss: 0.16023947298526764\n",
      "[step: 3584] loss: 0.165290966629982\n",
      "[step: 3585] loss: 0.16876225173473358\n",
      "[step: 3586] loss: 0.16946373879909515\n",
      "[step: 3587] loss: 0.1785944551229477\n",
      "[step: 3588] loss: 0.18074184656143188\n",
      "[step: 3589] loss: 0.18237315118312836\n",
      "[step: 3590] loss: 0.19401414692401886\n",
      "[step: 3591] loss: 0.1903223693370819\n",
      "[step: 3592] loss: 0.19069840013980865\n",
      "[step: 3593] loss: 0.18799962103366852\n",
      "[step: 3594] loss: 0.17863783240318298\n",
      "[step: 3595] loss: 0.16821709275245667\n",
      "[step: 3596] loss: 0.1700531244277954\n",
      "[step: 3597] loss: 0.15862315893173218\n",
      "[step: 3598] loss: 0.15756337344646454\n",
      "[step: 3599] loss: 0.16063985228538513\n",
      "[step: 3600] loss: 0.1572805941104889\n",
      "[step: 3601] loss: 0.16353870928287506\n",
      "[step: 3602] loss: 0.16992506384849548\n",
      "[step: 3603] loss: 0.17651070654392242\n",
      "[step: 3604] loss: 0.1861240416765213\n",
      "[step: 3605] loss: 0.21470136940479279\n",
      "[step: 3606] loss: 0.21984829008579254\n",
      "[step: 3607] loss: 0.23663461208343506\n",
      "[step: 3608] loss: 0.24375800788402557\n",
      "[step: 3609] loss: 0.22948867082595825\n",
      "[step: 3610] loss: 0.204194575548172\n",
      "[step: 3611] loss: 0.18859972059726715\n",
      "[step: 3612] loss: 0.1591457575559616\n",
      "[step: 3613] loss: 0.1563197821378708\n",
      "[step: 3614] loss: 0.16852407157421112\n",
      "[step: 3615] loss: 0.17964793741703033\n",
      "[step: 3616] loss: 0.19066131114959717\n",
      "[step: 3617] loss: 0.19037789106369019\n",
      "[step: 3618] loss: 0.18091215193271637\n",
      "[step: 3619] loss: 0.1635679304599762\n",
      "[step: 3620] loss: 0.1584245264530182\n",
      "[step: 3621] loss: 0.15305399894714355\n",
      "[step: 3622] loss: 0.15451250970363617\n",
      "[step: 3623] loss: 0.16097980737686157\n",
      "[step: 3624] loss: 0.17399832606315613\n",
      "[step: 3625] loss: 0.19684602320194244\n",
      "[step: 3626] loss: 0.2011314183473587\n",
      "[step: 3627] loss: 0.22116005420684814\n",
      "[step: 3628] loss: 0.2273760288953781\n",
      "[step: 3629] loss: 0.22338096797466278\n",
      "[step: 3630] loss: 0.19715647399425507\n",
      "[step: 3631] loss: 0.17061874270439148\n",
      "[step: 3632] loss: 0.1524907797574997\n",
      "[step: 3633] loss: 0.1517290621995926\n",
      "[step: 3634] loss: 0.15999169647693634\n",
      "[step: 3635] loss: 0.17247264087200165\n",
      "[step: 3636] loss: 0.19020332396030426\n",
      "[step: 3637] loss: 0.20083433389663696\n",
      "[step: 3638] loss: 0.2145487517118454\n",
      "[step: 3639] loss: 0.18366220593452454\n",
      "[step: 3640] loss: 0.16879728436470032\n",
      "[step: 3641] loss: 0.1646382063627243\n",
      "[step: 3642] loss: 0.15652483701705933\n",
      "[step: 3643] loss: 0.15893258154392242\n",
      "[step: 3644] loss: 0.17445504665374756\n",
      "[step: 3645] loss: 0.18370555341243744\n",
      "[step: 3646] loss: 0.1797126829624176\n",
      "[step: 3647] loss: 0.18778526782989502\n",
      "[step: 3648] loss: 0.1898101419210434\n",
      "[step: 3649] loss: 0.19307604432106018\n",
      "[step: 3650] loss: 0.163945734500885\n",
      "[step: 3651] loss: 0.15775969624519348\n",
      "[step: 3652] loss: 0.16294214129447937\n",
      "[step: 3653] loss: 0.15553836524486542\n",
      "[step: 3654] loss: 0.15532176196575165\n",
      "[step: 3655] loss: 0.1670161336660385\n",
      "[step: 3656] loss: 0.17758463323116302\n",
      "[step: 3657] loss: 0.17938175797462463\n",
      "[step: 3658] loss: 0.19257844984531403\n",
      "[step: 3659] loss: 0.20497608184814453\n",
      "[step: 3660] loss: 0.22183510661125183\n",
      "[step: 3661] loss: 0.18201830983161926\n",
      "[step: 3662] loss: 0.17057929933071136\n",
      "[step: 3663] loss: 0.17429788410663605\n",
      "[step: 3664] loss: 0.15506604313850403\n",
      "[step: 3665] loss: 0.15280257165431976\n",
      "[step: 3666] loss: 0.17184078693389893\n",
      "[step: 3667] loss: 0.18088501691818237\n",
      "[step: 3668] loss: 0.17310574650764465\n",
      "[step: 3669] loss: 0.18190647661685944\n",
      "[step: 3670] loss: 0.18891018629074097\n",
      "[step: 3671] loss: 0.19215701520442963\n",
      "[step: 3672] loss: 0.16230462491512299\n",
      "[step: 3673] loss: 0.15875975787639618\n",
      "[step: 3674] loss: 0.16510127484798431\n",
      "[step: 3675] loss: 0.15136472880840302\n",
      "[step: 3676] loss: 0.150223046541214\n",
      "[step: 3677] loss: 0.16695363819599152\n",
      "[step: 3678] loss: 0.1793859899044037\n",
      "[step: 3679] loss: 0.18494156002998352\n",
      "[step: 3680] loss: 0.21485115587711334\n",
      "[step: 3681] loss: 0.23525604605674744\n",
      "[step: 3682] loss: 0.2585047781467438\n",
      "[step: 3683] loss: 0.20451098680496216\n",
      "[step: 3684] loss: 0.20334362983703613\n",
      "[step: 3685] loss: 0.2017018347978592\n",
      "[step: 3686] loss: 0.15009194612503052\n",
      "[step: 3687] loss: 0.17126283049583435\n",
      "[step: 3688] loss: 0.20808225870132446\n",
      "[step: 3689] loss: 0.16578635573387146\n",
      "[step: 3690] loss: 0.16146360337734222\n",
      "[step: 3691] loss: 0.1975371092557907\n",
      "[step: 3692] loss: 0.151092991232872\n",
      "[step: 3693] loss: 0.1546040177345276\n",
      "[step: 3694] loss: 0.1850748211145401\n",
      "[step: 3695] loss: 0.14938320219516754\n",
      "[step: 3696] loss: 0.15633054077625275\n",
      "[step: 3697] loss: 0.17420528829097748\n",
      "[step: 3698] loss: 0.14451384544372559\n",
      "[step: 3699] loss: 0.15103116631507874\n",
      "[step: 3700] loss: 0.16688472032546997\n",
      "[step: 3701] loss: 0.14296723902225494\n",
      "[step: 3702] loss: 0.1453225314617157\n",
      "[step: 3703] loss: 0.16207002103328705\n",
      "[step: 3704] loss: 0.143045112490654\n",
      "[step: 3705] loss: 0.14143618941307068\n",
      "[step: 3706] loss: 0.15587644279003143\n",
      "[step: 3707] loss: 0.1414114385843277\n",
      "[step: 3708] loss: 0.13725152611732483\n",
      "[step: 3709] loss: 0.1461723893880844\n",
      "[step: 3710] loss: 0.14238569140434265\n",
      "[step: 3711] loss: 0.1352262794971466\n",
      "[step: 3712] loss: 0.13894426822662354\n",
      "[step: 3713] loss: 0.14686892926692963\n",
      "[step: 3714] loss: 0.15333732962608337\n",
      "[step: 3715] loss: 0.17783616483211517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3716] loss: 0.2560655176639557\n",
      "[step: 3717] loss: 0.5062583684921265\n",
      "[step: 3718] loss: 0.630992591381073\n",
      "[step: 3719] loss: 0.839268147945404\n",
      "[step: 3720] loss: 0.8850067853927612\n",
      "[step: 3721] loss: 0.3014296591281891\n",
      "[step: 3722] loss: 0.4121714234352112\n",
      "[step: 3723] loss: 0.46267247200012207\n",
      "[step: 3724] loss: 0.4460952579975128\n",
      "[step: 3725] loss: 0.34659773111343384\n",
      "[step: 3726] loss: 0.29085955023765564\n",
      "[step: 3727] loss: 0.355659157037735\n",
      "[step: 3728] loss: 0.25621581077575684\n",
      "[step: 3729] loss: 0.2852320373058319\n",
      "[step: 3730] loss: 0.2682543992996216\n",
      "[step: 3731] loss: 0.2580738365650177\n",
      "[step: 3732] loss: 0.23280659317970276\n",
      "[step: 3733] loss: 0.23655818402767181\n",
      "[step: 3734] loss: 0.20228774845600128\n",
      "[step: 3735] loss: 0.24932444095611572\n",
      "[step: 3736] loss: 0.20226174592971802\n",
      "[step: 3737] loss: 0.2290066033601761\n",
      "[step: 3738] loss: 0.179424449801445\n",
      "[step: 3739] loss: 0.20292267203330994\n",
      "[step: 3740] loss: 0.18787439167499542\n",
      "[step: 3741] loss: 0.17555534839630127\n",
      "[step: 3742] loss: 0.18281206488609314\n",
      "[step: 3743] loss: 0.17565251886844635\n",
      "[step: 3744] loss: 0.1727989912033081\n",
      "[step: 3745] loss: 0.16630049049854279\n",
      "[step: 3746] loss: 0.17534320056438446\n",
      "[step: 3747] loss: 0.15933652222156525\n",
      "[step: 3748] loss: 0.1647765040397644\n",
      "[step: 3749] loss: 0.16039972007274628\n",
      "[step: 3750] loss: 0.16255682706832886\n",
      "[step: 3751] loss: 0.1561000943183899\n",
      "[step: 3752] loss: 0.15230633318424225\n",
      "[step: 3753] loss: 0.15720967948436737\n",
      "[step: 3754] loss: 0.15056169033050537\n",
      "[step: 3755] loss: 0.15510956943035126\n",
      "[step: 3756] loss: 0.14464733004570007\n",
      "[step: 3757] loss: 0.15021640062332153\n",
      "[step: 3758] loss: 0.14633294939994812\n",
      "[step: 3759] loss: 0.14887496829032898\n",
      "[step: 3760] loss: 0.1438278704881668\n",
      "[step: 3761] loss: 0.14342458546161652\n",
      "[step: 3762] loss: 0.14210517704486847\n",
      "[step: 3763] loss: 0.14258362352848053\n",
      "[step: 3764] loss: 0.14045187830924988\n",
      "[step: 3765] loss: 0.13969722390174866\n",
      "[step: 3766] loss: 0.1386374980211258\n",
      "[step: 3767] loss: 0.1382177323102951\n",
      "[step: 3768] loss: 0.13763177394866943\n",
      "[step: 3769] loss: 0.1370629072189331\n",
      "[step: 3770] loss: 0.13573889434337616\n",
      "[step: 3771] loss: 0.13496531546115875\n",
      "[step: 3772] loss: 0.1348332017660141\n",
      "[step: 3773] loss: 0.13374543190002441\n",
      "[step: 3774] loss: 0.13426358997821808\n",
      "[step: 3775] loss: 0.1330186426639557\n",
      "[step: 3776] loss: 0.13285447657108307\n",
      "[step: 3777] loss: 0.13190704584121704\n",
      "[step: 3778] loss: 0.13127508759498596\n",
      "[step: 3779] loss: 0.13088299334049225\n",
      "[step: 3780] loss: 0.13006870448589325\n",
      "[step: 3781] loss: 0.12982957065105438\n",
      "[step: 3782] loss: 0.12949274480342865\n",
      "[step: 3783] loss: 0.12912023067474365\n",
      "[step: 3784] loss: 0.12896618247032166\n",
      "[step: 3785] loss: 0.12896877527236938\n",
      "[step: 3786] loss: 0.1291014403104782\n",
      "[step: 3787] loss: 0.13025899231433868\n",
      "[step: 3788] loss: 0.1326950192451477\n",
      "[step: 3789] loss: 0.13736723363399506\n",
      "[step: 3790] loss: 0.14962145686149597\n",
      "[step: 3791] loss: 0.1702738255262375\n",
      "[step: 3792] loss: 0.22460047900676727\n",
      "[step: 3793] loss: 0.26032382249832153\n",
      "[step: 3794] loss: 0.331648051738739\n",
      "[step: 3795] loss: 0.2525046169757843\n",
      "[step: 3796] loss: 0.16576924920082092\n",
      "[step: 3797] loss: 0.1476856917142868\n",
      "[step: 3798] loss: 0.20498226583003998\n",
      "[step: 3799] loss: 0.22842390835285187\n",
      "[step: 3800] loss: 0.17550128698349\n",
      "[step: 3801] loss: 0.14472408592700958\n",
      "[step: 3802] loss: 0.14711301028728485\n",
      "[step: 3803] loss: 0.16825202107429504\n",
      "[step: 3804] loss: 0.17616330087184906\n",
      "[step: 3805] loss: 0.1359952986240387\n",
      "[step: 3806] loss: 0.1423669308423996\n",
      "[step: 3807] loss: 0.16525472700595856\n",
      "[step: 3808] loss: 0.156065434217453\n",
      "[step: 3809] loss: 0.14272376894950867\n",
      "[step: 3810] loss: 0.12987753748893738\n",
      "[step: 3811] loss: 0.1397923231124878\n",
      "[step: 3812] loss: 0.15720148384571075\n",
      "[step: 3813] loss: 0.15126816928386688\n",
      "[step: 3814] loss: 0.14749720692634583\n",
      "[step: 3815] loss: 0.13712245225906372\n",
      "[step: 3816] loss: 0.12531179189682007\n",
      "[step: 3817] loss: 0.12673291563987732\n",
      "[step: 3818] loss: 0.13351811468601227\n",
      "[step: 3819] loss: 0.13594713807106018\n",
      "[step: 3820] loss: 0.1389778107404709\n",
      "[step: 3821] loss: 0.14122144877910614\n",
      "[step: 3822] loss: 0.13714949786663055\n",
      "[step: 3823] loss: 0.13907580077648163\n",
      "[step: 3824] loss: 0.13592834770679474\n",
      "[step: 3825] loss: 0.13129349052906036\n",
      "[step: 3826] loss: 0.1278141736984253\n",
      "[step: 3827] loss: 0.1246972307562828\n",
      "[step: 3828] loss: 0.11986439675092697\n",
      "[step: 3829] loss: 0.11987446248531342\n",
      "[step: 3830] loss: 0.1215238943696022\n",
      "[step: 3831] loss: 0.12077012658119202\n",
      "[step: 3832] loss: 0.12344333529472351\n",
      "[step: 3833] loss: 0.12898334860801697\n",
      "[step: 3834] loss: 0.13846661150455475\n",
      "[step: 3835] loss: 0.15050941705703735\n",
      "[step: 3836] loss: 0.1792987883090973\n",
      "[step: 3837] loss: 0.19837351143360138\n",
      "[step: 3838] loss: 0.23143595457077026\n",
      "[step: 3839] loss: 0.20457664132118225\n",
      "[step: 3840] loss: 0.1720178723335266\n",
      "[step: 3841] loss: 0.12856534123420715\n",
      "[step: 3842] loss: 0.12444046884775162\n",
      "[step: 3843] loss: 0.1533518135547638\n",
      "[step: 3844] loss: 0.18220165371894836\n",
      "[step: 3845] loss: 0.19690155982971191\n",
      "[step: 3846] loss: 0.16496750712394714\n",
      "[step: 3847] loss: 0.1307963728904724\n",
      "[step: 3848] loss: 0.120559923350811\n",
      "[step: 3849] loss: 0.1406055986881256\n",
      "[step: 3850] loss: 0.16193974018096924\n",
      "[step: 3851] loss: 0.15629030764102936\n",
      "[step: 3852] loss: 0.14275819063186646\n",
      "[step: 3853] loss: 0.12251576781272888\n",
      "[step: 3854] loss: 0.1169847920536995\n",
      "[step: 3855] loss: 0.12362237274646759\n",
      "[step: 3856] loss: 0.1360265612602234\n",
      "[step: 3857] loss: 0.1466466188430786\n",
      "[step: 3858] loss: 0.14425653219223022\n",
      "[step: 3859] loss: 0.14026953279972076\n",
      "[step: 3860] loss: 0.1261313259601593\n",
      "[step: 3861] loss: 0.1164410188794136\n",
      "[step: 3862] loss: 0.11355913430452347\n",
      "[step: 3863] loss: 0.12003830820322037\n",
      "[step: 3864] loss: 0.13196390867233276\n",
      "[step: 3865] loss: 0.14887593686580658\n",
      "[step: 3866] loss: 0.18371006846427917\n",
      "[step: 3867] loss: 0.19285406172275543\n",
      "[step: 3868] loss: 0.20063987374305725\n",
      "[step: 3869] loss: 0.15026992559432983\n",
      "[step: 3870] loss: 0.12455734610557556\n",
      "[step: 3871] loss: 0.132111594080925\n",
      "[step: 3872] loss: 0.15010060369968414\n",
      "[step: 3873] loss: 0.18213647603988647\n",
      "[step: 3874] loss: 0.18586599826812744\n",
      "[step: 3875] loss: 0.16653768718242645\n",
      "[step: 3876] loss: 0.13021384179592133\n",
      "[step: 3877] loss: 0.11399403214454651\n",
      "[step: 3878] loss: 0.12916502356529236\n",
      "[step: 3879] loss: 0.14733348786830902\n",
      "[step: 3880] loss: 0.15314719080924988\n",
      "[step: 3881] loss: 0.13865388929843903\n",
      "[step: 3882] loss: 0.12629081308841705\n",
      "[step: 3883] loss: 0.11631772667169571\n",
      "[step: 3884] loss: 0.1138712614774704\n",
      "[step: 3885] loss: 0.11775459349155426\n",
      "[step: 3886] loss: 0.12392514944076538\n",
      "[step: 3887] loss: 0.1374317705631256\n",
      "[step: 3888] loss: 0.1381710171699524\n",
      "[step: 3889] loss: 0.13729067146778107\n",
      "[step: 3890] loss: 0.12364187836647034\n",
      "[step: 3891] loss: 0.11562543362379074\n",
      "[step: 3892] loss: 0.11393256485462189\n",
      "[step: 3893] loss: 0.11073234677314758\n",
      "[step: 3894] loss: 0.10866785049438477\n",
      "[step: 3895] loss: 0.11230312287807465\n",
      "[step: 3896] loss: 0.11976934969425201\n",
      "[step: 3897] loss: 0.12893082201480865\n",
      "[step: 3898] loss: 0.14405208826065063\n",
      "[step: 3899] loss: 0.16467632353305817\n",
      "[step: 3900] loss: 0.22421444952487946\n",
      "[step: 3901] loss: 0.2106916606426239\n",
      "[step: 3902] loss: 0.20202656090259552\n",
      "[step: 3903] loss: 0.1326250582933426\n",
      "[step: 3904] loss: 0.136275514960289\n",
      "[step: 3905] loss: 0.19242823123931885\n",
      "[step: 3906] loss: 0.16642439365386963\n",
      "[step: 3907] loss: 0.16858503222465515\n",
      "[step: 3908] loss: 0.18733814358711243\n",
      "[step: 3909] loss: 0.16566674411296844\n",
      "[step: 3910] loss: 0.12934432923793793\n",
      "[step: 3911] loss: 0.11402397602796555\n",
      "[step: 3912] loss: 0.14235062897205353\n",
      "[step: 3913] loss: 0.14150682091712952\n",
      "[step: 3914] loss: 0.11962836980819702\n",
      "[step: 3915] loss: 0.12023019790649414\n",
      "[step: 3916] loss: 0.1367560625076294\n",
      "[step: 3917] loss: 0.12149805575609207\n",
      "[step: 3918] loss: 0.1076488345861435\n",
      "[step: 3919] loss: 0.11688756942749023\n",
      "[step: 3920] loss: 0.11988963931798935\n",
      "[step: 3921] loss: 0.11162392050027847\n",
      "[step: 3922] loss: 0.10640227049589157\n",
      "[step: 3923] loss: 0.11509350687265396\n",
      "[step: 3924] loss: 0.11550851166248322\n",
      "[step: 3925] loss: 0.10673607140779495\n",
      "[step: 3926] loss: 0.10619939118623734\n",
      "[step: 3927] loss: 0.11476278305053711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3928] loss: 0.11760343611240387\n",
      "[step: 3929] loss: 0.12103719264268875\n",
      "[step: 3930] loss: 0.130512073636055\n",
      "[step: 3931] loss: 0.1655290126800537\n",
      "[step: 3932] loss: 0.21881335973739624\n",
      "[step: 3933] loss: 0.3348407447338104\n",
      "[step: 3934] loss: 0.37549230456352234\n",
      "[step: 3935] loss: 0.3708674907684326\n",
      "[step: 3936] loss: 0.20668382942676544\n",
      "[step: 3937] loss: 0.26028749346733093\n",
      "[step: 3938] loss: 0.25142019987106323\n",
      "[step: 3939] loss: 0.27484777569770813\n",
      "[step: 3940] loss: 0.3178619146347046\n",
      "[step: 3941] loss: 0.1394764930009842\n",
      "[step: 3942] loss: 0.26811280846595764\n",
      "[step: 3943] loss: 0.29786309599876404\n",
      "[step: 3944] loss: 0.25570791959762573\n",
      "[step: 3945] loss: 0.3465697467327118\n",
      "[step: 3946] loss: 0.15593644976615906\n",
      "[step: 3947] loss: 0.2900876998901367\n",
      "[step: 3948] loss: 0.2312518209218979\n",
      "[step: 3949] loss: 0.20251165330410004\n",
      "[step: 3950] loss: 0.21980790793895721\n",
      "[step: 3951] loss: 0.18195566534996033\n",
      "[step: 3952] loss: 0.23101134598255157\n",
      "[step: 3953] loss: 0.14144040644168854\n",
      "[step: 3954] loss: 0.2042306363582611\n",
      "[step: 3955] loss: 0.14368849992752075\n",
      "[step: 3956] loss: 0.164606973528862\n",
      "[step: 3957] loss: 0.13358305394649506\n",
      "[step: 3958] loss: 0.16067400574684143\n",
      "[step: 3959] loss: 0.15404823422431946\n",
      "[step: 3960] loss: 0.13904690742492676\n",
      "[step: 3961] loss: 0.14641067385673523\n",
      "[step: 3962] loss: 0.1270386278629303\n",
      "[step: 3963] loss: 0.1501213163137436\n",
      "[step: 3964] loss: 0.11770225316286087\n",
      "[step: 3965] loss: 0.1374438852071762\n",
      "[step: 3966] loss: 0.12215543538331985\n",
      "[step: 3967] loss: 0.12739305198192596\n",
      "[step: 3968] loss: 0.12101252377033234\n",
      "[step: 3969] loss: 0.11610811203718185\n",
      "[step: 3970] loss: 0.12252593785524368\n",
      "[step: 3971] loss: 0.11299604177474976\n",
      "[step: 3972] loss: 0.1187632828950882\n",
      "[step: 3973] loss: 0.10848904401063919\n",
      "[step: 3974] loss: 0.1164739802479744\n",
      "[step: 3975] loss: 0.1099667102098465\n",
      "[step: 3976] loss: 0.11048562824726105\n",
      "[step: 3977] loss: 0.11037396639585495\n",
      "[step: 3978] loss: 0.10541928559541702\n",
      "[step: 3979] loss: 0.11028732359409332\n",
      "[step: 3980] loss: 0.10672948509454727\n",
      "[step: 3981] loss: 0.10644771158695221\n",
      "[step: 3982] loss: 0.10687632113695145\n",
      "[step: 3983] loss: 0.10194201022386551\n",
      "[step: 3984] loss: 0.10579977184534073\n",
      "[step: 3985] loss: 0.10152950882911682\n",
      "[step: 3986] loss: 0.10244530439376831\n",
      "[step: 3987] loss: 0.10265614837408066\n",
      "[step: 3988] loss: 0.09996673464775085\n",
      "[step: 3989] loss: 0.1019696444272995\n",
      "[step: 3990] loss: 0.0999334380030632\n",
      "[step: 3991] loss: 0.09921333193778992\n",
      "[step: 3992] loss: 0.09971269965171814\n",
      "[step: 3993] loss: 0.09797949343919754\n",
      "[step: 3994] loss: 0.09830089658498764\n",
      "[step: 3995] loss: 0.09804251790046692\n",
      "[step: 3996] loss: 0.09688945859670639\n",
      "[step: 3997] loss: 0.09703763574361801\n",
      "[step: 3998] loss: 0.0968402549624443\n",
      "[step: 3999] loss: 0.09585320204496384\n",
      "[step: 4000] loss: 0.09641966223716736\n",
      "[step: 4001] loss: 0.0959838405251503\n",
      "[step: 4002] loss: 0.09642470628023148\n",
      "[step: 4003] loss: 0.09882833063602448\n",
      "[step: 4004] loss: 0.10304620116949081\n",
      "[step: 4005] loss: 0.11624348908662796\n",
      "[step: 4006] loss: 0.14461199939250946\n",
      "[step: 4007] loss: 0.21676141023635864\n",
      "[step: 4008] loss: 0.30957409739494324\n",
      "[step: 4009] loss: 0.47832968831062317\n",
      "[step: 4010] loss: 0.3891270160675049\n",
      "[step: 4011] loss: 0.26586246490478516\n",
      "[step: 4012] loss: 0.15160305798053741\n",
      "[step: 4013] loss: 0.2311490923166275\n",
      "[step: 4014] loss: 0.29790937900543213\n",
      "[step: 4015] loss: 0.1895841360092163\n",
      "[step: 4016] loss: 0.17167708277702332\n",
      "[step: 4017] loss: 0.1658676266670227\n",
      "[step: 4018] loss: 0.19511494040489197\n",
      "[step: 4019] loss: 0.19752943515777588\n",
      "[step: 4020] loss: 0.11881422996520996\n",
      "[step: 4021] loss: 0.18960686028003693\n",
      "[step: 4022] loss: 0.1916533261537552\n",
      "[step: 4023] loss: 0.14435575902462006\n",
      "[step: 4024] loss: 0.1631034016609192\n",
      "[step: 4025] loss: 0.16417716443538666\n",
      "[step: 4026] loss: 0.20633168518543243\n",
      "[step: 4027] loss: 0.13569271564483643\n",
      "[step: 4028] loss: 0.12432568520307541\n",
      "[step: 4029] loss: 0.1581116020679474\n",
      "[step: 4030] loss: 0.13727840781211853\n",
      "[step: 4031] loss: 0.12096592038869858\n",
      "[step: 4032] loss: 0.11378709971904755\n",
      "[step: 4033] loss: 0.12260794639587402\n",
      "[step: 4034] loss: 0.14281554520130157\n",
      "[step: 4035] loss: 0.1344512552022934\n",
      "[step: 4036] loss: 0.12056576460599899\n",
      "[step: 4037] loss: 0.11210080981254578\n",
      "[step: 4038] loss: 0.11029264330863953\n",
      "[step: 4039] loss: 0.1300494372844696\n",
      "[step: 4040] loss: 0.11040434241294861\n",
      "[step: 4041] loss: 0.11022184789180756\n",
      "[step: 4042] loss: 0.1016446202993393\n",
      "[step: 4043] loss: 0.10439080744981766\n",
      "[step: 4044] loss: 0.10690812766551971\n",
      "[step: 4045] loss: 0.1034759059548378\n",
      "[step: 4046] loss: 0.09856884926557541\n",
      "[step: 4047] loss: 0.1002124473452568\n",
      "[step: 4048] loss: 0.09640964865684509\n",
      "[step: 4049] loss: 0.10178577899932861\n",
      "[step: 4050] loss: 0.09799008816480637\n",
      "[step: 4051] loss: 0.09645118564367294\n",
      "[step: 4052] loss: 0.09582021087408066\n",
      "[step: 4053] loss: 0.09213455021381378\n",
      "[step: 4054] loss: 0.09529536217451096\n",
      "[step: 4055] loss: 0.09543104469776154\n",
      "[step: 4056] loss: 0.09472361952066422\n",
      "[step: 4057] loss: 0.09578237682580948\n",
      "[step: 4058] loss: 0.09343638271093369\n",
      "[step: 4059] loss: 0.09211014211177826\n",
      "[step: 4060] loss: 0.09168893098831177\n",
      "[step: 4061] loss: 0.09015915542840958\n",
      "[step: 4062] loss: 0.08941957354545593\n",
      "[step: 4063] loss: 0.09008973091840744\n",
      "[step: 4064] loss: 0.08832500129938126\n",
      "[step: 4065] loss: 0.08914235234260559\n",
      "[step: 4066] loss: 0.08879995346069336\n",
      "[step: 4067] loss: 0.08876779675483704\n",
      "[step: 4068] loss: 0.08992026746273041\n",
      "[step: 4069] loss: 0.09173831343650818\n",
      "[step: 4070] loss: 0.09688466787338257\n",
      "[step: 4071] loss: 0.10649435967206955\n",
      "[step: 4072] loss: 0.12912383675575256\n",
      "[step: 4073] loss: 0.16122354567050934\n",
      "[step: 4074] loss: 0.24100758135318756\n",
      "[step: 4075] loss: 0.25814372301101685\n",
      "[step: 4076] loss: 0.2572229504585266\n",
      "[step: 4077] loss: 0.16014714539051056\n",
      "[step: 4078] loss: 0.1230907216668129\n",
      "[step: 4079] loss: 0.1571638137102127\n",
      "[step: 4080] loss: 0.21165527403354645\n",
      "[step: 4081] loss: 0.20479874312877655\n",
      "[step: 4082] loss: 0.11561020463705063\n",
      "[step: 4083] loss: 0.10990480333566666\n",
      "[step: 4084] loss: 0.1565820574760437\n",
      "[step: 4085] loss: 0.16868866980075836\n",
      "[step: 4086] loss: 0.1429406851530075\n",
      "[step: 4087] loss: 0.11872447282075882\n",
      "[step: 4088] loss: 0.11077478528022766\n",
      "[step: 4089] loss: 0.15565812587738037\n",
      "[step: 4090] loss: 0.1428328901529312\n",
      "[step: 4091] loss: 0.11542820185422897\n",
      "[step: 4092] loss: 0.1050652340054512\n",
      "[step: 4093] loss: 0.11856425553560257\n",
      "[step: 4094] loss: 0.11869136989116669\n",
      "[step: 4095] loss: 0.1269826740026474\n",
      "[step: 4096] loss: 0.1052028015255928\n",
      "[step: 4097] loss: 0.09304551780223846\n",
      "[step: 4098] loss: 0.10831702500581741\n",
      "[step: 4099] loss: 0.10951176285743713\n",
      "[step: 4100] loss: 0.1059485375881195\n",
      "[step: 4101] loss: 0.11281174421310425\n",
      "[step: 4102] loss: 0.10323436558246613\n",
      "[step: 4103] loss: 0.09180040657520294\n",
      "[step: 4104] loss: 0.09797141700983047\n",
      "[step: 4105] loss: 0.09511084854602814\n",
      "[step: 4106] loss: 0.10081487149000168\n",
      "[step: 4107] loss: 0.10378867387771606\n",
      "[step: 4108] loss: 0.10012675076723099\n",
      "[step: 4109] loss: 0.08971941471099854\n",
      "[step: 4110] loss: 0.09365838766098022\n",
      "[step: 4111] loss: 0.08536813408136368\n",
      "[step: 4112] loss: 0.08973026275634766\n",
      "[step: 4113] loss: 0.09380561858415604\n",
      "[step: 4114] loss: 0.09107258170843124\n",
      "[step: 4115] loss: 0.09132020175457001\n",
      "[step: 4116] loss: 0.09141236543655396\n",
      "[step: 4117] loss: 0.08642687648534775\n",
      "[step: 4118] loss: 0.08595936000347137\n",
      "[step: 4119] loss: 0.08605953305959702\n",
      "[step: 4120] loss: 0.08348307758569717\n",
      "[step: 4121] loss: 0.08289192616939545\n",
      "[step: 4122] loss: 0.08377854526042938\n",
      "[step: 4123] loss: 0.08162874728441238\n",
      "[step: 4124] loss: 0.08145906031131744\n",
      "[step: 4125] loss: 0.0826164036989212\n",
      "[step: 4126] loss: 0.08077558875083923\n",
      "[step: 4127] loss: 0.08065126091241837\n",
      "[step: 4128] loss: 0.0810576006770134\n",
      "[step: 4129] loss: 0.08102178573608398\n",
      "[step: 4130] loss: 0.0804479718208313\n",
      "[step: 4131] loss: 0.08229895681142807\n",
      "[step: 4132] loss: 0.08586157113313675\n",
      "[step: 4133] loss: 0.09436958283185959\n",
      "[step: 4134] loss: 0.12452153861522675\n",
      "[step: 4135] loss: 0.18841561675071716\n",
      "[step: 4136] loss: 0.3822348117828369\n",
      "[step: 4137] loss: 0.35270044207572937\n",
      "[step: 4138] loss: 0.31922823190689087\n",
      "[step: 4139] loss: 0.26261666417121887\n",
      "[step: 4140] loss: 0.1324102133512497\n",
      "[step: 4141] loss: 0.20925821363925934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4142] loss: 0.2631497085094452\n",
      "[step: 4143] loss: 0.17609946429729462\n",
      "[step: 4144] loss: 0.13125012814998627\n",
      "[step: 4145] loss: 0.18366029858589172\n",
      "[step: 4146] loss: 0.15630890429019928\n",
      "[step: 4147] loss: 0.21460022032260895\n",
      "[step: 4148] loss: 0.22654564678668976\n",
      "[step: 4149] loss: 0.1409187763929367\n",
      "[step: 4150] loss: 0.22263681888580322\n",
      "[step: 4151] loss: 0.2188325822353363\n",
      "[step: 4152] loss: 0.20121680200099945\n",
      "[step: 4153] loss: 0.21525748074054718\n",
      "[step: 4154] loss: 0.13288183510303497\n",
      "[step: 4155] loss: 0.1795790046453476\n",
      "[step: 4156] loss: 0.13883043825626373\n",
      "[step: 4157] loss: 0.1415891796350479\n",
      "[step: 4158] loss: 0.1434139907360077\n",
      "[step: 4159] loss: 0.10642249882221222\n",
      "[step: 4160] loss: 0.13733376562595367\n",
      "[step: 4161] loss: 0.12562574446201324\n",
      "[step: 4162] loss: 0.13403531908988953\n",
      "[step: 4163] loss: 0.11652954667806625\n",
      "[step: 4164] loss: 0.10169010609388351\n",
      "[step: 4165] loss: 0.13217754662036896\n",
      "[step: 4166] loss: 0.101105697453022\n",
      "[step: 4167] loss: 0.11827368289232254\n",
      "[step: 4168] loss: 0.09290175139904022\n",
      "[step: 4169] loss: 0.10397086292505264\n",
      "[step: 4170] loss: 0.0958278626203537\n",
      "[step: 4171] loss: 0.09522924572229385\n",
      "[step: 4172] loss: 0.09527292102575302\n",
      "[step: 4173] loss: 0.08927430957555771\n",
      "[step: 4174] loss: 0.09459024667739868\n",
      "[step: 4175] loss: 0.08748062700033188\n",
      "[step: 4176] loss: 0.09145624190568924\n",
      "[step: 4177] loss: 0.08708231151103973\n",
      "[step: 4178] loss: 0.08669795095920563\n",
      "[step: 4179] loss: 0.087664395570755\n",
      "[step: 4180] loss: 0.08425440639257431\n",
      "[step: 4181] loss: 0.0855632796883583\n",
      "[step: 4182] loss: 0.08402137458324432\n",
      "[step: 4183] loss: 0.08267666399478912\n",
      "[step: 4184] loss: 0.0839601531624794\n",
      "[step: 4185] loss: 0.07991509884595871\n",
      "[step: 4186] loss: 0.08303017914295197\n",
      "[step: 4187] loss: 0.079095259308815\n",
      "[step: 4188] loss: 0.0811862200498581\n",
      "[step: 4189] loss: 0.07907308638095856\n",
      "[step: 4190] loss: 0.07925063371658325\n",
      "[step: 4191] loss: 0.07904019951820374\n",
      "[step: 4192] loss: 0.07816702127456665\n",
      "[step: 4193] loss: 0.07788266241550446\n",
      "[step: 4194] loss: 0.07802870869636536\n",
      "[step: 4195] loss: 0.07686720788478851\n",
      "[step: 4196] loss: 0.0773811861872673\n",
      "[step: 4197] loss: 0.0764336884021759\n",
      "[step: 4198] loss: 0.07630100101232529\n",
      "[step: 4199] loss: 0.07623182982206345\n",
      "[step: 4200] loss: 0.07568664103746414\n",
      "[step: 4201] loss: 0.07564777135848999\n",
      "[step: 4202] loss: 0.07555623352527618\n",
      "[step: 4203] loss: 0.07515458762645721\n",
      "[step: 4204] loss: 0.07587792724370956\n",
      "[step: 4205] loss: 0.07676193118095398\n",
      "[step: 4206] loss: 0.08033113181591034\n",
      "[step: 4207] loss: 0.09140798449516296\n",
      "[step: 4208] loss: 0.11801932752132416\n",
      "[step: 4209] loss: 0.20602114498615265\n",
      "[step: 4210] loss: 0.3287169635295868\n",
      "[step: 4211] loss: 0.5868400931358337\n",
      "[step: 4212] loss: 0.44083938002586365\n",
      "[step: 4213] loss: 0.34501054883003235\n",
      "[step: 4214] loss: 0.2651847004890442\n",
      "[step: 4215] loss: 0.45612093806266785\n",
      "[step: 4216] loss: 0.26006340980529785\n",
      "[step: 4217] loss: 0.27730700373649597\n",
      "[step: 4218] loss: 0.2297770082950592\n",
      "[step: 4219] loss: 0.24749097228050232\n",
      "[step: 4220] loss: 0.2171202450990677\n",
      "[step: 4221] loss: 0.15676681697368622\n",
      "[step: 4222] loss: 0.2440609633922577\n",
      "[step: 4223] loss: 0.18314920365810394\n",
      "[step: 4224] loss: 0.1431644856929779\n",
      "[step: 4225] loss: 0.15520410239696503\n",
      "[step: 4226] loss: 0.2017885446548462\n",
      "[step: 4227] loss: 0.13097380101680756\n",
      "[step: 4228] loss: 0.12341144680976868\n",
      "[step: 4229] loss: 0.15269507467746735\n",
      "[step: 4230] loss: 0.13000772893428802\n",
      "[step: 4231] loss: 0.11083202809095383\n",
      "[step: 4232] loss: 0.12558555603027344\n",
      "[step: 4233] loss: 0.11628948897123337\n",
      "[step: 4234] loss: 0.109615758061409\n",
      "[step: 4235] loss: 0.10459522902965546\n",
      "[step: 4236] loss: 0.11028841882944107\n",
      "[step: 4237] loss: 0.10381094366312027\n",
      "[step: 4238] loss: 0.09961362183094025\n",
      "[step: 4239] loss: 0.09570115804672241\n",
      "[step: 4240] loss: 0.10327458381652832\n",
      "[step: 4241] loss: 0.09604562073945999\n",
      "[step: 4242] loss: 0.08960574865341187\n",
      "[step: 4243] loss: 0.09153136610984802\n",
      "[step: 4244] loss: 0.0958326905965805\n",
      "[step: 4245] loss: 0.09392276406288147\n",
      "[step: 4246] loss: 0.0843869000673294\n",
      "[step: 4247] loss: 0.08689827471971512\n",
      "[step: 4248] loss: 0.08560674637556076\n",
      "[step: 4249] loss: 0.08649173378944397\n",
      "[step: 4250] loss: 0.08270259946584702\n",
      "[step: 4251] loss: 0.08210788667201996\n",
      "[step: 4252] loss: 0.08143031597137451\n",
      "[step: 4253] loss: 0.08230934292078018\n",
      "[step: 4254] loss: 0.08062848448753357\n",
      "[step: 4255] loss: 0.07908890396356583\n",
      "[step: 4256] loss: 0.07856534421443939\n",
      "[step: 4257] loss: 0.0791286826133728\n",
      "[step: 4258] loss: 0.07813163846731186\n",
      "[step: 4259] loss: 0.0793231800198555\n",
      "[step: 4260] loss: 0.07684076577425003\n",
      "[step: 4261] loss: 0.07589994370937347\n",
      "[step: 4262] loss: 0.07578792423009872\n",
      "[step: 4263] loss: 0.07546078413724899\n",
      "[step: 4264] loss: 0.07604791224002838\n",
      "[step: 4265] loss: 0.07542267441749573\n",
      "[step: 4266] loss: 0.07486096024513245\n",
      "[step: 4267] loss: 0.07386043667793274\n",
      "[step: 4268] loss: 0.07353644073009491\n",
      "[step: 4269] loss: 0.07312826812267303\n",
      "[step: 4270] loss: 0.07293454557657242\n",
      "[step: 4271] loss: 0.07281171530485153\n",
      "[step: 4272] loss: 0.07291053980588913\n",
      "[step: 4273] loss: 0.07282731682062149\n",
      "[step: 4274] loss: 0.07301975786685944\n",
      "[step: 4275] loss: 0.07299777120351791\n",
      "[step: 4276] loss: 0.07351608574390411\n",
      "[step: 4277] loss: 0.07408705353736877\n",
      "[step: 4278] loss: 0.0744035392999649\n",
      "[step: 4279] loss: 0.0758911594748497\n",
      "[step: 4280] loss: 0.07760132104158401\n",
      "[step: 4281] loss: 0.08179260045289993\n",
      "[step: 4282] loss: 0.08696123212575912\n",
      "[step: 4283] loss: 0.09815973043441772\n",
      "[step: 4284] loss: 0.10712215304374695\n",
      "[step: 4285] loss: 0.12698760628700256\n",
      "[step: 4286] loss: 0.13209958374500275\n",
      "[step: 4287] loss: 0.14061325788497925\n",
      "[step: 4288] loss: 0.11604282259941101\n",
      "[step: 4289] loss: 0.09068487584590912\n",
      "[step: 4290] loss: 0.07247830182313919\n",
      "[step: 4291] loss: 0.07522043585777283\n",
      "[step: 4292] loss: 0.09219200164079666\n",
      "[step: 4293] loss: 0.10453483462333679\n",
      "[step: 4294] loss: 0.1091233342885971\n",
      "[step: 4295] loss: 0.09257905185222626\n",
      "[step: 4296] loss: 0.07637418806552887\n",
      "[step: 4297] loss: 0.0702042281627655\n",
      "[step: 4298] loss: 0.07736638933420181\n",
      "[step: 4299] loss: 0.09080465137958527\n",
      "[step: 4300] loss: 0.09800543636083603\n",
      "[step: 4301] loss: 0.10058797895908356\n",
      "[step: 4302] loss: 0.08898382633924484\n",
      "[step: 4303] loss: 0.07710009068250656\n",
      "[step: 4304] loss: 0.06899313628673553\n",
      "[step: 4305] loss: 0.07042019814252853\n",
      "[step: 4306] loss: 0.07819448411464691\n",
      "[step: 4307] loss: 0.08746927231550217\n",
      "[step: 4308] loss: 0.09952715039253235\n",
      "[step: 4309] loss: 0.10211576521396637\n",
      "[step: 4310] loss: 0.10317008942365646\n",
      "[step: 4311] loss: 0.08909662812948227\n",
      "[step: 4312] loss: 0.07687681913375854\n",
      "[step: 4313] loss: 0.06844738125801086\n",
      "[step: 4314] loss: 0.06821644306182861\n",
      "[step: 4315] loss: 0.07351035624742508\n",
      "[step: 4316] loss: 0.08045272529125214\n",
      "[step: 4317] loss: 0.08868370205163956\n",
      "[step: 4318] loss: 0.08951906114816666\n",
      "[step: 4319] loss: 0.08862821757793427\n",
      "[step: 4320] loss: 0.07979181408882141\n",
      "[step: 4321] loss: 0.07289808988571167\n",
      "[step: 4322] loss: 0.06778345257043839\n",
      "[step: 4323] loss: 0.06612665206193924\n",
      "[step: 4324] loss: 0.06779716163873672\n",
      "[step: 4325] loss: 0.07123523950576782\n",
      "[step: 4326] loss: 0.0764385312795639\n",
      "[step: 4327] loss: 0.08113285154104233\n",
      "[step: 4328] loss: 0.08946727961301804\n",
      "[step: 4329] loss: 0.09370918571949005\n",
      "[step: 4330] loss: 0.1046559289097786\n",
      "[step: 4331] loss: 0.1064838171005249\n",
      "[step: 4332] loss: 0.11118573695421219\n",
      "[step: 4333] loss: 0.09926823526620865\n",
      "[step: 4334] loss: 0.08696521073579788\n",
      "[step: 4335] loss: 0.07224345207214355\n",
      "[step: 4336] loss: 0.06542306393384933\n",
      "[step: 4337] loss: 0.06647748500108719\n",
      "[step: 4338] loss: 0.07296338677406311\n",
      "[step: 4339] loss: 0.08329886943101883\n",
      "[step: 4340] loss: 0.09101197123527527\n",
      "[step: 4341] loss: 0.10101954638957977\n",
      "[step: 4342] loss: 0.0972336158156395\n",
      "[step: 4343] loss: 0.09345132857561111\n",
      "[step: 4344] loss: 0.08056346327066422\n",
      "[step: 4345] loss: 0.07061423361301422\n",
      "[step: 4346] loss: 0.06476710736751556\n",
      "[step: 4347] loss: 0.06479492783546448\n",
      "[step: 4348] loss: 0.06974732875823975\n",
      "[step: 4349] loss: 0.07666190713644028\n",
      "[step: 4350] loss: 0.08594901114702225\n",
      "[step: 4351] loss: 0.0927884578704834\n",
      "[step: 4352] loss: 0.10718844085931778\n",
      "[step: 4353] loss: 0.10857617855072021\n",
      "[step: 4354] loss: 0.11441880464553833\n",
      "[step: 4355] loss: 0.09906263649463654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4356] loss: 0.08424903452396393\n",
      "[step: 4357] loss: 0.07012904435396194\n",
      "[step: 4358] loss: 0.06392132490873337\n",
      "[step: 4359] loss: 0.06847570836544037\n",
      "[step: 4360] loss: 0.07926767319440842\n",
      "[step: 4361] loss: 0.09012813121080399\n",
      "[step: 4362] loss: 0.09400199353694916\n",
      "[step: 4363] loss: 0.09911522269248962\n",
      "[step: 4364] loss: 0.09079103916883469\n",
      "[step: 4365] loss: 0.08296456187963486\n",
      "[step: 4366] loss: 0.06949383020401001\n",
      "[step: 4367] loss: 0.06365945935249329\n",
      "[step: 4368] loss: 0.06490787863731384\n",
      "[step: 4369] loss: 0.06909076869487762\n",
      "[step: 4370] loss: 0.07642513513565063\n",
      "[step: 4371] loss: 0.08358597010374069\n",
      "[step: 4372] loss: 0.09217888116836548\n",
      "[step: 4373] loss: 0.09360475838184357\n",
      "[step: 4374] loss: 0.09607334434986115\n",
      "[step: 4375] loss: 0.08983157575130463\n",
      "[step: 4376] loss: 0.08593636751174927\n",
      "[step: 4377] loss: 0.07452218234539032\n",
      "[step: 4378] loss: 0.06605901569128036\n",
      "[step: 4379] loss: 0.061582718044519424\n",
      "[step: 4380] loss: 0.06207339093089104\n",
      "[step: 4381] loss: 0.06574703007936478\n",
      "[step: 4382] loss: 0.07121026515960693\n",
      "[step: 4383] loss: 0.08044373244047165\n",
      "[step: 4384] loss: 0.08764934539794922\n",
      "[step: 4385] loss: 0.09845762699842453\n",
      "[step: 4386] loss: 0.09812887012958527\n",
      "[step: 4387] loss: 0.09809911251068115\n",
      "[step: 4388] loss: 0.08689948916435242\n",
      "[step: 4389] loss: 0.07677493244409561\n",
      "[step: 4390] loss: 0.06657812744379044\n",
      "[step: 4391] loss: 0.06102903559803963\n",
      "[step: 4392] loss: 0.060064490884542465\n",
      "[step: 4393] loss: 0.062430959194898605\n",
      "[step: 4394] loss: 0.066769540309906\n",
      "[step: 4395] loss: 0.0723516121506691\n",
      "[step: 4396] loss: 0.08206513524055481\n",
      "[step: 4397] loss: 0.0904349833726883\n",
      "[step: 4398] loss: 0.10775835067033768\n",
      "[step: 4399] loss: 0.10886019468307495\n",
      "[step: 4400] loss: 0.11376357078552246\n",
      "[step: 4401] loss: 0.0954238772392273\n",
      "[step: 4402] loss: 0.0816548615694046\n",
      "[step: 4403] loss: 0.07364337891340256\n",
      "[step: 4404] loss: 0.06899827718734741\n",
      "[step: 4405] loss: 0.06903552263975143\n",
      "[step: 4406] loss: 0.07477203756570816\n",
      "[step: 4407] loss: 0.08994205296039581\n",
      "[step: 4408] loss: 0.10143058001995087\n",
      "[step: 4409] loss: 0.10919365286827087\n",
      "[step: 4410] loss: 0.10402006655931473\n",
      "[step: 4411] loss: 0.09194860607385635\n",
      "[step: 4412] loss: 0.07654882967472076\n",
      "[step: 4413] loss: 0.06880247592926025\n",
      "[step: 4414] loss: 0.06408806145191193\n",
      "[step: 4415] loss: 0.06290207803249359\n",
      "[step: 4416] loss: 0.06453173607587814\n",
      "[step: 4417] loss: 0.06995387375354767\n",
      "[step: 4418] loss: 0.08049112558364868\n",
      "[step: 4419] loss: 0.09027073532342911\n",
      "[step: 4420] loss: 0.10739246755838394\n",
      "[step: 4421] loss: 0.09882742166519165\n",
      "[step: 4422] loss: 0.0867815688252449\n",
      "[step: 4423] loss: 0.0695599764585495\n",
      "[step: 4424] loss: 0.06934375315904617\n",
      "[step: 4425] loss: 0.07469063997268677\n",
      "[step: 4426] loss: 0.07548192888498306\n",
      "[step: 4427] loss: 0.07791996747255325\n",
      "[step: 4428] loss: 0.07991347461938858\n",
      "[step: 4429] loss: 0.0921480655670166\n",
      "[step: 4430] loss: 0.10527200251817703\n",
      "[step: 4431] loss: 0.10223746299743652\n",
      "[step: 4432] loss: 0.08870355784893036\n",
      "[step: 4433] loss: 0.07944385707378387\n",
      "[step: 4434] loss: 0.0782928466796875\n",
      "[step: 4435] loss: 0.08246815949678421\n",
      "[step: 4436] loss: 0.07073930650949478\n",
      "[step: 4437] loss: 0.06064034625887871\n",
      "[step: 4438] loss: 0.058011122047901154\n",
      "[step: 4439] loss: 0.06305969506502151\n",
      "[step: 4440] loss: 0.0684637799859047\n",
      "[step: 4441] loss: 0.06921719759702682\n",
      "[step: 4442] loss: 0.07294568419456482\n",
      "[step: 4443] loss: 0.08057378232479095\n",
      "[step: 4444] loss: 0.09238618612289429\n",
      "[step: 4445] loss: 0.09923602640628815\n",
      "[step: 4446] loss: 0.10619382560253143\n",
      "[step: 4447] loss: 0.11053993552923203\n",
      "[step: 4448] loss: 0.1319379061460495\n",
      "[step: 4449] loss: 0.11828651279211044\n",
      "[step: 4450] loss: 0.09580370038747787\n",
      "[step: 4451] loss: 0.0643514096736908\n",
      "[step: 4452] loss: 0.06641078740358353\n",
      "[step: 4453] loss: 0.08721185475587845\n",
      "[step: 4454] loss: 0.10031584650278091\n",
      "[step: 4455] loss: 0.11420275270938873\n",
      "[step: 4456] loss: 0.09654102474451065\n",
      "[step: 4457] loss: 0.09392452985048294\n",
      "[step: 4458] loss: 0.10614775866270065\n",
      "[step: 4459] loss: 0.10365559160709381\n",
      "[step: 4460] loss: 0.0852193534374237\n",
      "[step: 4461] loss: 0.06684203445911407\n",
      "[step: 4462] loss: 0.08044564723968506\n",
      "[step: 4463] loss: 0.10451074689626694\n",
      "[step: 4464] loss: 0.09492476284503937\n",
      "[step: 4465] loss: 0.08096614480018616\n",
      "[step: 4466] loss: 0.08520887047052383\n",
      "[step: 4467] loss: 0.09252849966287613\n",
      "[step: 4468] loss: 0.08684100955724716\n",
      "[step: 4469] loss: 0.0644511803984642\n",
      "[step: 4470] loss: 0.06496665626764297\n",
      "[step: 4471] loss: 0.07479170709848404\n",
      "[step: 4472] loss: 0.06585054099559784\n",
      "[step: 4473] loss: 0.05995352193713188\n",
      "[step: 4474] loss: 0.06963711231946945\n",
      "[step: 4475] loss: 0.07391922920942307\n",
      "[step: 4476] loss: 0.06795129925012589\n",
      "[step: 4477] loss: 0.06528382003307343\n",
      "[step: 4478] loss: 0.07125334441661835\n",
      "[step: 4479] loss: 0.07040706276893616\n",
      "[step: 4480] loss: 0.061463531106710434\n",
      "[step: 4481] loss: 0.06032881513237953\n",
      "[step: 4482] loss: 0.06341453641653061\n",
      "[step: 4483] loss: 0.060423843562603\n",
      "[step: 4484] loss: 0.055200014263391495\n",
      "[step: 4485] loss: 0.05721946060657501\n",
      "[step: 4486] loss: 0.060163840651512146\n",
      "[step: 4487] loss: 0.05670074746012688\n",
      "[step: 4488] loss: 0.0544738732278347\n",
      "[step: 4489] loss: 0.05873275548219681\n",
      "[step: 4490] loss: 0.06112075597047806\n",
      "[step: 4491] loss: 0.062268294394016266\n",
      "[step: 4492] loss: 0.06979124248027802\n",
      "[step: 4493] loss: 0.09168632328510284\n",
      "[step: 4494] loss: 0.11933242529630661\n",
      "[step: 4495] loss: 0.18996769189834595\n",
      "[step: 4496] loss: 0.22285665571689606\n",
      "[step: 4497] loss: 0.2582649886608124\n",
      "[step: 4498] loss: 0.2419625222682953\n",
      "[step: 4499] loss: 0.17564761638641357\n",
      "[step: 4500] loss: 0.11687981337308884\n",
      "[step: 4501] loss: 0.09300566464662552\n",
      "[step: 4502] loss: 0.1529536247253418\n",
      "[step: 4503] loss: 0.1441102921962738\n",
      "[step: 4504] loss: 0.09190771728754044\n",
      "[step: 4505] loss: 0.07050641626119614\n",
      "[step: 4506] loss: 0.08536025881767273\n",
      "[step: 4507] loss: 0.11359886825084686\n",
      "[step: 4508] loss: 0.09164763242006302\n",
      "[step: 4509] loss: 0.07351232320070267\n",
      "[step: 4510] loss: 0.07543253153562546\n",
      "[step: 4511] loss: 0.07686887681484222\n",
      "[step: 4512] loss: 0.08679226785898209\n",
      "[step: 4513] loss: 0.0988517627120018\n",
      "[step: 4514] loss: 0.09940143674612045\n",
      "[step: 4515] loss: 0.06480707973241806\n",
      "[step: 4516] loss: 0.06510704010725021\n",
      "[step: 4517] loss: 0.08444187790155411\n",
      "[step: 4518] loss: 0.08281700313091278\n",
      "[step: 4519] loss: 0.07487471401691437\n",
      "[step: 4520] loss: 0.06799023598432541\n",
      "[step: 4521] loss: 0.061102256178855896\n",
      "[step: 4522] loss: 0.057293981313705444\n",
      "[step: 4523] loss: 0.06952420622110367\n",
      "[step: 4524] loss: 0.06879536807537079\n",
      "[step: 4525] loss: 0.061569735407829285\n",
      "[step: 4526] loss: 0.05872083455324173\n",
      "[step: 4527] loss: 0.05774737522006035\n",
      "[step: 4528] loss: 0.05562390759587288\n",
      "[step: 4529] loss: 0.05779096856713295\n",
      "[step: 4530] loss: 0.06197601184248924\n",
      "[step: 4531] loss: 0.06343613564968109\n",
      "[step: 4532] loss: 0.058271992951631546\n",
      "[step: 4533] loss: 0.0567331537604332\n",
      "[step: 4534] loss: 0.054538872092962265\n",
      "[step: 4535] loss: 0.05171862989664078\n",
      "[step: 4536] loss: 0.05099564045667648\n",
      "[step: 4537] loss: 0.05230008065700531\n",
      "[step: 4538] loss: 0.053782641887664795\n",
      "[step: 4539] loss: 0.053632963448762894\n",
      "[step: 4540] loss: 0.05590556561946869\n",
      "[step: 4541] loss: 0.05753832310438156\n",
      "[step: 4542] loss: 0.05939639359712601\n",
      "[step: 4543] loss: 0.05944599583745003\n",
      "[step: 4544] loss: 0.061383262276649475\n",
      "[step: 4545] loss: 0.06250125914812088\n",
      "[step: 4546] loss: 0.06387115269899368\n",
      "[step: 4547] loss: 0.06375999748706818\n",
      "[step: 4548] loss: 0.06850676238536835\n",
      "[step: 4549] loss: 0.07000399380922318\n",
      "[step: 4550] loss: 0.07610015571117401\n",
      "[step: 4551] loss: 0.07500218600034714\n",
      "[step: 4552] loss: 0.07637661695480347\n",
      "[step: 4553] loss: 0.07315095514059067\n",
      "[step: 4554] loss: 0.06955109536647797\n",
      "[step: 4555] loss: 0.06253547966480255\n",
      "[step: 4556] loss: 0.05773225426673889\n",
      "[step: 4557] loss: 0.053679440170526505\n",
      "[step: 4558] loss: 0.052040208131074905\n",
      "[step: 4559] loss: 0.04994197562336922\n",
      "[step: 4560] loss: 0.04831267520785332\n",
      "[step: 4561] loss: 0.04717410355806351\n",
      "[step: 4562] loss: 0.04714398831129074\n",
      "[step: 4563] loss: 0.04789360985159874\n",
      "[step: 4564] loss: 0.04888708516955376\n",
      "[step: 4565] loss: 0.04992970451712608\n",
      "[step: 4566] loss: 0.05166206881403923\n",
      "[step: 4567] loss: 0.05620702728629112\n",
      "[step: 4568] loss: 0.06394083052873611\n",
      "[step: 4569] loss: 0.08096659928560257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4570] loss: 0.10274060815572739\n",
      "[step: 4571] loss: 0.15257297456264496\n",
      "[step: 4572] loss: 0.17459577322006226\n",
      "[step: 4573] loss: 0.2072393000125885\n",
      "[step: 4574] loss: 0.14828646183013916\n",
      "[step: 4575] loss: 0.08923204243183136\n",
      "[step: 4576] loss: 0.07928822189569473\n",
      "[step: 4577] loss: 0.11738567054271698\n",
      "[step: 4578] loss: 0.16271232068538666\n",
      "[step: 4579] loss: 0.11214622855186462\n",
      "[step: 4580] loss: 0.08512722700834274\n",
      "[step: 4581] loss: 0.11572712659835815\n",
      "[step: 4582] loss: 0.11064281314611435\n",
      "[step: 4583] loss: 0.10906637459993362\n",
      "[step: 4584] loss: 0.0623466856777668\n",
      "[step: 4585] loss: 0.0731489285826683\n",
      "[step: 4586] loss: 0.09408203512430191\n",
      "[step: 4587] loss: 0.05811456963419914\n",
      "[step: 4588] loss: 0.06930182129144669\n",
      "[step: 4589] loss: 0.08293186873197556\n",
      "[step: 4590] loss: 0.065943144261837\n",
      "[step: 4591] loss: 0.06475599855184555\n",
      "[step: 4592] loss: 0.06253943592309952\n",
      "[step: 4593] loss: 0.07360205054283142\n",
      "[step: 4594] loss: 0.06748706847429276\n",
      "[step: 4595] loss: 0.053734276443719864\n",
      "[step: 4596] loss: 0.07636066526174545\n",
      "[step: 4597] loss: 0.06167081370949745\n",
      "[step: 4598] loss: 0.05261024460196495\n",
      "[step: 4599] loss: 0.0708753913640976\n",
      "[step: 4600] loss: 0.060204993933439255\n",
      "[step: 4601] loss: 0.054198987782001495\n",
      "[step: 4602] loss: 0.05811150372028351\n",
      "[step: 4603] loss: 0.05324086174368858\n",
      "[step: 4604] loss: 0.05533262714743614\n",
      "[step: 4605] loss: 0.04923493415117264\n",
      "[step: 4606] loss: 0.05166638642549515\n",
      "[step: 4607] loss: 0.05304919183254242\n",
      "[step: 4608] loss: 0.046547628939151764\n",
      "[step: 4609] loss: 0.05013655498623848\n",
      "[step: 4610] loss: 0.0498153492808342\n",
      "[step: 4611] loss: 0.04595822095870972\n",
      "[step: 4612] loss: 0.04814321920275688\n",
      "[step: 4613] loss: 0.046820301562547684\n",
      "[step: 4614] loss: 0.04640495404601097\n",
      "[step: 4615] loss: 0.04658501595258713\n",
      "[step: 4616] loss: 0.04511786252260208\n",
      "[step: 4617] loss: 0.045643456280231476\n",
      "[step: 4618] loss: 0.04607778415083885\n",
      "[step: 4619] loss: 0.04493725299835205\n",
      "[step: 4620] loss: 0.04561970755457878\n",
      "[step: 4621] loss: 0.04715169593691826\n",
      "[step: 4622] loss: 0.04990145564079285\n",
      "[step: 4623] loss: 0.05590567737817764\n",
      "[step: 4624] loss: 0.06815195083618164\n",
      "[step: 4625] loss: 0.09593222290277481\n",
      "[step: 4626] loss: 0.14257186651229858\n",
      "[step: 4627] loss: 0.23675131797790527\n",
      "[step: 4628] loss: 0.26916831731796265\n",
      "[step: 4629] loss: 0.28840750455856323\n",
      "[step: 4630] loss: 0.23660476505756378\n",
      "[step: 4631] loss: 0.17058660089969635\n",
      "[step: 4632] loss: 0.1819978505373001\n",
      "[step: 4633] loss: 0.22456060349941254\n",
      "[step: 4634] loss: 0.22335149347782135\n",
      "[step: 4635] loss: 0.20307786762714386\n",
      "[step: 4636] loss: 0.12112198770046234\n",
      "[step: 4637] loss: 0.1782924383878708\n",
      "[step: 4638] loss: 0.21305248141288757\n",
      "[step: 4639] loss: 0.16591039299964905\n",
      "[step: 4640] loss: 0.10045412182807922\n",
      "[step: 4641] loss: 0.11890866607427597\n",
      "[step: 4642] loss: 0.1472606360912323\n",
      "[step: 4643] loss: 0.09383172541856766\n",
      "[step: 4644] loss: 0.15975746512413025\n",
      "[step: 4645] loss: 0.0982896164059639\n",
      "[step: 4646] loss: 0.10972405970096588\n",
      "[step: 4647] loss: 0.0904313325881958\n",
      "[step: 4648] loss: 0.09753764420747757\n",
      "[step: 4649] loss: 0.09455453604459763\n",
      "[step: 4650] loss: 0.07921794801950455\n",
      "[step: 4651] loss: 0.07792004942893982\n",
      "[step: 4652] loss: 0.08079486340284348\n",
      "[step: 4653] loss: 0.06975895166397095\n",
      "[step: 4654] loss: 0.081564761698246\n",
      "[step: 4655] loss: 0.07440273463726044\n",
      "[step: 4656] loss: 0.06408455967903137\n",
      "[step: 4657] loss: 0.06828901916742325\n",
      "[step: 4658] loss: 0.0721837505698204\n",
      "[step: 4659] loss: 0.07540146261453629\n",
      "[step: 4660] loss: 0.058892443776130676\n",
      "[step: 4661] loss: 0.05867777392268181\n",
      "[step: 4662] loss: 0.05784811079502106\n",
      "[step: 4663] loss: 0.06575236469507217\n",
      "[step: 4664] loss: 0.05203094333410263\n",
      "[step: 4665] loss: 0.055651843547821045\n",
      "[step: 4666] loss: 0.05299818888306618\n",
      "[step: 4667] loss: 0.05565056577324867\n",
      "[step: 4668] loss: 0.055463701486587524\n",
      "[step: 4669] loss: 0.05322999134659767\n",
      "[step: 4670] loss: 0.0516359806060791\n",
      "[step: 4671] loss: 0.0469781868159771\n",
      "[step: 4672] loss: 0.04903215914964676\n",
      "[step: 4673] loss: 0.050055187195539474\n",
      "[step: 4674] loss: 0.05009991303086281\n",
      "[step: 4675] loss: 0.04872371256351471\n",
      "[step: 4676] loss: 0.04900522902607918\n",
      "[step: 4677] loss: 0.04533066973090172\n",
      "[step: 4678] loss: 0.04557601734995842\n",
      "[step: 4679] loss: 0.04540694132447243\n",
      "[step: 4680] loss: 0.04513290897011757\n",
      "[step: 4681] loss: 0.04597901180386543\n",
      "[step: 4682] loss: 0.04485645890235901\n",
      "[step: 4683] loss: 0.045762866735458374\n",
      "[step: 4684] loss: 0.0440889336168766\n",
      "[step: 4685] loss: 0.04367908090353012\n",
      "[step: 4686] loss: 0.04356888309121132\n",
      "[step: 4687] loss: 0.042317748069763184\n",
      "[step: 4688] loss: 0.04267146438360214\n",
      "[step: 4689] loss: 0.04176071286201477\n",
      "[step: 4690] loss: 0.04195043072104454\n",
      "[step: 4691] loss: 0.04140910506248474\n",
      "[step: 4692] loss: 0.04127056524157524\n",
      "[step: 4693] loss: 0.04128305986523628\n",
      "[step: 4694] loss: 0.040743835270404816\n",
      "[step: 4695] loss: 0.04086451977491379\n",
      "[step: 4696] loss: 0.04058251529932022\n",
      "[step: 4697] loss: 0.04062113165855408\n",
      "[step: 4698] loss: 0.040598947554826736\n",
      "[step: 4699] loss: 0.040756650269031525\n",
      "[step: 4700] loss: 0.04135216400027275\n",
      "[step: 4701] loss: 0.04254859313368797\n",
      "[step: 4702] loss: 0.04476412385702133\n",
      "[step: 4703] loss: 0.0499558225274086\n",
      "[step: 4704] loss: 0.05815970152616501\n",
      "[step: 4705] loss: 0.0787816271185875\n",
      "[step: 4706] loss: 0.10124293714761734\n",
      "[step: 4707] loss: 0.15563032031059265\n",
      "[step: 4708] loss: 0.1588452011346817\n",
      "[step: 4709] loss: 0.161107137799263\n",
      "[step: 4710] loss: 0.09475338459014893\n",
      "[step: 4711] loss: 0.05489526689052582\n",
      "[step: 4712] loss: 0.07425668090581894\n",
      "[step: 4713] loss: 0.1148456260561943\n",
      "[step: 4714] loss: 0.14163920283317566\n",
      "[step: 4715] loss: 0.10056035965681076\n",
      "[step: 4716] loss: 0.053412120789289474\n",
      "[step: 4717] loss: 0.05450810119509697\n",
      "[step: 4718] loss: 0.08754798769950867\n",
      "[step: 4719] loss: 0.1076773926615715\n",
      "[step: 4720] loss: 0.07765470445156097\n",
      "[step: 4721] loss: 0.04649052023887634\n",
      "[step: 4722] loss: 0.056372638791799545\n",
      "[step: 4723] loss: 0.08419231325387955\n",
      "[step: 4724] loss: 0.09314955025911331\n",
      "[step: 4725] loss: 0.06712059676647186\n",
      "[step: 4726] loss: 0.04565376415848732\n",
      "[step: 4727] loss: 0.05214637145400047\n",
      "[step: 4728] loss: 0.06972088664770126\n",
      "[step: 4729] loss: 0.08017132431268692\n",
      "[step: 4730] loss: 0.059460852295160294\n",
      "[step: 4731] loss: 0.04709135740995407\n",
      "[step: 4732] loss: 0.0515158474445343\n",
      "[step: 4733] loss: 0.06037115305662155\n",
      "[step: 4734] loss: 0.06411825865507126\n",
      "[step: 4735] loss: 0.052021149545907974\n",
      "[step: 4736] loss: 0.04537077620625496\n",
      "[step: 4737] loss: 0.049178048968315125\n",
      "[step: 4738] loss: 0.05289081111550331\n",
      "[step: 4739] loss: 0.05713260918855667\n",
      "[step: 4740] loss: 0.048729732632637024\n",
      "[step: 4741] loss: 0.0447770431637764\n",
      "[step: 4742] loss: 0.045216117054224014\n",
      "[step: 4743] loss: 0.046458858996629715\n",
      "[step: 4744] loss: 0.04870825260877609\n",
      "[step: 4745] loss: 0.04517577216029167\n",
      "[step: 4746] loss: 0.042301710695028305\n",
      "[step: 4747] loss: 0.040349893271923065\n",
      "[step: 4748] loss: 0.0409894622862339\n",
      "[step: 4749] loss: 0.042303916066884995\n",
      "[step: 4750] loss: 0.042320601642131805\n",
      "[step: 4751] loss: 0.042136095464229584\n",
      "[step: 4752] loss: 0.04053010419011116\n",
      "[step: 4753] loss: 0.04022301360964775\n",
      "[step: 4754] loss: 0.040562503039836884\n",
      "[step: 4755] loss: 0.039708055555820465\n",
      "[step: 4756] loss: 0.03988928720355034\n",
      "[step: 4757] loss: 0.03894089534878731\n",
      "[step: 4758] loss: 0.03819645941257477\n",
      "[step: 4759] loss: 0.037340570241212845\n",
      "[step: 4760] loss: 0.037141647189855576\n",
      "[step: 4761] loss: 0.03716282173991203\n",
      "[step: 4762] loss: 0.0379600003361702\n",
      "[step: 4763] loss: 0.03929920122027397\n",
      "[step: 4764] loss: 0.04094281047582626\n",
      "[step: 4765] loss: 0.043104514479637146\n",
      "[step: 4766] loss: 0.046826209872961044\n",
      "[step: 4767] loss: 0.04965318739414215\n",
      "[step: 4768] loss: 0.05529722943902016\n",
      "[step: 4769] loss: 0.057675983756780624\n",
      "[step: 4770] loss: 0.06402618438005447\n",
      "[step: 4771] loss: 0.06423167139291763\n",
      "[step: 4772] loss: 0.0682859793305397\n",
      "[step: 4773] loss: 0.06454252451658249\n",
      "[step: 4774] loss: 0.06254245340824127\n",
      "[step: 4775] loss: 0.06211528554558754\n",
      "[step: 4776] loss: 0.06389214843511581\n",
      "[step: 4777] loss: 0.07827044278383255\n",
      "[step: 4778] loss: 0.09312059730291367\n",
      "[step: 4779] loss: 0.12237896770238876\n",
      "[step: 4780] loss: 0.09148077666759491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4781] loss: 0.06797762960195541\n",
      "[step: 4782] loss: 0.061396244913339615\n",
      "[step: 4783] loss: 0.08776715397834778\n",
      "[step: 4784] loss: 0.1212099939584732\n",
      "[step: 4785] loss: 0.08128015697002411\n",
      "[step: 4786] loss: 0.05830445885658264\n",
      "[step: 4787] loss: 0.08099856227636337\n",
      "[step: 4788] loss: 0.08496425300836563\n",
      "[step: 4789] loss: 0.06677160412073135\n",
      "[step: 4790] loss: 0.04122697934508324\n",
      "[step: 4791] loss: 0.0659697949886322\n",
      "[step: 4792] loss: 0.0627918690443039\n",
      "[step: 4793] loss: 0.045670848339796066\n",
      "[step: 4794] loss: 0.04858747124671936\n",
      "[step: 4795] loss: 0.05760060250759125\n",
      "[step: 4796] loss: 0.04554224759340286\n",
      "[step: 4797] loss: 0.04174593463540077\n",
      "[step: 4798] loss: 0.0454411506652832\n",
      "[step: 4799] loss: 0.04797350615262985\n",
      "[step: 4800] loss: 0.04097968712449074\n",
      "[step: 4801] loss: 0.040722984820604324\n",
      "[step: 4802] loss: 0.046281296759843826\n",
      "[step: 4803] loss: 0.04751111939549446\n",
      "[step: 4804] loss: 0.0399491973221302\n",
      "[step: 4805] loss: 0.04301097244024277\n",
      "[step: 4806] loss: 0.046551208943128586\n",
      "[step: 4807] loss: 0.044955652207136154\n",
      "[step: 4808] loss: 0.04273315519094467\n",
      "[step: 4809] loss: 0.045790623873472214\n",
      "[step: 4810] loss: 0.04990827292203903\n",
      "[step: 4811] loss: 0.05305499583482742\n",
      "[step: 4812] loss: 0.05499522387981415\n",
      "[step: 4813] loss: 0.06671489775180817\n",
      "[step: 4814] loss: 0.07676622271537781\n",
      "[step: 4815] loss: 0.08573274314403534\n",
      "[step: 4816] loss: 0.09370135515928268\n",
      "[step: 4817] loss: 0.09681237488985062\n",
      "[step: 4818] loss: 0.09418542683124542\n",
      "[step: 4819] loss: 0.07724642008543015\n",
      "[step: 4820] loss: 0.04943244531750679\n",
      "[step: 4821] loss: 0.04552565515041351\n",
      "[step: 4822] loss: 0.051696937531232834\n",
      "[step: 4823] loss: 0.07324161380529404\n",
      "[step: 4824] loss: 0.08144810795783997\n",
      "[step: 4825] loss: 0.06797092407941818\n",
      "[step: 4826] loss: 0.07237181067466736\n",
      "[step: 4827] loss: 0.06770218163728714\n",
      "[step: 4828] loss: 0.05407996475696564\n",
      "[step: 4829] loss: 0.05331137403845787\n",
      "[step: 4830] loss: 0.04768658056855202\n",
      "[step: 4831] loss: 0.07466083019971848\n",
      "[step: 4832] loss: 0.10189748555421829\n",
      "[step: 4833] loss: 0.07251841574907303\n",
      "[step: 4834] loss: 0.0747414156794548\n",
      "[step: 4835] loss: 0.07450252771377563\n",
      "[step: 4836] loss: 0.06833156198263168\n",
      "[step: 4837] loss: 0.07236804813146591\n",
      "[step: 4838] loss: 0.041795749217271805\n",
      "[step: 4839] loss: 0.09022741764783859\n",
      "[step: 4840] loss: 0.06402221322059631\n",
      "[step: 4841] loss: 0.05746207386255264\n",
      "[step: 4842] loss: 0.06672593206167221\n",
      "[step: 4843] loss: 0.05102917179465294\n",
      "[step: 4844] loss: 0.05486341565847397\n",
      "[step: 4845] loss: 0.05246786028146744\n",
      "[step: 4846] loss: 0.048586323857307434\n",
      "[step: 4847] loss: 0.05837971344590187\n",
      "[step: 4848] loss: 0.04445815831422806\n",
      "[step: 4849] loss: 0.052624817937612534\n",
      "[step: 4850] loss: 0.039769429713487625\n",
      "[step: 4851] loss: 0.04430808871984482\n",
      "[step: 4852] loss: 0.0398767814040184\n",
      "[step: 4853] loss: 0.035571493208408356\n",
      "[step: 4854] loss: 0.03832276538014412\n",
      "[step: 4855] loss: 0.03466091677546501\n",
      "[step: 4856] loss: 0.037604961544275284\n",
      "[step: 4857] loss: 0.034377261996269226\n",
      "[step: 4858] loss: 0.03456266596913338\n",
      "[step: 4859] loss: 0.03633498772978783\n",
      "[step: 4860] loss: 0.03241565823554993\n",
      "[step: 4861] loss: 0.03470021113753319\n",
      "[step: 4862] loss: 0.0325477235019207\n",
      "[step: 4863] loss: 0.034137483686208725\n",
      "[step: 4864] loss: 0.03365060314536095\n",
      "[step: 4865] loss: 0.0329318605363369\n",
      "[step: 4866] loss: 0.035642217844724655\n",
      "[step: 4867] loss: 0.036228395998477936\n",
      "[step: 4868] loss: 0.04060305282473564\n",
      "[step: 4869] loss: 0.049334749579429626\n",
      "[step: 4870] loss: 0.05938092991709709\n",
      "[step: 4871] loss: 0.0733182355761528\n",
      "[step: 4872] loss: 0.09866786003112793\n",
      "[step: 4873] loss: 0.12186308950185776\n",
      "[step: 4874] loss: 0.145316943526268\n",
      "[step: 4875] loss: 0.11935914307832718\n",
      "[step: 4876] loss: 0.09190190583467484\n",
      "[step: 4877] loss: 0.059458885341882706\n",
      "[step: 4878] loss: 0.05354388803243637\n",
      "[step: 4879] loss: 0.0604662261903286\n",
      "[step: 4880] loss: 0.08282215148210526\n",
      "[step: 4881] loss: 0.09537148475646973\n",
      "[step: 4882] loss: 0.09191426634788513\n",
      "[step: 4883] loss: 0.06272155791521072\n",
      "[step: 4884] loss: 0.04766028746962547\n",
      "[step: 4885] loss: 0.05125075951218605\n",
      "[step: 4886] loss: 0.08629582077264786\n",
      "[step: 4887] loss: 0.07724983245134354\n",
      "[step: 4888] loss: 0.07959721982479095\n",
      "[step: 4889] loss: 0.05414770543575287\n",
      "[step: 4890] loss: 0.04492209106683731\n",
      "[step: 4891] loss: 0.04121999442577362\n",
      "[step: 4892] loss: 0.044918034225702286\n",
      "[step: 4893] loss: 0.045927684754133224\n",
      "[step: 4894] loss: 0.04588525369763374\n",
      "[step: 4895] loss: 0.03963443636894226\n",
      "[step: 4896] loss: 0.03879018872976303\n",
      "[step: 4897] loss: 0.03157034143805504\n",
      "[step: 4898] loss: 0.03699357435107231\n",
      "[step: 4899] loss: 0.03614673763513565\n",
      "[step: 4900] loss: 0.039986755698919296\n",
      "[step: 4901] loss: 0.03687918931245804\n",
      "[step: 4902] loss: 0.035126108676195145\n",
      "[step: 4903] loss: 0.03170299157500267\n",
      "[step: 4904] loss: 0.030752867460250854\n",
      "[step: 4905] loss: 0.031538866460323334\n",
      "[step: 4906] loss: 0.03390031307935715\n",
      "[step: 4907] loss: 0.032517656683921814\n",
      "[step: 4908] loss: 0.03253905102610588\n",
      "[step: 4909] loss: 0.030954081565141678\n",
      "[step: 4910] loss: 0.03139336779713631\n",
      "[step: 4911] loss: 0.0312562994658947\n",
      "[step: 4912] loss: 0.03246210888028145\n",
      "[step: 4913] loss: 0.03251988813281059\n",
      "[step: 4914] loss: 0.032417796552181244\n",
      "[step: 4915] loss: 0.031281862407922745\n",
      "[step: 4916] loss: 0.03167608752846718\n",
      "[step: 4917] loss: 0.03192990645766258\n",
      "[step: 4918] loss: 0.033711597323417664\n",
      "[step: 4919] loss: 0.03700621426105499\n",
      "[step: 4920] loss: 0.04034893214702606\n",
      "[step: 4921] loss: 0.047372106462717056\n",
      "[step: 4922] loss: 0.054424867033958435\n",
      "[step: 4923] loss: 0.06694948673248291\n",
      "[step: 4924] loss: 0.07377127557992935\n",
      "[step: 4925] loss: 0.08302474021911621\n",
      "[step: 4926] loss: 0.0865568146109581\n",
      "[step: 4927] loss: 0.07713064551353455\n",
      "[step: 4928] loss: 0.05808698758482933\n",
      "[step: 4929] loss: 0.04129040613770485\n",
      "[step: 4930] loss: 0.03605091944336891\n",
      "[step: 4931] loss: 0.03254318609833717\n",
      "[step: 4932] loss: 0.03857729583978653\n",
      "[step: 4933] loss: 0.04738066345453262\n",
      "[step: 4934] loss: 0.057605233043432236\n",
      "[step: 4935] loss: 0.056566398590803146\n",
      "[step: 4936] loss: 0.058962382376194\n",
      "[step: 4937] loss: 0.044904861599206924\n",
      "[step: 4938] loss: 0.03941246122121811\n",
      "[step: 4939] loss: 0.030356304720044136\n",
      "[step: 4940] loss: 0.02985296957194805\n",
      "[step: 4941] loss: 0.03076157160103321\n",
      "[step: 4942] loss: 0.03583969548344612\n",
      "[step: 4943] loss: 0.038094133138656616\n",
      "[step: 4944] loss: 0.04160543158650398\n",
      "[step: 4945] loss: 0.03954194486141205\n",
      "[step: 4946] loss: 0.04295092076063156\n",
      "[step: 4947] loss: 0.04384630545973778\n",
      "[step: 4948] loss: 0.054440662264823914\n",
      "[step: 4949] loss: 0.07989075034856796\n",
      "[step: 4950] loss: 0.10791783779859543\n",
      "[step: 4951] loss: 0.1526973694562912\n",
      "[step: 4952] loss: 0.11632717400789261\n",
      "[step: 4953] loss: 0.08749911189079285\n",
      "[step: 4954] loss: 0.10070154070854187\n",
      "[step: 4955] loss: 0.16816480457782745\n",
      "[step: 4956] loss: 0.14944154024124146\n",
      "[step: 4957] loss: 0.05488651990890503\n",
      "[step: 4958] loss: 0.13522817194461823\n",
      "[step: 4959] loss: 0.14005284011363983\n",
      "[step: 4960] loss: 0.06726491451263428\n",
      "[step: 4961] loss: 0.14290088415145874\n",
      "[step: 4962] loss: 0.09858766198158264\n",
      "[step: 4963] loss: 0.06921807676553726\n",
      "[step: 4964] loss: 0.11515545845031738\n",
      "[step: 4965] loss: 0.11911822855472565\n",
      "[step: 4966] loss: 0.1443101167678833\n",
      "[step: 4967] loss: 0.09865397959947586\n",
      "[step: 4968] loss: 0.11293894052505493\n",
      "[step: 4969] loss: 0.09014598280191422\n",
      "[step: 4970] loss: 0.14886678755283356\n",
      "[step: 4971] loss: 0.08777357637882233\n",
      "[step: 4972] loss: 0.09974631667137146\n",
      "[step: 4973] loss: 0.08374831080436707\n",
      "[step: 4974] loss: 0.08066334575414658\n",
      "[step: 4975] loss: 0.10239963233470917\n",
      "[step: 4976] loss: 0.05954161658883095\n",
      "[step: 4977] loss: 0.08941017836332321\n",
      "[step: 4978] loss: 0.05159153789281845\n",
      "[step: 4979] loss: 0.06497734785079956\n",
      "[step: 4980] loss: 0.05201208218932152\n",
      "[step: 4981] loss: 0.056467607617378235\n",
      "[step: 4982] loss: 0.05691603198647499\n",
      "[step: 4983] loss: 0.04978613555431366\n",
      "[step: 4984] loss: 0.04721139371395111\n",
      "[step: 4985] loss: 0.04774225503206253\n",
      "[step: 4986] loss: 0.041720375418663025\n",
      "[step: 4987] loss: 0.04800330102443695\n",
      "[step: 4988] loss: 0.03962023928761482\n",
      "[step: 4989] loss: 0.03848204389214516\n",
      "[step: 4990] loss: 0.043292734771966934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4991] loss: 0.03367822989821434\n",
      "[step: 4992] loss: 0.04078979045152664\n",
      "[step: 4993] loss: 0.031703345477581024\n",
      "[step: 4994] loss: 0.03804904967546463\n",
      "[step: 4995] loss: 0.037138354033231735\n",
      "[step: 4996] loss: 0.04041282460093498\n",
      "[step: 4997] loss: 0.03826652467250824\n",
      "[step: 4998] loss: 0.03837522119283676\n",
      "[step: 4999] loss: 0.03789706155657768\n",
      "RMSE: 0.059057340025901794\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f9f1bfc278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "#     tf.reset_default_graph()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss, pred = sess.run([train, loss, Y_pred], feed_dict={\n",
    "                                X: x_train, Y: y_train})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: x_test})\n",
    "    rmse_val = sess.run(rmse, feed_dict={\n",
    "                    targets: y_test, predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "    \n",
    "    plt.plot(pred, 'r')\n",
    "    plt.plot(y_train, 'grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f9f325c940>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f9f1ba1da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pred, 'r' ,alpha=0.2)\n",
    "plt.plot(y_train, 'grey',alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f9efc71cf8>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f9f32be908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_predict, 'r' ,alpha=0.1)\n",
    "plt.plot(y_test, 'grey',alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e42ce0a978>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e42cd6f470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    plt.plot(pred, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e42ce69c18>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJztnXmYHFW5/79vZiaZmSyzJJM9MCSgEkBZIqAgggEJRMGLgEEFVBS9rD56L0RRriwPCj9REbl4UZBFdrhA9AZZAl6FSyIJS0iCMUMCIRCyTMi+zEzm/P54+/WcWrq7uqe7uqrn/TzPPNVdU1116tQ53/rWe5YiYwwURVGU6mJApROgKIqilB4Vd0VRlCpExV1RFKUKUXFXFEWpQlTcFUVRqhAVd0VRlCpExV1RFKUKUXFXFEWpQlTcFUVRqpDaSh14xIgRpr29vVKHVxRFSSULFixYb4xpy7ddxcS9vb0d8+fPr9ThFUVRUgkRvRVlOw3LKIqiVCEq7oqiKFWIiruiKEoVouKuKIpShai4K4qiVCF5xZ2IbiOitUS0KMv/iYh+SUQdRLSQiA4ufTIVRVGUQoji3G8HMC3H/08AsE/m71wAN/c9WYqiKEpfyCvuxpi/ANiQY5OTAdxpmLkAmoloTKkSqCiK4rJtG3DXXYC+ITQ3pYi5jwPwtvN9VWZdACI6l4jmE9H8devWleDQiqL0N/7wB+Css4AVKyqdkmRTCnGnkHWh91RjzC3GmCnGmCltbXlHzyqKogTo6vIulXBKIe6rAExwvo8H8G4J9qsAWLIEWL++0qlQlOTQ28vL3bsrm46kUwpxnwXgrEyvmcMBbDLGrC7BfhUA06cD11xT6VQoSnIQcZelEk7eicOI6F4ARwMYQUSrAPwHgDoAMMb8GsBsACcC6ACwHcBXy5XY/sjWrfynKAqjzj0aecXdGHNGnv8bAOeXLEWKh95edSiK4qLOPRo6QjXhqLgrihd17tFQcU84vb1aiBXFRZ17NFTcE446d0XxIoOX1PTkRsU94ai4K4oXde7RUHFPOCruiuJFY+7RUHFPOHGJ+6ZNwFNPlf84itJX1LlHQ8U94cTVoPr73wPHH6996pXko849GiruCScu575jBzdUdXeX/1iK0hdU3KOh4p5w4hJ3fdRV0oKW1WiouCcY6fIVRyEWF6QVRkk66tyjoeKeYOJ0KOqGlLSgZTUaKu4JJk6Hom5ISQtaVqOh4p5g1LkrShAtq9FQcU8wKu6KEkSdezRU3BOMiruiBNGyGg0V9wQTZyHW3jJKWtCJw6Kh4p5gKtGgquKuJB0tq9FQcU8wGpZRlCAac4+GinuCUXFXlCBaVqOh4p5gVNwVJYg692iouCeYOBs5tUFVSQsq7tFQcU8wOkJVUYLoU2Y0VNwTjIZlFCWIGpFoqLgnGBV3RQmiZTUaKu4JRsVdUYKoc4+GinuCUXFXlCBaVqOh4p5g4nQo2ltGSQvq3KOh4p5g1LkrShAtq9FQcU8wKu6KEkQnDouGinuCUXFXlCBaVqMRSdyJaBoRLSWiDiKaGfL/PYjoWSJ6mYgWEtGJpU9q/6MS4q5uSEk6WlajkVfciagGwE0ATgAwGcAZRDTZt9kPADxgjDkIwAwA/1nqhPZHtEFVUYKouEcjinM/FECHMWa5MaYLwH0ATvZtYwAMy3xuAvBu6ZLYf9GwjKIE0bIajdoI24wD8LbzfRWAw3zb/AjAk0R0IYDBAI4tSer6OSruihJEnXs0ojh3CllnfN/PAHC7MWY8gBMB3EVEgX0T0blENJ+I5q9bt67w1PYzVNwVJYiW1WhEEfdVACY438cjGHY5B8ADAGCMeQFAPYAR/h0ZY24xxkwxxkxpa2srLsX9CBV3RQmizj0aUcT9RQD7ENFeRDQQ3GA6y7fNSgBTAYCI9gWLu1rzPqLvUFWUIFpWo5FX3I0xPQAuAPAEgNfBvWIWE9GVRHRSZrPvAvgGEb0K4F4AXzHG+EM3SoHEWYi1t4ySFtS5RyNKgyqMMbMBzPatu9z5vATAEaVNmqJhGUUJomU1GjpCNcGouCtKEHXu0VBxTzA6QlVRgqgRiYaKe4LRBlVFCaITh0VDxT3BaFhGUYLoU2Y0VNwTjPaWUZQgakSioeKeYNS5K0oQde7RUHFPMK7QlnvUgIq7kha0rEZDxT3BuIW33C5FK4ySFtS5R0PFPcG4Qltu0VVxV9KCltVoqLgnmDjFXRtUlbSgzj0aKu4JphLOXSuMknTUuUdDxT3BaFhGUYKoEYmGinuCcQuvNqgqCqNlNRoq7glGnbuiBFHnHg0V9wSj4q4oQbSsRkPFPcFobxlFCaITh0VDxT3BqHNXlCAalomGinuC0RGqihJEy2o0VNwTjDp3RQmizj0aKu4JRgcxKUoQNSLRUHFPMNqgqihB1IhEQ8U9wWjMXVGCaFmNhop7gtGYu6IEUeceDRX3CvDGG8DhhwMbNuTerj+IuzHA7NlAT0+8x1XSixqRaKi4V4CFC4F584Bly3Jv1x/EffFiYPp0YM6ceI+rpBd17tFQca8A4lJ37sy9XX8Q9y1beLl1a7zHVdKLOvdoqLhXABH3Xbtybxdng2qlest0d/NSwzJKVNS5R0PFvQKoc7eouCuFouIejaoQ99/8BvjVryqdiuiouFtU3JVC0bBMNKpC3O+5B7j77kqnIjrFhGWqdYSqiLssFSUfOitkNKpC3Lu60iUO6twt6tyVQlHnHo1I4k5E04hoKRF1ENHMLNucTkRLiGgxEd1T2mTmpqsrXeJQjLhrg6qiMBpzj0ZecSeiGgA3ATgBwGQAZxDRZN82+wD4HoAjjDH7Afh2GdKalbSKe1LCMsbYR10VdyUKS5YAd9xRmWOrc49GFOd+KIAOY8xyY0wXgPsAnOzb5hsAbjLGvA8Axpi1pU1mbrq7NSzTF0TYy32cMFTc08mttwLnnVeZY6tzj0YUcR8H4G3n+6rMOpcPAPgAET1PRHOJaFrYjojoXCKaT0Tz161bV1yKQ0irc0+KuMcZ2/fT1cXLNN2cFb5ucu3iRp17NKKIO4WsM77vtQD2AXA0gDMA/JaImgM/MuYWY8wUY8yUtra2QtOalbQ2qCYlLFNJcVfnnk66u/maGb8SxIA692hEEfdVACY438cDeDdkm8eMMd3GmBUAloLFPhb6g3MvZ0FWcVcKpZLXTZ17NKKI+4sA9iGivYhoIIAZAGb5tnkUwDEAQEQjwGGa5aVMaC76g7iXsyC7Nw4VdyUKlRyfoM49GnnF3RjTA+ACAE8AeB3AA8aYxUR0JRGdlNnsCQCdRLQEwLMA/t0Y01muRPvRsEzfiLPLpR8dxJROVNyTT22UjYwxswHM9q273PlsAHwn8xc7XV0AhbUMJJSkOfdSHue557gnxW23Rbsm6txLx7ZtwKBBQG2kWt035HpVUtz9ZbW7mwW/vj7+NCWR1I9QNUa7QvaVUh5nzhzg9tvzP5UIKu6lY8gQ4ItfjOdYlXLu7pgMv3P/5CeBhoZ405NkUi/uaRSHpE35W0pxL7RrYxqvX5J58MF4jlNJcXc/u99feIGX27fHm6akUjXivnt3ZbplFUNU5x5XQ2cpjyPiHlWsVdxLQ9xlv9LiLqGnsPL68svxpSfJpF7c3YEUaRGIag7LFOvckx5WS7pxiDv/KhVzl/JZV+f9DgB77MHLv/0t3jQlFRX3CpDk3jJpDcsYAzz2WHny6bzzgAEJrylxjxat1E1Zrq84d/epc9gwXs6bF2+akkrCi2x+3EKddPcnqHO3lErc580DPvc54K9/7dt+wrj5Zl5GbSSuBP1N3MOcu9SnlSvjTVNSqSpxT5tzr8YRqpUSd3nBdjlftL1+ffn23VfiFtm4xP3224G3nZmtcjn3HTviSVNaUHGvAP0hLFNog2pfK6T8vhwOdsgQXpZqrrstW4D33y/NvoRqdO47dgBf/Spw1112XS7nLuJeqQnNkkZViXta7thJC8u47qevTwiFVvpSOXcpB+Wo2E1NvCyVuJ9/PnD66aXZl1CNDapyLd16EsW5q7gzVSXuaXPuSRH3agjLyHHLEReXhrpSifvq1fxXStx6EIe4xeHco4i7fDemeHFftQp49NHi05lUUi/ubuFKm3PftSt3F7ve3tz9eUtFNYl7OYRt6FBelkrcd+7Mf2MvFPe8t2wp7b7DiFPc3Ru2Pywjx3e3CSsDTz4JbN4cfpzf/Ab4/Oerb5bJ1It7mp27TJ2Qjd5eW4irtUG10Bh9vv2UQ9xlSHupxH3XrtKLu5vf/UHcBw/mpeSjuHb3d8LGjcC0acDvfx9+nG3beL+lviaVRsW9ArjpzFWg0uzcq6lBVfZdSnEvdfjIPe9sDrWUxBlzzyXuIupSjwYODJaBrVvZSGW76cn+VdwTRpobVIHclTzN4l5NYRm5Rkl27nGLe6Wdu/RgknlkROSbm4NlQPI6W12T/6u4Jwx17n2nHHPLFCvuL74IHHdc4c62nA2qpRb3nTtLn043v/uDuPuduyybmooXdze0Uw1Ulbin0bnnE/ew/rylJknO/ZvfBJ5+Gli0qLjjpsW5y9zjpaK/OHfpgJBN3IcN47IU1v89m7iXKiyzfTv3vEkKVSXuaXTuUcMy1dqg6hcJedwutFEwDnEvZODRW28Bn/408G//Frx2sr9Suve4e8skLebuOnd/uuIKy3z608CECfm3iwsV9wrQ02PfFpOEsIzse8CAvt9E+jrlb7HiXs4GVRGFQh7b580DnnoKuP56YNky7//yiU0xxBmWMcaWk0r1c88Wcxdxd8tBXGGZ55/3prHSVJW4pyksI4UzbnHfvj3Yt94dGFLpsIz0Kd+0qbjjJkXcXSHxO/6wMIAx7PJ//OPiynGczj2usSXF9Jbxi/vKlcCGDcH9uJS6t0xYOanE+15TL+5u4UqTcxdx37ULWLMG+N//DW5X6pj75s1cKa680rteCl6SxL2zwNerx9Gg2tMTvYxlE3c3Huxus2ULu/zvfx/47W8LT6Mr7tu2Ff77QkiiuGdz7h/9KHD11cH9uJTCuefK/64urluXX45YSb24p9W5u4MwfvlL4Pjjwx2169y7u4FzzwXefDN8vytWAE88kf24MuT97ruDxwGSIe4yYKhYcS+Xc29s5M9RBcB1ga64u+vdz+5slkuWFJ5GN7/LOTOm/1iVFvdcYZmuLmDtWmD58uB+XEoRc5djAEFxl6kNbrml+P0XQ1WJexqd+86d3Atj165gpfQ3qC5bxkOlswn49dcD//Iv2QVaCp3/JcJJEHe/yCdF3MVpNzfzdxGTN97IPT9MNufurs8m7q5QREXOe8iQ8jt3t55VWtzDesvIb6XtQa5ZOcMy//iH/ezPfxH1I44ofv/FoOJeAfxhGYkv++PMfue+cSN/lqWfNWu4kK9dG/5/EZls4l5X1zdx7+21IZ5iG1SlopVL3J96qrAXekh6RNxFRL7wBY6R5/sdYGO+/vXuZxH3ujq+cRSK5GNzc+XDMv/+78Cf/tT34+SbW6auLntYxq1XQjnDMrnEfenS8PXlpqrEPa1hmTBxP+EEYM4cr7iLOGcTd+mHnS1sk0/c++rci3lcL5W4R+0t8/3vB9scciEVv6WFlyIAq1fnfnmHnMfgwdmd+5o1tsJLI+hHPsLhtUIb4OS8W1oqK+6bNwM//SmX376SS9wHDOBynCvmXqi498W5u9fY/wQueRTH+AOXqhL3NDr3MHHv7bXOJ0zcs/W3FrHJJu7iICV+LJRK3Iu50frFXfbR2cmCFzVEEbVBdevWwnri+J27POJv3Gg/Z/vdoEFAa2v2mPvJJwN77WXTBbC4d3UB77wTPY1AcsT9tdd4SdT340QRdzfmTmTrVSHiXoqwjFsW/PkveRTH+AOXqhL3NDn3XGEZV5wHDOC/KGGZYp17qXrLFHotdu+2jcj+qVs7O4F99wUmTSrs2Pmc+7ZthTmosLBMdzdX5lzivnMni3tLS3bnDthr5oo7UHhoprubxa2pqbIx91de4eW4cX0/Tq5+7gMGsElxu0I2NHCey2/917kcYZnnnwf22cc7etmf/5Jf6twLpKvLuoRyOvdNm4BTTwXee6/v++ruzh2WkQoCWHHfvTt3WKa314Yy8jl3v6ty45h96Y9bqLjLNgMHWqGXCrhypXWvuea89x+7XOLuhmXkOuUSUXHu+cQd4PMTcT/4YF4uXhw9jQCfd10dlys3LNDVBdx6a2kH1oQ59+XLeVTuq6/ydxHZviDXcvduWy5zhWUaGrgsyW+jOHe3zBXj3BcsADo6uLODPGWrcy8RXV02zFBO5/7b3wIPPwxcd13f95Uv5i4VBPA691zivnGjrQD5nLu/EJeqQbXQEJlsL9dv9+5wcb7mGuCii6LtK4q49zUsI/nf17CMsGaNFeR992XXK6Mdo9LVxcI2eLBXXP7jP4Cvfx2YNauw/eUiTNw//GGgvR3429/4e7b2iELy3j2OXIdc4l5fX7i4d3db81CMc5djrF8PtLXx51zOPYpRKRWpF/fubisO5XTucsH88epC6e3lC1xXx5V/61a77zBx7+5mR9DdnTssI5VpwIDsj/Ti3LOJe9xhGdlGwkQ9PVwBDz8c+NWvbIjiBz8AbryR8623F7jqquCgrygNqrt38/63b89fVp57DvjgB22euWEZyf+ozl328eST4QNZli+3rm7IEODII7lHTyFC0N0dLu7z5/NSXGUpCBN3OearrwIjRnBZ9l+Le+7hfFy4MNpx3N+LMEueSFhGbrDvv8/7ziXu27YBM2d6ey9lG3cQFSkL69aFi7u8kEeeiuOcVjj14u46954e4Ec/Av74x77v1xjgkEPs21vkgonjLhYRldparvxut0UpjGvWeNdJIc7l3CXmd8wx/Jj49tvBbaRQ+x1KOcR9/nzg3ntzby/C4D557doFDB/OL5H+4Q+9269cyd3sLr8cuOyy8GPnalB1nXa+R+SLL+bubS+/zN/dsExU515f7w3LHH888Je/BLd94w2+yQ8cyH9HHskhqZUrc6fRxQ3LuOIi5YCI07tgQfR9+lmzhkd7ht3EpWH461+3vZH87v2RR3j5979HO16YuGdz7u+9B4wZk1vc168Hrr0WePzx4H6BcOF9883sbVyAPcaWLXxzqa31hsXkabq11W4XF5HEnYimEdFSIuogopk5tjuViAwRTSldEnOza5cV3O5u4IYbgPvu6/t+d+wAXnoJeOEF/i4XrK+PVa6419d7xV1iwW7h2LjRDkzJJe5Skc4+m5duARayhWXK0aD6178CX/xi7u3DnHtXl43XilsWXn0V+N3v+HPYEG9/Gvy4v8kXdxfRkOsl4r59u63QO3dmzy+3QXXHjtw3neXL+ZpLI/uhh/LSbXvJh4Rlhgyx+QjYKWh37ABuuw342MeKH8H66KN8w339dbvOde7f/CYPshs5ktf5p0iWPPU36Oc6JyGKuI8e7RX3bNfY7Wbr1oWwsMzxxwdNhotbFxsagjdXKT8i7nE2quYVdyKqAXATgBMATAZwBhFNDtluKICLAMwrdSJzsXNnsPtTvrje7t35+1H7XbQIo3tx/vpXFtMwwX/kEftI7OIXd79LB7jyjRlj10mBkYIkb/Lp6rKP71KRjjoK2HPP8BhrnM5dyOVus4VlpIL6xf3pp+11WLbMm+9RxN1Ni1xH6UL30596t5U8knISFpbx79NFwjJjx/L3XKGIFSu84j58OC8LjU9LWAawr5YTodmxg8tId3fhYwgEuSm4hmTpUi7nW7faY48YwUu/c5e8kuubjyjiLhPhhYl7tvzr7OSn22OP9d6Awpz7O++EPwUL7jEaG4PiLmU8qc79UAAdxpjlxpguAPcBODlku6sAXAcg1pdVSReomhoWL2PyV4o77+TGnzAHs3Ur71MqsIivDDV3L87//A/vK+yCnXIKT1rkxx+WCRP3LVuAPfbgzxs32gLjNsxt3AjcdReL+a232orU1gaceSan7Q9/8B67VA2qvb08e+G773rXhwnraafxlAhhSHpEzETcszn3++/n5Wc+w/nhDv8v1LlLXovQ+d2ZiLvka7Hifvjh/D3XvCIbN3K5k0nTZFmIy3PDMgCfqyuuO3bY8y9kXnoX+b1bZl9/ncv59u325iSx52zOvaODy0++spZP3KUr5KZNXKbzhWWEzk6+3nPmAA8+GEyf0NPD57xhA+9v6lTg2We927hlIYq4J8q5AxgHwL13rcqs+ydEdBCACcaYEkS787Npk2103LmTHXBtrS3M+cT973/nyhT21pSmJmDy5KBzly6Q7sURYcgVk/OTLSwzeLDXuYu49/Z6xX30aHtMOe6ll/K5DB7MBewHP+BeF1dd5T2u3ITyNag+9hjHtrPxl7/wSM/vfMe7PkxY//QnngsnrIul3Bz23NOmMZe4S1595jO8dOdJj9KgGhaWyZYnfufe2Mji6faW8e/TRWLuH/oQ37yyzfQ4ciRf9y1brDiKuBfi8tzeMpKujg7v+RQi7j093Pj41lt2nV/c/eEVOXY2cZcb4cUXc/nxtz8sWuR9A1fYnOxhYRmpm1Gd+/r1tmy5XZv9ZUDyv7OTb2LPPBNsyHeP0dAQnNtH6rsYmKQ597CxZv98ICaiAQB+DuC7eXdEdC4RzSei+ev68M6y888HDjyQC1kx4i4X1O88AS48K1YExT3MuUvF91cW15H8539y7HTaNI6D+8Vdtt1jDz6mNCqKuANW+Lds4ScOOaaIzIYNXOjkLTCDBvFbYRYvtvt38yRfWOarX+UwxamncgX3IyEG/+N1mLD29vLxZH4NYdUq22VTxL2724oUYIeSA5wW4dhjeTlzpr0+URpUw8Q9m5PyO/dBg6yYuHmZzblLzJ2IuwlmY8IEToMblqmv5yfRV17hfu/f+lb+3kf+sMy2bV5xdcU9ihmZO5cbH++8066T38tN1t9zTNLf2srnvW4d93qS7SWvpP3CL+4HHMB/QpSwzIYNrAVAeMw9rHdbZ2dQ3Gtrrbi/9x6HY+Q6b9hgbzp+2QoLy7gRgaQ791UA3JdHjQfgyuJQAPsD+DMRvQngcACzwhpVjTG3GGOmGGOmtMntvQhEVB94wIp7XZ3N+Hzi7hfsMETANm9mIZV9RnHuroicfz5w0EHsXk880faAkLCMsOeefAz5rcRqAS4wciOSXgkbN3ofuxcvBsaPt9/3248rkxxP0t/Wlt+5S2V5+GGu4P6JyOZlWlWiiLvw0kv2886dLGoXXcSVVM7V79xra61guI2z7e28fu5cvnm6x+7qyt7oHRZzd6+n+3QhldIVdwkDRHXuch5f+Qq7cekt4iI3dVfciXh2w6ef5t46//Vf4e03LmFhGTe2vn27FZ0ozl2E171ufnH3D1SS9NfU8I35tdeACy/kLpCAvWHK7+QYr7wC/Pd/h5+TECbu/nejuuIuE+i59Ujo7LQ3GBH35ma7v9NO47CqK+4yrYK/HSFfWMbfoJo05/4igH2IaC8iGghgBoB/NtcZYzYZY0YYY9qNMe0A5gI4yRiTpzgWz9578/Luu73O3RX3XPE8uaALF3orjSsK7qx2bq8F9+JIA6Vf3HPdnR96iJfi3IUPfIAFXEJFMnUpwJVGKqpf3OUR3hivuE/ONHnL3OBSUEeP5jxzz9XtLbN7ty2QX/kKL6VyCtKDyO9iRBDDRieeeaaNa7sVZPRomw9+cQesw9pnH+BTn2IXW1Njr5vcvKIMoMrn3KXhzH2yCXPuhcTcAeCss/g3xx8f3G7UKCvuci0B/uz2xxZxAXj+/y9/2X6fP5+fjKS3jJyrK+7ZwjILFvBN0o8I75w5wLe/zWnxh2X8bVZuN+GmJn4CBmx9k3yV/HvuOS5v3/se8PnP299K2cwn7qNGeY8/erR9uc0dd3D5lP26o7I7O21axeA1N3O9WLOGB5AtWGD/19VlDY1bdqVTgxDWWybRDarGmB4AFwB4AsDrAB4wxiwmoiuJ6KRyJzAMudCLFtnYpuvc3eHcYUjhvO46b6On62ifecZ+lsn299473Lm7leWhh7hgZUO6kbni3trKArBrF3DTTbxOKulHPuKtNBKW2biRj3/AAXaAiivu++7LSxnGLuI+ahTnj1so/Q2qmzYBJ53E3Q7328/brfLSS22l9Tt62ad/LIAItPSRdivI2LG2Qu7axZXdfSKQ3w4fzkJz8838/YMf5MZKuRl2dXGFd9PhJ5+4d3TwuZ9zjl0n11jEXWLu8rjvivvWrez63ngjeJOSMIKfpqZgWAbwCn1Dg7e3zY03srGRdqfPf55vTP7eMp2dXDZaW7OHZb7zHY6Bu+zezQLX0MBidMMN3A4jv5c88ZsYN/3NzdnFXfazaxev8z+VSPq6uuz5+MWdCLjkEm+7QksLr5dpkw880BoUNz87O62hES1obubzufNO+45YVwPCDI3f1OVqUG1q4rQV+vrIvhBp3JoxZjaA2b51oS+NMsYc3fdk5cZ9p2VNjXXurmPbtMnrfoXdu4OiZAxnfLa76t13cwU56igrdMaEh2VOOy132l1xl8o/ciQPmDrsMODXv+Z1Q4ZwYRs4ELjiCvt7v3OfNIkf7Zcv94p7ays7Gb+4S4OsxIQBm5/SBrBpk413jxljK/GGDXxDPPNMLrQy1FxwpxNwXef//R9XjnPOAf78ZyvCAIu73JxEKMOcuzRIuYwfb2OhMl/Pli1eUXBxhTgszLZiBYtC2OArf1hm7FgWFrci/+53fHMfOdKajnw0NdkbnptmKbtDhvANXsT9H/+wgnbhhRzakqeXsLDM8OF23vOwsMybbwZHr773Hm/7hS/YHkpr1+afkMxNv4glwA7YmPB+5B0dwVDH6tUs1F1dLMrbtoU790GDuPw//jiXMXHndXVcHqZOtfnopmfbtqD5GzKEz/GSS9gArVnjfXqX47tp9Qt1YyOn1y1TokkDB3KdLLYbajGkcoSqe6G3bLHO3SVXNyh/yEZEKZu4d3ZySMBfQOR3t93GL27I17XroIOsm3GduzxeSrc5gAvJ0KFcgN1KM2YMrxNxHzHCNki64g6wq5fHede5A+Ghh7a2oLi7TkRGa551FqdDXM9rr7GTFhddU2P3PXiLU/vYAAAbIElEQVQw9xj54he5Av3sZ94eCuPGWXGRCucX9yFDwvtGjx/PjlXeluTOtBmGnEdbW7hzX7MmfF6eYcM4jW6Dqsx66N4wRID32MN788yFiE9vr+1lAlin2drKDbILF3J47IMf5PXTp/MYhzPOsL8h8jr3DRv495Juf1imp4cbDt0bseQDwI3Y0uPqzTdzj1kAgs5deO+97PO2yBw63/62PRdpXxJxB8LFXZg2zTtHv6Szvd3mh9s4DwQN3imncNjsjjs4JLX33t5QGMBPsevX27CR37k3NHB+b9xo0ynOva6Or+/atWy44nhxR6rFHeCMHjQo6D6yibvbR1eQwiAVXXqqjBsHTJzInz/zGesi/IOgXnuNH5XDula6uCGgMHEXVw14K4rfETU3cwUVcZdQjV/cDz6Y0+Z2C5NjuSGodet4n/X1fG6bN3vFXfJHhq4ffDC7023b+O/ZZ9lRipOXwt/Wxj1biHjfP/kJNyy73Sxd5y4F3hXFceO8PYdcxo/39gpxB7O5LF7M7m7bNk7LvvvaeK/0qGhu5rIhTwLTptmnJGmUc8MyMsjMraQiBtLjKZe4H3AAsP/+XtFxG/9ccf/oR/n6feMbnB/f+hZPsXHttd6n1XfesQ2FnZ3WuWcT99WrOQ/cSecAW0fGjeNutYccwuJeiHN3z+u997I34j7/PJuBa67hqUMkXYBX3Jcv52v39NP8fUAE5dpzTy4TbW3cpuXi14HTT2enftZZvO1++wX3d8wxfG3FBEqdkvLb2Mj5L6YTCIr74sV83WWkdTlJvbgDNizjkk3cw6bsFfGSCyLdsWpquC/1ypUcihCXJfFMl66u/DP5HXKI/eyGZURwRTCA7OLe0sJCtGoVF5zhw60IhYl7dzcXqFzivnYtF7wBA+zkXGHOfcECvpG0tnqHmEt/8xde8MZ9r7vOtlcA3HNo//3tNTj6aH50lqcueYJwXfqPfxw+lYJ7vvJCDxECv7hffTU/OWzbxhXwoos4HPDwwyzuw4bZR/FFi4AZM/iYIraybGzka795s13nvp9TGt7ff5/FMpe4L1zINwM3dBgm7i0t3Hj68Y/zsR580LY7HHmkd58rVnBeDh/O55JN3MVxun3YXRcqwifXeK+9wsVdpkkQsjn3deuCje/C8uWc9w0Ntvy74i43K6lbv/iFN225aG/nOvzGG8B553n/5xd3/7WSOiUMGWLrr5yLhMPEfIi4A96nI4Dre1ub7RLsr6vloGrEXQRCLlI+537PPTackU3cN21iwZswgZfuyMGw2NmcOd7vS5bw4ybAhcN1A7W1NkYoBTWbc5fPAwdyJWhpsXHXESN4To/77w/GpWVu8Jde4nNpaLBi4j4my4x2boOfiHtjo63UL71kC7ikee1aK+7r13OFErfinoMgL99oaWHH/7GPAVOmsBiIaLkVraUlt3MHrLjLTcUv7m+/zeK1fDlv87nPcfoff9wr7m+8wSK2//7e9IvoDh1qn8784v7OO8G4bFjM/e23vY/72Zy7XKfWVi7bf/yjzS/hkEO8Uxy4N3C/uId1hXQnJnNDMxKyECPQ3s754o9TP/WUdx/+J0zBGA5JhrFhg22cHjqU9+GK+4gRLNCuKZs8OXuZcJH6PXRosDHbvVH5n6AA+8TubiNhM7m+jz7Kx5Bzk7op5wUEnbs81U5wO5eXiaoRd3HuctFfeCF8rggpJNOnWxfgF3ep3P4bhFS4LVvCxd1tXQc4bidupKmJG0wFY2wYKMy5u637Ummam/mG0NzsFfeRI/mx0s/EiZzm555jcWtqsoLjd+4jR3oLnN+5G8PCJo5GxH3NGm+PhYkT7b5zibt7Ixo6lJ2VON+oL3oQcZfjuzF3t+FLXvqxYAELSU0N5/WGDVbcR460bQpyc/eLu4wmlfQPGhTevVCcXdh5jB9vyxeQPywjYtHSwk86LoMGcU+ZGTP4uwzuChN3Nw78/vscW/7Sl+y+XHFfs4bzSc6/vZ2vaWen94bV2BgcjyH4RxcD3qdfMTbvv+8V3rFjuRdQb6+dRK6lxfu6xenTg/sOw30q8l8L19xcfHHwBTauuA8dyu0e7pw5mzfzze2UU2w+NTbaLo9SHvziLqhzz8LOnd5Gu/p6HiAEWOG/8UZu7fcPaFmzhrcfOtTbnW3ZMtvf1x0l5yKF5Q9/8DZkCdJYCnCBrauzhVymA336aRsDFKGQQuM6d7eiyGep6M3N9rzkt2EMGMAV/447eABNU5OtSDt3cgG9+moW7bY263QAr7j39rKI7dxpRUcK/4svehshJ0601yBM3OV3/hGXn/2s/Rx1YqkJE1jIpFeD3BxnzuR48e9/z+cn4i5TNAB2Kl7pVSVdRAErvvIE4Iq70NTE+9q8mXtIuWMBsg3yCcMVIPezG3PPxd13A9dfz8IrcwmNGsViuGuXbVCVm11DA5+3dBEUNmzgNpNVq7iOuH3IpU0H4JvwDTfwU1ZtLdfDYcPs/E6CX9xrariboeShG75wR5F+7Wv8hLL//hw2kl4mrhi7o5WjIjc2SaPs7xe/4BHZflxxnzWLp5mWtC9ZwnPzd3XxvElSpqKEZYCg0JeLEk7hHx+7dnEmuo+/0hhz4IF24MKsWTyBlsxFAtjZ44i84u42uEhhlpdFCHJxH3uMlzfeyN3RXI4+mrv7uV2w3OXUqbbyi7iLkLpC7e9xAnjFXfDHBv387GfA7NlcaSdNss5rxw6ezEoGFo0c6X3UdcUdsOEsEZ0RI7ih7+abOb4sXVHzibs4d/9TkXseUZ37gAH8kmmZlOvoo1non3ySv595ZvA3kqbmZvtSk0mTrJg1NtrrL09yYeLe3MzX+OGHvfHktrbczt2P69xd9xhV3AV3u1GjbGhDnLs73/9sp1Pz/vvbOV0uuYTLfFubV9zdazN6dPCtWE1Nwd4wcl4HHcThHBm/4c4btGEDlx3XuV96KZe5a6/l7yLuANeJbFMKuEyd6n3yBbxPkzLR2JAhwT7+gntDkycmY/jzFVdwG8jQodzDTW6qMogJ4BvgunW2V5Ur6OPGRWsQ7iupdO7iSIT6es6sK6/kx6Srr7ZT3i5dyuJ92WXsZlxXEjYQBeCLvmhRMMwiYYsFC7hwXnABj5x0OessXvrF3R/TA2wPBdkm2wV3wzLuUsIJuRg8mAu7pEHEffVq70yFbW3eR+x84g4AJ5xgb7DHHMPLSZOKE3f3PAp5/+Ypp9jP9fVW0D/0ofDtJYQhzt0NywDcLiLXIZ+4n3desKFwzz29o1rzETYWw10vN/RCcIV51Cgu5+Igv/AFu8+5c+0sh5dcwstXXw06d/eJLmz8QFNT8FpLGW1v555Uxx3H36X8ueflijWR7b8PcDmVut7SEu1NaE8/HZzqYcoUDlvJBG47duR+O1XYgDMiO8jpqae4zLtjC1zn/txzXD4k311xjyPeDlSRuLtcdpkVtJ4ednPXXMPCsmiRDX9IQfH3AhgwgCu53zW1tXGF7eqyF+iZZ7h7n3DKKVxopHKKSIbFIG+/nbsFylwu2cjm3MeNC8YKw/j4x3m5fr0ttF//undWxZEjvYU9m7i7lViE9bLLeEQrwC4vl7iLUEhjr+CeRyHiPnUqC9Npp/F5fve7fHN/6SXvIKsvfYmFX0TDL+4iZm48XNpEpH3AFbzm5vDRnXvuac1ClEFM/vEZQqHO3cVN52GHeYWqtZVDMmPH2oZsP6+95t3H4MFWmLKJe7ZRyf79y7V1xT1MSOXp+623vOJeLPX1LMjSIWDnzvyvHrzzTv6Ny4QJduqHI47gpVwrmZXVvaYSfqytteYgjng7kPKwjBBWieTC7d7tnfxo9er8zj0bRHxxOzqsuI8fz3/3388OpamJK0K2sIzLpEnRXrgtIukX9yjdwQAr7osX231MmsRCd8MN/ETjj937xV0aol3n/pGP2Jc0rFvHn932ijBxHziQB9/IYBwXInZFUWPuAF9neYQX5BV87riCn/7U26bR0mL76Q8dGi7ut97KsVZ52vA795oajtmuXMlOsaGhuCeQT3yCw0sush+3kT0qfufuiqeEPC6/3Bv6A/jGeP31/NkvQO3tfI3DxP2AA4IdDKSM+gVZ6qpbH8LEfdIknkPnmGP47U5AcTc6PyK8O3YEQzd+wsJ6AOdffb2dpmLGDG9jatg7ZuvqbF6ouOfAL+5hlUjEvaeHxf0DH2DxBYoXd4Dj0q64C25vlb33tv/PFZYJY/XqYG8gf1hGKkPURhmZROzss/k3y5ZxZa2t5cbQn/88GB5wHzWBcOfubtfWFpweONsjtL9/tjBuHLcNFOLco+K/EbrlZ/RoFqjp0+0TCMCi676cwc1vN79k362tXsGIeh5h71U9/HBeLzfmQvA30PvF3W3od3HF/V//1fu/9nZuPA8Td+nC6pLtiTXMuWcrJ9KeVQrnLogu9PYGb25RaW5mUySMHcsDy8JwwzIjR/KTpoSoyk1ViHuYc5e4aU8Px8hPPJHDEhs2BMMy27d736SeCxHtXHGzRx6xDkHmeBGBzYfrLoXGRo7fSY8SGXAS1bkPGMCuWvJJZtUE2IUcd5ztpnnrrTxQRsIkuZx7Ni69lPdbaKPR+PEs7qUcmj13Ls/o50+LKzoykjHfi9Xr61nUjfEKg5iF1lav6PflZepEfHMpBnnSkFcH+sU9G2PGsBHaY4/gmAlpVI16Tq2tPNuj2yYChL+IJd87VUXcS+ncgfxhmVLghmVqauwI2zhIrbjLLGvGZI9t1tayWKxdyzHeefNY3KUyyu+2bcs+94Uf6VGSS9zdilFXl3ve+CgQ2d4GAMePn3zShh+ikK1S1tVxw6jwta/xn/93YQ2q2fjJT/ivUO69l3siuCN5+8phh3nHFwiuOXAbDPMhk4L518k+XXGXmTnjZsQIbxfgfOK+YIENhWUbbCS9R6KKOxG3c/nJ16AaRimde5zi3tDgDcvETeoaVGWSqPp6WyiyiXtNjRXWsWOtqEthl2lYOzu5Mnz2s3be5mxEce7lZvhwdplhLyIoNbkaVEtNezvPuVFIzL1YXKFwu73lY+TIYKghW1gmLPRRCVxxl655Lgcf7G1rCOO447i3iX+OlkIR5z50qBXXOJ27K+jlEncxNl1d3n7ucZM65y6uSaZg3bYtt3OX7evqOFRw+uleJ9fYaLuzHX98cL4MP1On8nb+3h7VSjFhmTTginshInzmmcHZQ8PCMoXcMMqNDMa64oriHeTeewd7jhSD1NX6ev7burX6nPull7Jj/+EPbUSgEs499eIO5BZ3GbxQW8vC7Z/StbHR+5LqfEyc6J3nudrxO/co/YzTQLFCEdZw5oZlpBL75yapJF/+MteXs8+udEqsc29osOKez7lLj6FSPKnG4dwBe54yH4+KewQKEfeaGrt9tgvpOvdyhhzSitvoPGRIPCPr4qCUIRNx7iNGWOcedf6TOBg2zPt2qUoioifOHcgv7nvsweHSUjwtE7EuyKjqciHnJuKuYZkIhIl7ti5nfuceRmOjnemvL70bqpWaGs7fXbuqJyQD2DLj9hwqlpYWnobgE5/g7pJz5+YP7/VXXEGXz1GeBkuZn3V18Ym7hPBU3CPgivvgwbaLURg1NdHEXZ17bgYP5nyvtvyRbn+lwO3yF9Y7R2H8YRn5HCd1ddFGqPYF17m703vHSarFvbEx9xBvt0E1l7gL6tzDGTyYu5BWk3MHsnf7U8qHv0EViL8dR7Qgjpj7li2Vce1ACrtCihOPKu5RnLug4h6OOPZqc+5K/CTFuQPxOfdKNKYCKRR317m7c7KHEbVBVVDxCueoo3jpvmdTUYohrEE1bueu4p5QXHGfOZOHy2cjinPP9q5SxfKNb/Ay3ztiFSUfBxzA3UTHjq2cc48jLOM2qFYqLJPamHt9Pc/ZnW3eboCde74RYu4QcRX3cA45hGctTFL3PiWdHH64fUlKNYdl3Jh7pZx7asU9yox7UQYsuHOpV+oipIFHH610CpRqo9JhmWJnhYyCG5bJ9SrMcpLqsEw+ChV3RVHiQxx7NYdlpCtkJahqcXfvzNkyuBTzVSiKUjjVHJZxe/Fpg2pESu3cAQ3HKEolGDaMOzSUMzwSRpwxd/d4cdPvY+4ATwscdT53RVFKw4UX8gyrcRNnWKbcx8lFVYt7lLAMEHzrjKIo5WfUKO/7XuOiv4RlUifun/0sv44tSpwuruk9FUVJD/0lLBMp5k5E04hoKRF1ENHMkP9/h4iWENFCIppDRAW8uKwwPvQh4IwzosXpojp3RVH6D3GEZYiswCe2twwR1QC4CcAJACYDOIOI/K97fhnAFGPMhwE8BOC6Uie0GNS5K4riJw7nDlhxT7JzPxRAhzFmuTGmC8B9AE52NzDGPGuM2Z75OhfA+NImszhU3BVF8ROXuEvcPcniPg7A2873VZl12TgHwONh/yCic4loPhHNXyeTqJcRNyxTLW8QUhSlb8QRlgGsuCc2LAMgbJp5E7oh0ZcBTAHw/8L+b4y5xRgzxRgzpa2tLXoqiySui6goSnqIy7nLCzrkHbBxE+X0VgGY4HwfD+Bd/0ZEdCyAywB80hizqzTJ6xvi3FXcFUUR4hL3t97i5Uknlfc42Yji3F8EsA8R7UVEAwHMADDL3YCIDgLwXwBOMsasLX0yi0Odu6IofuLWhU99Kp7j+Ml7esaYHiK6AMATAGoA3GaMWUxEVwKYb4yZBQ7DDAHwIPGzyEpjTIXuVxYVd0VR/MQxKyQA/OAHwMaNud8WV04iyZ4xZjaA2b51lzufjy1xukqChmUURfETV1jmqqvKu/98VHUfEnXuiqL46S+6UNXirs5dURQ/cTn3SlPV4t5f7tCKokRHxb0KUHFXFMWPinsVoGEZRVH89BfTV9Xi3l8uoqIo0VHnXgWoc1cUxY+KexWgzl1RFD/9RRdU3BVF6Veoc68CNCyjKIofFfcqQJ27oih++osuVLW4q3NXFMVPXBOHVZqqFvf+codWFCU6GpapAlTcFUXx0190oarFXcMyiqL4GTGCl62tlU1Hualqce8vd2hFUaLz8Y8DS5YA++5b6ZSUl6oWd3XuiqL4Iap+YQeqXNxF1Ku9VVxRFMVPvxB3de6KovQ3qlrcNSyjKEp/parFXZ27oij9laoWd3XuiqL0V6pa3NW5K4rSX1FxVxRFqUKqWtw1LKMoSn+lqsVdnbuiKP0VFXdFUZQqpKrFXcMyiqL0V6pa3NW5K4rSX6lqcVfnrihKfyWSuBPRNCJaSkQdRDQz5P+DiOj+zP/nEVF7qRNaDOrcFUXpr+QVdyKqAXATgBMATAZwBhFN9m12DoD3jTF7A/g5gGtLndBiUHFXFKW/EsW5Hwqgwxiz3BjTBeA+ACf7tjkZwB2Zzw8BmEpEVLpkFoeGZRRF6a9EEfdxAN52vq/KrAvdxhjTA2ATgOGlSGBf0PncFUXpr0QR9zAHborYBkR0LhHNJ6L569ati5K+PjFhAvDDHwLTp5f9UIqiKIkiirivAjDB+T4ewLvZtiGiWgBNADb4d2SMucUYM8UYM6Wtra24FBfAgAHAlVcCY8eW/VCKoiiJIoq4vwhgHyLai4gGApgBYJZvm1kAzs58PhXAM8aYgHNXFEVR4iFvU6MxpoeILgDwBIAaALcZYxYT0ZUA5htjZgG4FcBdRNQBduwzyploRVEUJTeR+pEYY2YDmO1bd7nzeSeA00qbNEVRFKVYqnqEqqIoSn9FxV1RFKUKUXFXFEWpQlTcFUVRqhAVd0VRlCqEKtUdnYjWAXiryJ+PALC+hMmJmzSnX9NeGdKcdiDd6U9a2vc0xuQdBVoxce8LRDTfGDOl0ukoljSnX9NeGdKcdiDd6U9r2jUsoyiKUoWouCuKolQhaRX3WyqdgD6S5vRr2itDmtMOpDv9qUx7KmPuiqIoSm7S6twVRVGUHKRO3PO9rDtpENGbRPQaEb1CRPMz61qJ6CkiWpZZtlQ6nQIR3UZEa4lokbMuNL3E/DJzLRYS0cGVS3nWtP+IiN7J5P8rRHSi87/vZdK+lIiOr0yq/5mWCUT0LBG9TkSLiejizPrE532OtCc+74monoj+RkSvZtJ+RWb9XkQ0L5Pv92emOwcRDcp878j8v71Sac+LMSY1f+Aph98AMBHAQACvAphc6XTlSfObAEb41l0HYGbm80wA11Y6nU7ajgJwMIBF+dIL4EQAj4PfxHU4gHkJTPuPAPxbyLaTM+VnEIC9MuWqpoJpHwPg4MznoQD+kUlj4vM+R9oTn/eZ/BuS+VwHYF4mPx8AMCOz/tcA/jXz+TwAv858ngHg/krle76/tDn3KC/rTgPuC8XvAPC5CqbFgzHmLwi+RStbek8GcKdh5gJoJqIx8aQ0SJa0Z+NkAPcZY3YZY1YA6ACXr4pgjFltjHkp83kLgNfB7yZOfN7nSHs2EpP3mfzbmvlal/kzAD4F4KHMen++y/V4CMBUIgp7zWjFSZu4R3lZd9IwAJ4kogVEdG5m3ShjzGqAKwaAkRVLXTSypTct1+OCTOjiNicElti0Zx71DwK7yFTlvS/tQArynohqiOgVAGsBPAV+kthojOkJSd8/0575/yYAw+NNcTTSJu6RXsSdMI4wxhwM4AQA5xPRUZVOUAlJw/W4GcAkAAcCWA3g+sz6RKadiIYAeBjAt40xm3NtGrKuoukPSXsq8t4Ys9sYcyD4/dCHAtg3bLPMMlFpz0XaxD3Ky7oThTHm3cxyLYBHwIVnjTxCZ5ZrK5fCSGRLb+KvhzFmTaby9gL4Dezjf+LSTkR1YHG82xjz35nVqcj7sLSnKe8BwBizEcCfwTH3ZiKSN9W56ftn2jP/b0L0UGCspE3co7ysOzEQ0WAiGiqfAXwawCJ4Xyh+NoDHKpPCyGRL7ywAZ2V6bhwOYJOEEJKCLw79L+D8BzjtMzK9H/YCsA+Av8WdPiETt70VwOvGmJ85/0p83mdLexrynojaiKg587kBwLHgNoNnAZya2cyf73I9TgXwjMm0riaOSrfoFvoH7iXwD3Bc7LJKpydPWieCewW8CmCxpBcco5sDYFlm2VrptDppvhf8CN0NdinnZEsv+BH1psy1eA3AlASm/a5M2haCK+YYZ/vLMmlfCuCECqf9SPDj/UIAr2T+TkxD3udIe+LzHsCHAbycSeMiAJdn1k8E33A6ADwIYFBmfX3me0fm/xMrWW5y/ekIVUVRlCokbWEZRVEUJQIq7oqiKFWIiruiKEoVouKuKIpShai4K4qiVCEq7oqiKFWIiruiKEoVouKuKIpShfx/031QJ1MbaW8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e42ccf1908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "   plt.plot(y_train, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(323, 1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.00685722e-02],\n",
       "       [-4.10929024e-02],\n",
       "       [-6.11326471e-02],\n",
       "       [ 5.15271276e-02],\n",
       "       [-1.55648619e-01],\n",
       "       [-3.06044631e-02],\n",
       "       [ 5.78232110e-02],\n",
       "       [ 2.62083896e-02],\n",
       "       [ 5.01899309e-02],\n",
       "       [ 1.46631794e-02],\n",
       "       [-2.86684372e-02],\n",
       "       [-3.93530801e-02],\n",
       "       [-5.63274212e-02],\n",
       "       [-3.52026150e-02],\n",
       "       [ 2.23676711e-02],\n",
       "       [-2.37763636e-02],\n",
       "       [ 5.86481094e-02],\n",
       "       [-3.30538228e-02],\n",
       "       [-3.79800647e-02],\n",
       "       [-1.60404500e-02],\n",
       "       [-2.66300291e-02],\n",
       "       [-2.12738756e-02],\n",
       "       [ 7.01994300e-02],\n",
       "       [-2.96880975e-02],\n",
       "       [-2.74342410e-02],\n",
       "       [ 1.49717815e-02],\n",
       "       [ 9.75511968e-03],\n",
       "       [-4.86188643e-02],\n",
       "       [-2.08432339e-02],\n",
       "       [ 1.33199990e-02],\n",
       "       [-3.94493416e-02],\n",
       "       [ 4.39155847e-04],\n",
       "       [ 4.40618917e-02],\n",
       "       [-2.47845221e-02],\n",
       "       [ 2.08798051e-02],\n",
       "       [-8.30221325e-02],\n",
       "       [ 1.16243586e-02],\n",
       "       [-1.55948587e-02],\n",
       "       [-3.13308649e-02],\n",
       "       [ 1.25948749e-02],\n",
       "       [-2.77243946e-02],\n",
       "       [-4.61737812e-02],\n",
       "       [-1.94374472e-03],\n",
       "       [ 2.78072320e-02],\n",
       "       [ 3.10921259e-02],\n",
       "       [ 2.35429071e-02],\n",
       "       [-7.33312033e-03],\n",
       "       [-3.00924052e-02],\n",
       "       [ 4.60349359e-02],\n",
       "       [-5.37082739e-03],\n",
       "       [ 1.34826694e-02],\n",
       "       [-3.23482342e-02],\n",
       "       [ 5.78570142e-02],\n",
       "       [ 1.26985209e-02],\n",
       "       [ 3.67424190e-02],\n",
       "       [ 2.00076308e-02],\n",
       "       [-2.40293145e-03],\n",
       "       [ 1.69031173e-02],\n",
       "       [ 5.74682578e-02],\n",
       "       [ 4.48467210e-03],\n",
       "       [-4.54915278e-02],\n",
       "       [-7.11214989e-02],\n",
       "       [ 2.19983421e-02],\n",
       "       [-9.78277624e-03],\n",
       "       [ 2.30426379e-02],\n",
       "       [-3.05390432e-02],\n",
       "       [ 7.85404816e-04],\n",
       "       [-6.33158162e-03],\n",
       "       [ 6.66715056e-02],\n",
       "       [ 5.21482825e-02],\n",
       "       [ 3.03082019e-02],\n",
       "       [-4.44820747e-02],\n",
       "       [ 2.23842002e-02],\n",
       "       [-4.62319702e-02],\n",
       "       [-1.07883416e-01],\n",
       "       [-2.65810173e-04],\n",
       "       [-1.37273213e-02],\n",
       "       [-6.33323658e-03],\n",
       "       [-4.64353152e-02],\n",
       "       [-5.84366098e-02],\n",
       "       [ 1.89683512e-02],\n",
       "       [ 6.06934205e-02],\n",
       "       [ 6.37686923e-02],\n",
       "       [-8.17200728e-03],\n",
       "       [-1.72619056e-02],\n",
       "       [ 5.24816886e-02],\n",
       "       [ 4.58263978e-02],\n",
       "       [ 3.76243070e-02],\n",
       "       [-1.58955753e-02],\n",
       "       [-2.11414304e-02],\n",
       "       [-3.66212521e-03],\n",
       "       [-5.24754040e-02],\n",
       "       [-1.80563182e-02],\n",
       "       [-3.34278233e-02],\n",
       "       [ 1.70357171e-02],\n",
       "       [-2.44395100e-02],\n",
       "       [-1.77132152e-03],\n",
       "       [-3.55237946e-02],\n",
       "       [-1.91897247e-02],\n",
       "       [ 3.01996209e-02],\n",
       "       [ 6.21182434e-02],\n",
       "       [ 5.72711602e-02],\n",
       "       [ 1.57852098e-02],\n",
       "       [-2.19960511e-02],\n",
       "       [ 4.50545698e-02],\n",
       "       [ 5.85704073e-02],\n",
       "       [ 4.27474082e-02],\n",
       "       [ 6.61761835e-02],\n",
       "       [ 6.37741461e-02],\n",
       "       [ 4.12664711e-02],\n",
       "       [-8.08420479e-02],\n",
       "       [-3.01897936e-02],\n",
       "       [-2.23721359e-02],\n",
       "       [-1.18856743e-01],\n",
       "       [-9.68827233e-02],\n",
       "       [-6.50664568e-02],\n",
       "       [-9.60524082e-02],\n",
       "       [-1.08019516e-01],\n",
       "       [-9.00924299e-03],\n",
       "       [-3.35701928e-02],\n",
       "       [ 5.45543022e-02],\n",
       "       [-2.95170695e-02],\n",
       "       [ 1.67881772e-02],\n",
       "       [ 1.25136729e-02],\n",
       "       [-3.49771976e-02],\n",
       "       [ 6.56157583e-02],\n",
       "       [ 5.17139584e-03],\n",
       "       [-2.42404118e-02],\n",
       "       [-3.06426734e-02],\n",
       "       [-5.89029826e-02],\n",
       "       [-1.91123262e-02],\n",
       "       [-3.76401022e-02],\n",
       "       [ 8.49382579e-03],\n",
       "       [-1.92929059e-02],\n",
       "       [ 4.39295694e-02],\n",
       "       [-4.19318900e-02],\n",
       "       [-3.45578119e-02],\n",
       "       [-2.75617689e-02],\n",
       "       [-7.64685310e-03],\n",
       "       [ 4.59045842e-02],\n",
       "       [ 1.97886936e-02],\n",
       "       [-2.72321198e-02],\n",
       "       [-3.43480408e-02],\n",
       "       [ 5.87061495e-02],\n",
       "       [ 2.51493827e-02],\n",
       "       [ 1.21243671e-02],\n",
       "       [ 2.29193792e-02],\n",
       "       [-1.88421048e-02],\n",
       "       [ 4.12937775e-02],\n",
       "       [ 1.11909900e-02],\n",
       "       [ 2.58174073e-02],\n",
       "       [-1.38449818e-02],\n",
       "       [-1.51097346e-02],\n",
       "       [-3.43061872e-02],\n",
       "       [ 4.34216335e-02],\n",
       "       [-3.68530862e-02],\n",
       "       [ 6.60754442e-02],\n",
       "       [-1.53771741e-03],\n",
       "       [ 1.33927651e-02],\n",
       "       [ 1.19812917e-02],\n",
       "       [ 1.44405961e-02],\n",
       "       [ 2.14560255e-02],\n",
       "       [ 2.17734613e-02],\n",
       "       [ 2.84169032e-03],\n",
       "       [-2.55091134e-02],\n",
       "       [ 2.65252590e-02],\n",
       "       [-8.66867602e-02],\n",
       "       [-5.39056510e-02],\n",
       "       [ 6.77732080e-02],\n",
       "       [-2.94401348e-02],\n",
       "       [ 3.45940702e-03],\n",
       "       [ 7.31729344e-02],\n",
       "       [-1.82351992e-02],\n",
       "       [-4.17912155e-02],\n",
       "       [-2.84031592e-02],\n",
       "       [ 4.92217243e-02],\n",
       "       [-2.20870171e-02],\n",
       "       [-4.55405340e-02],\n",
       "       [-2.26974878e-02],\n",
       "       [-2.13486366e-02],\n",
       "       [-9.89750400e-03],\n",
       "       [ 1.59938633e-02],\n",
       "       [ 1.91698931e-02],\n",
       "       [ 2.98935827e-02],\n",
       "       [-2.69024000e-02],\n",
       "       [ 1.44281238e-02],\n",
       "       [-6.05849363e-02],\n",
       "       [ 2.54278351e-02],\n",
       "       [ 6.69658184e-03],\n",
       "       [-5.03310598e-02],\n",
       "       [-2.11096276e-03],\n",
       "       [-1.56089328e-02],\n",
       "       [ 1.66533291e-02],\n",
       "       [-8.63618590e-03],\n",
       "       [-7.57205039e-02],\n",
       "       [-1.09097883e-01],\n",
       "       [-5.46146743e-02],\n",
       "       [ 4.61250544e-02],\n",
       "       [-2.37055123e-03],\n",
       "       [-3.57889570e-02],\n",
       "       [ 1.45949600e-02],\n",
       "       [ 2.22984329e-02],\n",
       "       [ 5.29132560e-02],\n",
       "       [ 1.07786153e-02],\n",
       "       [-1.33345976e-01],\n",
       "       [-2.21402664e-02],\n",
       "       [ 5.64722437e-03],\n",
       "       [ 4.97133806e-02],\n",
       "       [-2.85789780e-02],\n",
       "       [-2.99075674e-02],\n",
       "       [-2.40422115e-02],\n",
       "       [-5.31679317e-02],\n",
       "       [ 5.93845397e-02],\n",
       "       [ 5.91390915e-02],\n",
       "       [ 6.03891909e-02],\n",
       "       [ 5.01099415e-03],\n",
       "       [-2.23351009e-02],\n",
       "       [-2.59910524e-02],\n",
       "       [ 7.32226148e-02],\n",
       "       [ 7.48727471e-05],\n",
       "       [ 5.79364039e-02],\n",
       "       [ 5.92003912e-02],\n",
       "       [ 1.12994350e-02],\n",
       "       [ 6.18628487e-02],\n",
       "       [ 5.61289862e-02],\n",
       "       [ 3.41452137e-02],\n",
       "       [-2.03825217e-02],\n",
       "       [ 6.70541078e-02],\n",
       "       [ 3.25904973e-02],\n",
       "       [ 1.94662698e-02],\n",
       "       [ 1.58622004e-02],\n",
       "       [ 5.80916889e-02],\n",
       "       [ 7.62131289e-02],\n",
       "       [ 7.03778416e-02],\n",
       "       [ 2.89451592e-02],\n",
       "       [-1.82839483e-02],\n",
       "       [-3.25032324e-02],\n",
       "       [-2.55037434e-02],\n",
       "       [ 3.50029804e-02],\n",
       "       [ 1.91970728e-02],\n",
       "       [-2.66751423e-02],\n",
       "       [-4.29271199e-02],\n",
       "       [-4.38997000e-02],\n",
       "       [-4.96407133e-03],\n",
       "       [-3.47999558e-02],\n",
       "       [-3.10456380e-02],\n",
       "       [ 1.44477990e-02],\n",
       "       [ 2.18910147e-02],\n",
       "       [ 5.68595305e-02],\n",
       "       [-2.15712003e-02],\n",
       "       [ 7.11827800e-02],\n",
       "       [-1.97611358e-02],\n",
       "       [-1.80576257e-02],\n",
       "       [ 7.41323680e-02],\n",
       "       [-2.64959484e-02],\n",
       "       [-2.78620794e-02],\n",
       "       [-2.57028546e-02],\n",
       "       [-5.06831631e-02],\n",
       "       [-4.16479260e-03],\n",
       "       [-3.12944725e-02],\n",
       "       [-3.37099060e-02],\n",
       "       [-3.75130288e-02],\n",
       "       [-2.23529320e-02],\n",
       "       [ 3.03194877e-02],\n",
       "       [ 5.74535429e-02],\n",
       "       [-1.91767979e-02],\n",
       "       [ 7.27822352e-03],\n",
       "       [-9.12738964e-03],\n",
       "       [-3.78263332e-02],\n",
       "       [-2.07334943e-02],\n",
       "       [-2.00571977e-02],\n",
       "       [-6.64840639e-02],\n",
       "       [-4.46616113e-03],\n",
       "       [-8.54658112e-02],\n",
       "       [-4.19193506e-02],\n",
       "       [-1.56290643e-03],\n",
       "       [ 5.08997869e-03],\n",
       "       [-1.91979840e-01],\n",
       "       [-1.90602735e-01],\n",
       "       [-1.05192468e-01],\n",
       "       [-4.30537164e-02],\n",
       "       [ 1.50780352e-02],\n",
       "       [-1.53543800e-02],\n",
       "       [ 3.64469737e-02],\n",
       "       [-4.96941954e-02],\n",
       "       [-1.94803663e-02],\n",
       "       [ 1.45522458e-02],\n",
       "       [-4.92863879e-02],\n",
       "       [ 2.51104832e-02],\n",
       "       [-3.46811712e-02],\n",
       "       [-2.22746730e-02],\n",
       "       [ 1.76673308e-02],\n",
       "       [ 2.48302706e-02],\n",
       "       [ 3.17468122e-02],\n",
       "       [-5.07805422e-02],\n",
       "       [ 3.63919288e-02],\n",
       "       [-5.26678842e-03],\n",
       "       [ 2.35507246e-02],\n",
       "       [-1.98884942e-02],\n",
       "       [ 5.51651716e-02],\n",
       "       [-3.84493843e-02],\n",
       "       [ 1.47840474e-02],\n",
       "       [-2.14157999e-02],\n",
       "       [-2.65533756e-02],\n",
       "       [-2.86697820e-02],\n",
       "       [-3.23998556e-02],\n",
       "       [-2.89042778e-02],\n",
       "       [-2.69824229e-02],\n",
       "       [ 5.97120076e-03],\n",
       "       [ 1.01474226e-02],\n",
       "       [-1.45914527e-02],\n",
       "       [-3.45216133e-02],\n",
       "       [-2.08941158e-02],\n",
       "       [ 3.74558680e-02],\n",
       "       [-6.96666166e-03],\n",
       "       [-2.27913186e-02],\n",
       "       [-3.78916338e-02],\n",
       "       [ 1.54590961e-02],\n",
       "       [ 1.83956735e-02],\n",
       "       [ 2.38981955e-02],\n",
       "       [-7.12750806e-03],\n",
       "       [ 5.17703444e-02],\n",
       "       [ 1.31280962e-02]], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
